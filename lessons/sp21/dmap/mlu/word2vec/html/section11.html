<!DOCTYPE html><html lang='en'><head><title>Word2Vec</title><meta charset="utf-8"><style>/*! normalize.css v8.0.1 | MIT License | github.com/necolas/normalize.css */html{line-height:1.15;-webkit-text-size-adjust:100%}body{margin:0}h1{font-size:2em;margin:.67em 0}pre{font-family:monospace,monospace;font-size:1em}a{background-color:transparent}strong{font-weight:bolder}code{font-family:monospace,monospace;font-size:1em}img{border-style:none}::-webkit-file-upload-button{-webkit-appearance:button;font:inherit}blockquote,h1,h2,h3,h4,p,pre{margin:0}ol,ul{list-style:none;margin:0;padding:0}html{font-family:system-ui,-apple-system,BlinkMacSystemFont,"Segoe UI",Roboto,"Helvetica Neue",Arial,"Noto Sans",sans-serif,"Apple Color Emoji","Segoe UI Emoji","Segoe UI Symbol","Noto Color Emoji";line-height:1.5}*,::after,::before{box-sizing:border-box;border-width:0;border-style:solid;border-color:#e2e8f0}img{border-style:solid}table{border-collapse:collapse}h1,h2,h3,h4{font-size:inherit;font-weight:inherit}a{color:inherit;text-decoration:inherit}code,pre{font-family:Menlo,Monaco,Consolas,"Liberation Mono","Courier New",monospace}img{display:block;vertical-align:middle}img{max-width:100%;height:auto}.bg-gray-200{--bg-opacity:1;background-color:#edf2f7;background-color:rgba(237,242,247,var(--bg-opacity))}.bg-gray-300{--bg-opacity:1;background-color:#e2e8f0;background-color:rgba(226,232,240,var(--bg-opacity))}.bg-blue-200{--bg-opacity:1;background-color:#bee3f8;background-color:rgba(190,227,248,var(--bg-opacity))}.bg-blue-300{--bg-opacity:1;background-color:#90cdf4;background-color:rgba(144,205,244,var(--bg-opacity))}.border-gray-400{--border-opacity:1;border-color:#cbd5e0;border-color:rgba(203,213,224,var(--border-opacity))}.border-gray-500{--border-opacity:1;border-color:#a0aec0;border-color:rgba(160,174,192,var(--border-opacity))}.rounded{border-radius:.25rem}.rounded-full{border-radius:9999px}.border-solid{border-style:solid}.border{border-width:1px}.border-t{border-top-width:1px}.inline-block{display:inline-block}.flex{display:flex}.justify-center{justify-content:center}.justify-around{justify-content:space-around}.float-right{float:right}.float-left{float:left}.clearfix:after{content:"";display:table;clear:both}.clear-both{clear:both}.font-serif{font-family:Georgia,Cambria,"Times New Roman",Times,serif}.font-semibold{font-weight:600}.font-bold{font-weight:700}.h-64{height:16rem}.text-xs{font-size:.75rem}.text-sm{font-size:.875rem}.text-base{font-size:1rem}.text-xl{font-size:1.25rem}.leading-none{line-height:1}.m-2{margin:.5rem}.my-3{margin-top:.75rem;margin-bottom:.75rem}.mx-3{margin-left:.75rem;margin-right:.75rem}.my-4{margin-top:1rem;margin-bottom:1rem}.mt-1{margin-top:.25rem}.mt-2{margin-top:.5rem}.mr-2{margin-right:.5rem}.mb-2{margin-bottom:.5rem}.mr-3{margin-right:.75rem}.mb-3{margin-bottom:.75rem}.ml-3{margin-left:.75rem}.mt-4{margin-top:1rem}.mt-6{margin-top:1.5rem}.mr-6{margin-right:1.5rem}.max-w-sm{max-width:24rem}.max-w-2xl{max-width:42rem}.object-contain{-o-object-fit:contain;object-fit:contain}.overflow-hidden{overflow:hidden}.p-1{padding:.25rem}.py-1{padding-top:.25rem;padding-bottom:.25rem}.py-2{padding-top:.5rem;padding-bottom:.5rem}.px-3{padding-left:.75rem;padding-right:.75rem}.py-4{padding-top:1rem;padding-bottom:1rem}.px-4{padding-left:1rem;padding-right:1rem}.px-6{padding-left:1.5rem;padding-right:1.5rem}.pl-3{padding-left:.75rem}.shadow-lg{box-shadow:0 10px 15px -3px rgba(0,0,0,.1),0 4px 6px -2px rgba(0,0,0,.05)}.shadow-inner{box-shadow:inset 0 2px 4px 0 rgba(0,0,0,.06)}.text-left{text-align:left}.text-center{text-align:center}.text-black{--text-opacity:1;color:#000;color:rgba(0,0,0,var(--text-opacity))}.text-gray-700{--text-opacity:1;color:#4a5568;color:rgba(74,85,104,var(--text-opacity))}.text-gray-800{--text-opacity:1;color:#2d3748;color:rgba(45,55,72,var(--text-opacity))}.whitespace-no-wrap{white-space:nowrap}.w-1\/2{width:50%}.w-full{width:100%}.text-tiny{font-size:.5rem!important}body{color:#000!important;font-size:1.25rem!important}.main-content{max-width:900px}.lesson{padding-left:15px!important;padding-right:10px!important;--bg-opacity:1;background-color:#edf2f7;background-color:rgba(237,242,247,var(--bg-opacity))}.main-content,html{font-family:Arial,Georgia,Verdana,"Times New Roman"!important}.lesson-footer-card,.lesson-overview-card{font-family:"Times New Roman"!important}blockquote em:first-child{font-family:Times!important;font-size:1.35em;margin-right:10px}blockquote em:first-child:after{content:":"}.lesson-footer{margin-top:50px;margin-top:20px}table{border-spacing:10px;border:1px solid #666;border-collapse:collapse!important;margin-bottom:1em}td{padding:0 10px 10px 10px}thead{border:.5px solid #778899}table td+td{border-left:2px solid #778899}li>p{display:inline!important}.lesson ol{list-style-type:decimal;list-style-position:inside;margin-left:1em}.lesson ul{list-style-position:inside;list-style-type:none;margin-left:1em}.lesson ul li{padding-left:1em;padding-right:5px}.lesson ul li::before{content:"â€¢";padding-right:5px}span{white-space:nowrap}p.new{padding-top:0;padding-bottom:.5em}p.new+p{padding-top:.5em}h1,h2,h3,h4{font-weight:700;margin-top:.25em!important;margin-bottom:.05em!important;font-family:Georgia,Cambria,"Times New Roman",Times,serif!important}h1{font-size:2em!important;clear:both;color:#000!important}div+h1,h2{margin-top:0!important}h2{margin-top:.5em!important;font-size:1.5em!important;clear:both;color:#8b0000!important}h3{font-size:1.25em!important;clear:both;color:#006400!important}h4{font-size:1em!important;clear:both;color:#00008b!important}ul{margin-bottom:30px}p.new a{text-decoration:underline}.lesson a{text-decoration:underline;color:#00f}.title-text{font-size:2rem}blockquote{font-size:1em;background:#f9f9f9;border-left:10px solid #ccc;margin:.5em 10px;padding:.5em 10px;border-left-color:#ffcd69;border-right-color:#f6ba59;quotes:"\201C""\201D""\2018""\2019"}blockquote:before{color:#ccc;content:open-quote;font-size:4em;line-height:.1em;margin-right:.25em;vertical-align:-.4em}blockquote:after{color:#ccc;content:no-close-quote}blockquote p{display:inline}.shadow-lg{box-shadow:0 10px 15px -3px rgba(0,0,0,.1),0 4px 6px -2px rgba(0,0,0,.5)}img.center{-o-object-position:center;object-position:center;margin-left:auto;margin-right:auto}img.border{border:1px solid #021a40;margin-top:.5rem;margin-bottom:.75rem}img.iw400{height:auto;width:auto;max-width:400px}img.iw300{height:auto;width:auto;max-width:300px}code{font-size:smaller}pre code{font-size:15px}pre code:not(.line-number){background:#f4f4f4;font-family:monospace;font-size:15px;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;cursor:default;touch-action:none;-webkit-touch-callout:none;-webkit-tap-highlight-color:transparent;clear:both;border:1px solid #ddd;color:#666;page-break-inside:avoid;display:block;min-width:840px;max-width:840px;overflow:scroll;line-height:1.6;margin-bottom:1.6em;padding:1em 1.5em;-moz-tab-size:2;-o-tab-size:2;tab-size:2;word-wrap:break-word;white-space:pre-wrap;border-left:3px solid #f36d33}div.code-starter>pre code{border-left:3px solid #fdff44!important;background-image:radial-gradient(rgba(0,150,0,.75),#000 120%);color:#fff;font:.9rem Inconsolata,monospace}div.code-starter>pre code::after{content:"\a$_"}</style>
<script src="https://kit.fontawesome.com/7efc4bcee2.js" crossOrigin="anonymous"></script>
<script>
    let stateCheck = setInterval(function(){
      if (document.readyState === 'complete') {
        clearInterval(stateCheck);
        let s1 = document.getElementById('start');
        // console.log('doc is ready', s1);
        if (s1) {
           s1.setAttribute('tabindex', '-1');
           s1.focus(); 
           s1.scrollIntoView({behavior: 'smooth'}); 
           setTimeout(function(){s1.blur()}, 500);
           // console.log('focus set');
        }
      }
    }, 200);
    </script>
</head><body class="lesson"><div class="main-content lesson bg-gray-200 text-black p-1 pl-3 font-serif"><div class="md-inner">
<div id="start" class="section">&nbsp;</div><pre><code># add the following code to the above cell (`build_model_v1`) and run `test_v1`
def test_v1():
  document, df = build_dataset()
  model = build_model_v1(document)
  print(evaluate_model(model,df))

test_v1()</code></pre><p class="new">That's much better (your output will be different, but all the car models should
 be there):</p><pre><code>0 models are missing of 928
Toyota Camry-&gt;Honda Accord 0.9584
Toyota Camry-&gt;Nissan Van 0.9442
Toyota Camry-&gt;Mercedes-Benz SLK-Class 0.6755
Toyota Previa, Pontiac Montana, Chevrolet Uplander</code></pre><h2 id="randomness-of-ml">Randomness of ML</h2><p class="new">You may see different numbers in your output than what is shown. Many machine learning 
algorithms use randomization to make sure things are evenly spaced out in 
high dimensional space to start.  So if you re-run your above model, 
you should see different results each time -- but on average, your results 
should be close on each run.</p><h3 id="cpus-and-threads">CPUS and Threads</h3><p class="new">However, this randomness causes issues with reproducibility.  We can control the
 randomness by doing a few things.  The main issue for <code>Word2Vec</code> is that the
work it does is split across many threads.  You can think of a thread as an
 independent worker.  Usually you want to at least match the number of CPUs 
to the number of threads. That way if you have multiple CPUS, you can take
advantage of parallel processing.  For this lesson, we will not worry about
 how to find the number of CPUS our VM has.  But you can get this information
 from within a Python program.</p><pre><code>import multiprocessing
print(multiprocessing.cpu_count())</code></pre><blockquote><p class="new"><strong><em>Coder's Log</em></strong>  a <strong>process</strong> is an active program.  It has it's own
 memory and resources.  A <strong>thread</strong> is 'lightweight' in that it can share the
 same memory of it's parent process.  Processes are isolated; threads are not.</p></blockquote><p class="new">When work is split up between threads, each thread may be assigned different
 units of work, finish at different times and their results may be combined
 differently.  At the cost of being less efficient, we can tell <code>Word2Vec</code> to
 only use a single thread.  That will stop the randomness.  Note that there
 is also a <code>seed</code> hyperparameter that can also be used to control randomness.</p><p class="new">Go back to the previous code cell and update your code (and <strong>run</strong> it):</p><pre><code>def build_model_v1(doc):
  model = gensim.models.Word2Vec(
             doc,
             min_count=1, # ignore words that occur less than 1 times
             workers=1
          ) 
  return model
  
def test_v1():
  document, df = build_dataset()
  model = build_model_v1(document)
  print(evaluate_model(model,df))

test_v1()</code></pre><p class="new">You should now see consistent numbers between multiple runs. Here's the
 output we get (your output will be slightly different):</p><pre><code>0 models are missing of 928
Toyota Camry-&gt;Honda Accord 0.9773
Toyota Camry-&gt;Nissan Van 0.9501
Toyota Camry-&gt;Mercedes-Benz SLK-Class 0.4989
GMC Jimmy, Ford Five Hundred, GMC Envoy</code></pre><h2 id="windows-of-context">Windows of Context</h2><p class="new">The output of word2vec is a set of word vectors. And each word vector is
 essentially the same as shown in the previous lesson on word embeddings. 
The goal of the algorithm is to have words with similar context occupy close spatial positions.  As 
discussed in the word embeddings lesson, the cosine similarity can be used as
 a metric of closeness.</p><p class="new">For word2vec there is a concept of defining both a 'target word' and 'context
 words'. Below shows an example of the target word '<code>by</code>' 
with its context window (<code>word is known the company it</code>):</p><pre><code>             target word
                 ðŸ‘‡
a word is known [by] the company it keeps
  ðŸ‘†          ðŸ‘†     ðŸ‘†           ðŸ‘†        
     context            context</code></pre><p class="new">For the window of size <strong><code>n</code></strong> the contexts are defined by capturing <strong><code>n</code></strong> 
words to the left of the target and <strong><code>n</code></strong> words to its right. This window of 
context (shown here to be size 3) slides along the text.  So the next word 
that is processed is <code>the</code>:</p><pre><code>                target word
                    ðŸ‘‡
a word is known by [the] company it keeps
       ðŸ‘†       ðŸ‘†       ðŸ‘†            ðŸ‘†        
         context            context</code></pre><p class="new">Given that information, it's clear that where the target word appears in the
 document and the size of the context window can affect the quality of the
 output. If the window is too small, 'meaning' becomes very narrow.  If the
 window is too big, words no longer separate from each other.</p><h3 id="exercise">Exercise</h3><p class="new">Go all the way back to the code cell that creates the function <code>build_dataset</code>.
Move the column <code>'Make_Model'</code> from the front of the list to the second
 position.  Now Re-run the cell with <code>build_dataset</code> in it. 
You should see the following (from the output of <code>test_pd_data</code>:</p><pre><code>['Factory Tuner', 'Luxury', 'High-Performance', 'BMW 1 Series M', 'Compact']</code></pre><p class="new">Now re-run the cell with test_v1():</p><pre><code>test_v1()</code></pre><p class="new">Notice that the position of where the make/model appears in the document
 affects the result (the similarity of Camry and Accord went <strong>down</strong>).  We 
can avoid this issue by creating a wide context window.</p><p class="new">The default window size is 5.  Do the following modifications:</p><ul><li>update <code>build_model_v1</code> to be the following:</li></ul><pre><code>def build_model_v1(doc):
  model = gensim.models.Word2Vec(
             doc,
             min_count=1, # ignore words that occur less than 1 times
             workers=1,   # one thread to remove randomness
             window=10,   # wide window size
          ) 
  return model</code></pre><p class="new">When you re-run the cell (<code>test_v1()</code>) the output becomes:</p><pre><code>0 models are missing of 928
Toyota Camry-&gt;Honda Accord 0.9734
Toyota Camry-&gt;Nissan Van 0.9292
Toyota Camry-&gt;Mercedes-Benz SLK-Class 0.1101
Dodge Ramcharger, Pontiac Montana, GMC Jimmy</code></pre><h2 id="epoch-training">Epoch Training</h2><img alt="Epoch.png" class="border iw400 my-4 float-right ml-3 mr-6" src="https://raw.githubusercontent.com/habermanUIUC/CodeStoryLessons/main/lessons/dmap/mlu/word2vec/html/Epoch.png"/><p class="new">As we saw in the ML Prep lesson, each machine learning algorithm involves
 iteration over the dataset to help adjust and improve. Initially, the 
word vectors are assigned random locations in very high dimensional space. 
As the algorithm iterates, these word vectors move closer to neighborhoods with 'similar words'.  </p><p class="new">Remember, that 'closeness' is defined by how similar these words are.  And 
being similar, means the words share similar contexts. </p><div class="clear-both"></div><p class="new">So you expect common misspellings and upper/lower case versions of the 
same word to be located near each other in high dimensional space.
<img alt="3DWordVectors.png" class="border iw400 my-4 mr-3 float-left" src="https://raw.githubusercontent.com/habermanUIUC/CodeStoryLessons/main/lessons/dmap/mlu/word2vec/html/3DWordVectors.png"/>The image to the left shows how the days of the week (orange circle) 
might be near each other.  Also similar relationships would have similar
 distances (e.g. king to queen and uncle to aunt)</p><p class="new">You can control how many times word2vec iterates on it training through the <code>iter</code> parameter
 (whose default is 5).  Let's up this to 15.  Of course, this is a choice
 that comes from experimentation and evaluation.  If your corpus is huge, you
 may not have enough years to iterate.</p><p class="new">Let's create another function:</p></div></div></body></html>