<!DOCTYPE html><html lang='en'><head><title>Clustering</title><meta charset="utf-8"><style>/*! normalize.css v8.0.1 | MIT License | github.com/necolas/normalize.css */html{line-height:1.15;-webkit-text-size-adjust:100%}body{margin:0}h1{font-size:2em;margin:.67em 0}pre{font-family:monospace,monospace;font-size:1em}a{background-color:transparent}strong{font-weight:bolder}code{font-family:monospace,monospace;font-size:1em}img{border-style:none}input{font-family:inherit;font-size:100%;line-height:1.15;margin:0}input{overflow:visible}[type=checkbox]{box-sizing:border-box;padding:0}::-webkit-file-upload-button{-webkit-appearance:button;font:inherit}h1,h2,h3,h4,p,pre{margin:0}ol,ul{list-style:none;margin:0;padding:0}html{font-family:system-ui,-apple-system,BlinkMacSystemFont,"Segoe UI",Roboto,"Helvetica Neue",Arial,"Noto Sans",sans-serif,"Apple Color Emoji","Segoe UI Emoji","Segoe UI Symbol","Noto Color Emoji";line-height:1.5}*,::after,::before{box-sizing:border-box;border-width:0;border-style:solid;border-color:#e2e8f0}img{border-style:solid}input:-ms-input-placeholder{color:#a0aec0}input::-ms-input-placeholder{color:#a0aec0}input::-moz-placeholder{color:#a0aec0}table{border-collapse:collapse}h1,h2,h3,h4{font-size:inherit;font-weight:inherit}a{color:inherit;text-decoration:inherit}input{padding:0;line-height:inherit;color:inherit}code,pre{font-family:Menlo,Monaco,Consolas,"Liberation Mono","Courier New",monospace}img{display:block;vertical-align:middle}img{max-width:100%;height:auto}.container{width:100%}@media (min-width:640px){.container{max-width:640px}}@media (min-width:768px){.container{max-width:768px}}@media (min-width:1024px){.container{max-width:1024px}}@media (min-width:1280px){.container{max-width:1280px}}.bg-gray-200{--bg-opacity:1;background-color:#edf2f7;background-color:rgba(237,242,247,var(--bg-opacity))}.bg-gray-300{--bg-opacity:1;background-color:#e2e8f0;background-color:rgba(226,232,240,var(--bg-opacity))}.bg-orange-300{--bg-opacity:1;background-color:#fbd38d;background-color:rgba(251,211,141,var(--bg-opacity))}.bg-green-200{--bg-opacity:1;background-color:#c6f6d5;background-color:rgba(198,246,213,var(--bg-opacity))}.bg-blue-200{--bg-opacity:1;background-color:#bee3f8;background-color:rgba(190,227,248,var(--bg-opacity))}.bg-blue-300{--bg-opacity:1;background-color:#90cdf4;background-color:rgba(144,205,244,var(--bg-opacity))}.border-gray-400{--border-opacity:1;border-color:#cbd5e0;border-color:rgba(203,213,224,var(--border-opacity))}.border-gray-500{--border-opacity:1;border-color:#a0aec0;border-color:rgba(160,174,192,var(--border-opacity))}.border-indigo-500{--border-opacity:1;border-color:#667eea;border-color:rgba(102,126,234,var(--border-opacity))}.rounded{border-radius:.25rem}.rounded-full{border-radius:9999px}.border-solid{border-style:solid}.border{border-width:1px}.border-l-2{border-left-width:2px}.border-t{border-top-width:1px}.cursor-pointer{cursor:pointer}.block{display:block}.inline-block{display:inline-block}.flex{display:flex}.justify-center{justify-content:center}.justify-around{justify-content:space-around}.float-right{float:right}.float-left{float:left}.clearfix:after{content:"";display:table;clear:both}.clear-both{clear:both}.font-sans{font-family:system-ui,-apple-system,BlinkMacSystemFont,"Segoe UI",Roboto,"Helvetica Neue",Arial,"Noto Sans",sans-serif,"Apple Color Emoji","Segoe UI Emoji","Segoe UI Symbol","Noto Color Emoji"}.font-serif{font-family:Georgia,Cambria,"Times New Roman",Times,serif}.font-semibold{font-weight:600}.font-bold{font-weight:700}.h-64{height:16rem}.text-xs{font-size:.75rem}.text-sm{font-size:.875rem}.text-base{font-size:1rem}.text-xl{font-size:1.25rem}.leading-none{line-height:1}.leading-normal{line-height:1.5}.m-2{margin:.5rem}.my-1{margin-top:.25rem;margin-bottom:.25rem}.my-2{margin-top:.5rem;margin-bottom:.5rem}.my-3{margin-top:.75rem;margin-bottom:.75rem}.my-4{margin-top:1rem;margin-bottom:1rem}.mt-1{margin-top:.25rem}.mb-1{margin-bottom:.25rem}.mt-2{margin-top:.5rem}.mr-2{margin-right:.5rem}.mb-2{margin-bottom:.5rem}.mr-3{margin-right:.75rem}.mb-3{margin-bottom:.75rem}.mt-4{margin-top:1rem}.mb-4{margin-bottom:1rem}.mt-6{margin-top:1.5rem}.max-w-sm{max-width:24rem}.max-w-2xl{max-width:42rem}.object-contain{-o-object-fit:contain;object-fit:contain}.opacity-0{opacity:0}.overflow-hidden{overflow:hidden}.p-1{padding:.25rem}.p-3{padding:.75rem}.py-1{padding-top:.25rem;padding-bottom:.25rem}.py-2{padding-top:.5rem;padding-bottom:.5rem}.px-3{padding-left:.75rem;padding-right:.75rem}.py-4{padding-top:1rem;padding-bottom:1rem}.px-4{padding-left:1rem;padding-right:1rem}.px-6{padding-left:1.5rem;padding-right:1.5rem}.pl-3{padding-left:.75rem}.absolute{position:absolute}.shadow-md{box-shadow:0 4px 6px -1px rgba(0,0,0,.1),0 2px 4px -1px rgba(0,0,0,.06)}.shadow-lg{box-shadow:0 10px 15px -3px rgba(0,0,0,.1),0 4px 6px -2px rgba(0,0,0,.05)}.shadow-inner{box-shadow:inset 0 2px 4px 0 rgba(0,0,0,.06)}.text-left{text-align:left}.text-center{text-align:center}.text-black{--text-opacity:1;color:#000;color:rgba(0,0,0,var(--text-opacity))}.text-gray-700{--text-opacity:1;color:#4a5568;color:rgba(74,85,104,var(--text-opacity))}.text-gray-800{--text-opacity:1;color:#2d3748;color:rgba(45,55,72,var(--text-opacity))}.whitespace-no-wrap{white-space:nowrap}.w-1\/2{width:50%}.w-2\/3{width:66.666667%}.w-full{width:100%}@media (min-width:768px){.md\:w-2\/3{width:66.666667%}}.text-tiny{font-size:.5rem!important}body{color:#000!important;font-size:1.25rem!important}.main-content{max-width:900px}.lesson{padding-left:15px!important;padding-right:10px!important;--bg-opacity:1;background-color:#edf2f7;background-color:rgba(237,242,247,var(--bg-opacity))}.main-content,html{font-family:Arial,Georgia,Verdana,"Times New Roman"!important}.lesson-footer-card,.lesson-overview-card{font-family:"Times New Roman"!important}.lesson-footer{margin-top:50px;margin-top:20px}table{border-spacing:10px;border:1px solid #666;border-collapse:collapse!important;margin-bottom:1em}td{padding:0 10px 10px 10px}thead{border:.5px solid #778899}table td+td{border-left:2px solid #778899}li>p{display:inline!important}.lesson ol{list-style-type:decimal;list-style-position:inside;margin-left:1em}.lesson ul{list-style-position:inside;list-style-type:none;margin-left:1em}.lesson ul li{padding-left:1em;padding-right:5px}.lesson ul li::before{content:"‚Ä¢";padding-right:5px}span{white-space:nowrap}p.new{padding-top:0;padding-bottom:.5em}p.new+p{padding-top:.5em}h1,h2,h3,h4{font-weight:700;margin-top:.25em!important;margin-bottom:.05em!important;font-family:Georgia,Cambria,"Times New Roman",Times,serif!important}h1{font-size:2em!important;clear:both;color:#000!important}div+h1,h2{margin-top:0!important}h2{margin-top:.5em!important;font-size:1.5em!important;clear:both;color:#8b0000!important}h3{font-size:1.25em!important;clear:both;color:#006400!important}h4{font-size:1em!important;clear:both;color:#00008b!important}ul{margin-bottom:30px}p.new a{text-decoration:underline}.lesson a{text-decoration:underline;color:#00f}.title-text{font-size:2rem}.shadow-lg{box-shadow:0 10px 15px -3px rgba(0,0,0,.1),0 4px 6px -2px rgba(0,0,0,.5)}img.formula-block{margin-left:auto;margin-right:auto;margin-top:.25rem;margin-bottom:.75rem}img.formula-inline{margin-top:.25rem;margin-bottom:.25rem}img.center{-o-object-position:center;object-position:center;margin-left:auto;margin-right:auto}img.border{border:1px solid #021a40;margin-top:.5rem;margin-bottom:.75rem}img.iw600{height:auto;width:auto;max-width:600px}img.iw400{height:auto;width:auto;max-width:400px}img.iw500{height:auto;width:auto;max-width:500px}img.iw300{height:auto;width:auto;max-width:300px}code{font-size:smaller}pre code{font-size:15px}pre code:not(.line-number){background:#f4f4f4;font-family:monospace;font-size:15px;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;cursor:default;touch-action:none;-webkit-touch-callout:none;-webkit-tap-highlight-color:transparent;clear:both;border:1px solid #ddd;color:#666;page-break-inside:avoid;display:block;min-width:840px;max-width:840px;overflow:scroll;line-height:1.6;margin-bottom:1.6em;padding:1em 1.5em;-moz-tab-size:2;-o-tab-size:2;tab-size:2;word-wrap:break-word;white-space:pre-wrap;border-left:3px solid #f36d33}div.code-starter>pre code{border-left:3px solid #fdff44!important;background-image:radial-gradient(rgba(0,150,0,.75),#000 120%);color:#fff;font:.9rem Inconsolata,monospace}div.code-starter>pre code::after{content:"\a$_"}.tab{font-size:1rem;border-color:#8c6728}.tab-content{max-height:0;max-width:100%;transition:max-height .35s}.tab input:checked~.tab-content{max-height:100vh}.tab input:checked+label{padding:1rem;border-left-width:2px;border-color:#6574cd;background-color:#f8fafc;color:#6574cd}.tab label::after{float:right;right:0;top:0;display:block;width:1em;height:1.5em;line-height:1.5;font-size:1rem;text-align:center;transition:all .35s}.tab input[type=checkbox]+label::after{content:"+";font-weight:700;border-width:1px;border-radius:9999px;border-color:#8c6728}.tab input[type=checkbox]:checked+label::after{transform:rotate(315deg);background-color:#6574cd;color:#f8fafc}</style>
<script src="https://kit.fontawesome.com/7efc4bcee2.js" crossOrigin="anonymous"></script>
<script>
    let stateCheck = setInterval(function(){
      if (document.readyState === 'complete') {
        clearInterval(stateCheck);
        let s1 = document.getElementById('start');
        // console.log('doc is ready', s1);
        if (s1) {
           s1.setAttribute('tabindex', '-1');
           s1.focus(); 
           s1.scrollIntoView({behavior: 'smooth'}); 
           setTimeout(function(){s1.blur()}, 500);
           // console.log('focus set');
        }
      }
    }, 200);
    </script>
</head><body class="lesson"><div class="main-content lesson bg-gray-200 text-black p-1 pl-3 font-serif"><div class="md-inner">
<div id="start" class="section">&nbsp;</div><h1 class="overview"></h1><div class="lesson-overview bg-gray-200 flex justify-center"><div class="text-center px-4 py-2 m-2"><div class="lesson-overview-card displaycard bg-blue-200 max-w-sm rounded overflow-hidden shadow-lg"><div>¬†</div><img alt="Text" class="object-contain h-64 w-full" src="https://raw.githubusercontent.com/habermanUIUC/CodeStoryLessons/main/lessons/dmap/mlu/clustering/html/MachineLearningV1-sm.png"/><div class="px-6 py-4"><div class="title-text text-center leading-none font-bold text-xl">Clustering</div><p class="text-center mt-2 text-gray-800 text-xl">finding patterns in a sea of data</p><div class="text-gray-700 text-base">¬†</div><div class="text-center mb-3"><span class="inline-block bg-gray-300 rounded-full px-3 py-1 text-sm font-semibold text-gray-700 mr-2">#machine learning</span></div><div class="flex border-t border-solid border-gray-500 shadow-inner justify-around bg-blue-300"><div class="text-gray-700 text-center px-4 m-2 text-sm"><span class="whitespace-no-wrap">D.M. &amp; the üêç</span></div><div class="text-gray-700 text-center px-4 m-2 text-sm"><span class="whitespace-no-wrap"><strong>Version:</strong> <!-- -->SP21</span></div></div><div class="text-gray-700 mt-1 text-center text-tiny">All Rights Reserved</div></div></div></div><div class="text-center px-4 py-2 m-2 w-1/2"><div class="displaycard bg-gray-200 max-w-sm rounded overflow-hidden shadow-lg"><div class="px-6 py-4 text-left"><div class="text-center font-bold text-xl">Clustering<br/><div><span>prerequisites</span><div class="text-center text-xs mb-2">(start only after finishing)</div><p class="max-w-sm text-gray-800 text-sm">‚¶ø <strong>mlprep</strong></p><p class="max-w-sm text-gray-800 text-sm">‚¶ø <strong>Python classes</strong></p></div></div></div><div class="px-6 py-4 text-left text-gray-800"><div class="text-center font-bold text-xl">Colab Notes</div><p class="max-w-sm text-sm">1. <strong>Copy</strong> this notebook <img alt="copy2drive.png" class="inline-block" src="https://raw.githubusercontent.com/habermanUIUC/CodeStoryLessons/main/lessons/dmap/mlu/clustering/html/copy2drive.png"/></p><p class="max-w-sm text-sm">2. <strong>Update</strong> the <strong><code>NET_ID</code></strong> in the notebook</p><p class="max-w-sm text-gray-800 text-sm">3. <strong>Hit ‚ñ∂Ô∏è¬†</strong> to install the INFO 490 IDE</p><div class="text-center font-bold text-xl">¬†</div><div class="text-center font-bold text-xl">Jupyter/PyCharm Notes</div><p class="max-w-sm text-gray-800 text-sm text-left">The testing framework does <strong>not work</strong> (at this time) for Jupyter  notebooks or local code development.</p></div></div></div></div><h1 class="section" id="section1">Clustering</h1><h1 class="section" id="section2">finding patterns in a sea of data</h1><img alt="kmeans2d.jpg" class="iw300 border float-left mb-1 mr-3" src="https://raw.githubusercontent.com/habermanUIUC/CodeStoryLessons/main/lessons/dmap/mlu/clustering/html/kmeans2d.jpg"/><p class="new">Clustering is an unsupervised machine learning technique whose task is to
assign instances (i.e. rows) of data to clusters of groups who share some
 common feature (or set of features).</p><p class="new"> Clustering is similar to classification (a supervised learning task) but it
  works with <u>unlabeled</u> data.  Clustering has a wide variety of
   applications from grouping DNA sequences, customer segmentation (understanding 
   the traits of purchases or shopping behaviors) and recommender systems (like IMDB or Amazon). </p><h2 id="unlabeled-data">Unlabeled Data</h2><p class="new">Because a majority of data is unlabeled, clustering is the go-to technique 
to first understand the characteristics and perhaps hidden patterns
  and structure in the data. Other than visualization, clustering is usually
   the first tool out of the box to explore an unlabeled dataset.</p><h2 id="visualizing-the-first-step">Visualizing, the first step</h2><p class="new">As just mentioned, when you have an unlabeled dataset, your first task
 (assuming you already know the goals of the project) is to visualize the
  data to get some kind bearing.  The great thing about visualizing the
   data first is that it forces you (or your team) to describe the intent of
    the project and what variables/attributes you are interested in.  Many
     people will say, ‚Äúlet's do some machine learning on that data‚Äù without
      knowing the goals or what they expect to find. As of now, there is no
       analysis technique that can consume arbitrary data and do something
        <em>meaningful</em> with it. </p><p class="new"> There's a lot of issues when it comes to visualizing a raw dataset.  Since
  displays have only the capacity to draw in 2D (even when we draw in '3D' we
   are projecting down to a 2D surface), the best we can do is to pick a few 
  dimensions (usually 2) on which to visualize. If a dataset has 20
   attributes, time needs to be invested into understanding what to
    visualize. You can't just give raw data to a scatter plot, for example, without
     telling it what variables (e.g. 'x' and 'y' axes for 2D plot) to use.</p><h3 id="data-preparation">Data Preparation</h3><p class="new">The following code builds a pandas dataframe from a small dataset. We will
 use this dataset to illustrate some of the techniques we will explore. </p><div class="ide code-starter clearfix"><pre><code>import numpy as np
import matplotlib.pyplot as plt
import pandas as pd

import LessonUtil as Util

pd.set_option('display.max_columns', None)
pd.set_option('display.width', 200)

def build_dataset():
  p = Util.path_for_data('canon.csv')
  # Importing the dataset
  df = pd.read_csv(p)

  keep = ['a', 'b', 'c', 'd', 'e', 'f']

  return df[keep]
  
data_df = build_dataset()
print(data_df.head(5))</code></pre></div><p class="new">As you can see, the column names offer no help (on purpose). How would you
 try to visualize this?</p><pre><code>          a    b         c         d         e         f
0  0.359148  0.2  0.343460  0.125000  0.129137  1.000000
1  0.109290  1.0  0.728682  0.646739  0.767139  0.684211
2  0.598361  0.2  0.054264  0.135870  0.092199  0.578947
3  0.191257  1.0  0.604651  0.510870  0.542553  0.736842
4  0.136612  1.0  0.857881  0.565217  0.634752  0.789474</code></pre><h3 id="pairwise-plots">Pairwise Plots</h3><p class="new">One exploratory technique is to limit the attributes of interests and then do 
pairwise combinations of them and visualize (using a scatter plot) of 
each combination. This could reveal patterns between different pairs of attributes.</p><p class="new">Matplotlib does not offer an easy way (you need to build your own solution), 
but Pandas <a href="https://pandas.pydata.org/docs/reference/api/pandas.plotting.scatter_matrix.html" target="_blank">does</a>.
It is quite easy to do this:</p><div class="ide code-starter clearfix"><pre><code>def pair_wise_plots(df):
  axes = pd.plotting.scatter_matrix(df, figsize=(12,12))

pair_wise_plots(data_df)</code></pre></div><h3 id="pairwise-plotting">Pairwise Plotting</h3><img alt="pairwisecars.png" class="iw300 border float-left my-2 mr-3" src="https://raw.githubusercontent.com/habermanUIUC/CodeStoryLessons/main/lessons/dmap/mlu/clustering/html/pairwisecars.png"/><p class="new">The pairwise plot creates scatter plots for each combination of the columns
 (the reason why you will usually want to drop some of the columns). </p><p class="new">Note that the visualization is essentially symmetric. A few things to point out.  </p><h4 id="distributions">Distributions</h4><p class="new">The graphs down the diagonal are the distributions of the different features/attributes.  </p><ul><li>attribute <code>b</code> has 3 modes</li><li>attribute <code>f</code> looks close to being normally distributed.</li></ul><h4 id="pair-wise-correlations">Pair-wise correlations</h4><ul><li>Attribute <code>a</code> demonstrates a linear relationship with <code>c</code>, <code>d</code>, <code>e</code></li><li>Attribute <code>f</code> seems to have no 'visible' relationship with the other columns</li></ul><p class="new">It helps to look at this data while being 'blinded' by anonymity. Having some
 domain information about the dataset is clearly useful, but sometimes when
 you are given data that you have no expertise in, it feels very much like
 this process.</p><h3 id="finding-clusters">Finding Clusters</h3><p class="new">Do any column pairs seem to have some interesting 'clusters' of points? 
A <em>cluster</em> is a group of points that seem to share a common characteristic. 
For example if you were to cluster dogs using weight and height, small dogs
 (like dachshunds and chihuahaus) may be in one group, while large dogs (e.g. 
collies and hounds), would be in a different group.  Some dogs, like terriers
 could belong in either group.  How would you decide? </p><p class="new">So if you were wanting to create an algorithm for clustering points/objects/dogs
 how and where would you start?</p><div class="font-sans container mt-1 mb-4 "><div></div><div class="w-2/3 md:w-2/3"><div class="shadow-md"><div class="tab overflow-hidden border-t bg-green-200"><input class="absolute opacity-0" id="tab-multi-10" name="tabs" type="checkbox"/><label class="block p-3 leading-normal cursor-pointer" for="tab-multi-10"><span> What is your clustering algorithm? </span></label><div class="tab-content overflow-hidden border-l-2 bg-orange-300 border-indigo-500 leading-normal"><p class="p-3"><span>Think about it before reading on</span></p></div></div></div></div></div><h1 class="section" id="section3">An Algorithmic Overview</h1><p class="new">Clustering is the process of dividing data points into 
homogeneous classes or clusters. When we have a collection of points/objects, 
we want to put objects into groups based on similarity:</p><ul><li>Objects in the same group are as similar as possible</li><li>Objects in different group are as dissimilar as possible</li></ul><h2 id="distance-metric">Distance Metric</h2><p class="new">Since clusters are defined based on having 'similar' objects, we need to
 create a metric to measure similarity. However, instead of trying to maximize a 
similarity metric we can minimize a distance metric. One of the most basic
 distance metrics (as discussed in another lesson) is euclidean distance.  </p><p class="new">Given two points (<code>p</code> and <code>q</code>) both with two attributes (<code>x</code>, <code>y</code>), the distance 
between those points is defined as <img alt="math?math=%5Clarge%20%5Csqrt%7B(p_x%20-%20q_x)%5E2%20%2B%20(p_y%20-%20q_y)%5E2%7D" class="my-1 formula-inline" src="https://render.githubusercontent.com/render/math?math=%5Clarge%20%5Csqrt%7B(p_x%20-%20q_x)%5E2%20%2B%20(p_y%20-%20q_y)%5E2%7D" style="display:inline-block"/>.
If the points contain <em>n</em> dimensions, the general formula 
becomes: <img alt="math?math=%5Clarge%20d(p%2Cq)%3D%5Csqrt%7B%5Csum_%7Bi%3D1%7D%5En%7B(q_i%20-%20p_i)%5E2%7D%7D" class="my-1 formula-inline" src="https://render.githubusercontent.com/render/math?math=%5Clarge%20d(p%2Cq)%3D%5Csqrt%7B%5Csum_%7Bi%3D1%7D%5En%7B(q_i%20-%20p_i)%5E2%7D%7D" style="display:inline-block"/></p><h2 id="distance-with-cluster-centers-or-centroids">Distance with Cluster Centers (or Centroids)</h2><p class="new">It's not enough to measure distances between points in the dataset; we need to 
<img alt="d1.png" class="iw300 border float-left my-2 mr-3" src="https://raw.githubusercontent.com/habermanUIUC/CodeStoryLessons/main/lessons/dmap/mlu/clustering/html/d1.png"/>define cluster centers as a way to mathematically represent the center. It
 is from this center object (aka point) (a virtual point in the dataset) that
 all the other points in the cluster will be measured.</p><p class="new">So each cluster will be 'measured' as the sum of squared distances (similar
 to the SSE measurement in statistics) between the points in the cluster and 
the center.  Looking at the diagram, <img alt="math?math=%5Clarge%20D_%7BC1%7D" class="my-1 formula-inline" src="https://render.githubusercontent.com/render/math?math=%5Clarge%20D_%7BC1%7D" style="display:inline-block"/> is the sum of the 
distances between each point and the cluster center <img alt="math?math=%5Clarge%20C_1" class="my-1 formula-inline" src="https://render.githubusercontent.com/render/math?math=%5Clarge%20C_1" style="display:inline-block"/>. This
 distance is also called the <strong><em>intra</em></strong> cluster distance.</p><p class="new">If you look closely, you'll see it's the square of the euclidean distance. 
This helps with penalizing outliers and objects/points far from the center.</p><br/><p class="new">With that in place, let's discuss a very common clustering algorithm: K-means.</p><h1 class="section" id="section4">K-Means Clustering</h1><p class="new">One of the popular clustering algorithms is K-means.  The algorithm
 essentially works like this:</p><ol start="1"><li>Clusters the data into k groups</li><li>Select k points at random as cluster centers.</li><li>Assign objects to their closest cluster center according to the distance metric.</li><li>Calculate the new centroid (i.e. mean) of all objects in each cluster.</li><li>Repeat steps 2, 3 and 4 until the same points are assigned to each cluster in consecutive rounds.</li></ol><br/><h2 id="objective-function">Objective <em>Fun</em>ction</h2><p class="new">Essentially, we are minimizing the cost of the sum of <strong><em>intra</em></strong> cluster distances.
This is the <strong>objective</strong> function for the machine learning algorithm.  This
 calculation is called <strong>inertia</strong> and we want to minimize it.</p><img alt="inertia.png" class="iw600 border my-3 center" src="https://raw.githubusercontent.com/habermanUIUC/CodeStoryLessons/main/lessons/dmap/mlu/clustering/html/inertia.png"/><p class="new">Please take the time to digest the above diagram.  The values dc1c3, dc1c2, 
dc2c3 are the <strong><em>inter</em></strong> cluster distances. The objective function can be summarized as
<img alt="objective.png" class="iw500 border my-3 center" src="https://raw.githubusercontent.com/habermanUIUC/CodeStoryLessons/main/lessons/dmap/mlu/clustering/html/objective.png"/>This metric is also commonly written as <img alt="math?math=%5Clarge%20SSE%3D%5Csum_%7Bi%3D1%7D%5Ek%5Csum_%7Bx%5Cin%7BC_i%7D%7Ddist(c_i%2C%20x)%5E2" class="my-1 formula-inline" src="https://render.githubusercontent.com/render/math?math=%5Clarge%20SSE%3D%5Csum_%7Bi%3D1%7D%5Ek%5Csum_%7Bx%5Cin%7BC_i%7D%7Ddist(c_i%2C%20x)%5E2" style="display:inline-block"/> where <img alt="math?math=%5Clarge%20dist" class="my-1 formula-inline" src="https://render.githubusercontent.com/render/math?math=%5Clarge%20dist" style="display:inline-block"/> is the euclidean (L2) distance between the cluster
 center and a point in the same cluster. For clustering documents, 
perhaps using tf‚Ä¢idf scores, one could replace it with the cosine distance.</p><h3 id="did-we-forget-anything--">Did we forget anything?  </h3><div class="font-sans container mt-1 mb-4 "><div></div><div class="w-2/3 md:w-2/3"><div class="shadow-md"><div class="tab overflow-hidden border-t bg-green-200"><input class="absolute opacity-0" id="tab-multi-11" name="tabs" type="checkbox"/><label class="block p-3 leading-normal cursor-pointer" for="tab-multi-11"><span> What is the issue with this clustering algorithm? </span></label><div class="tab-content overflow-hidden border-l-2 bg-orange-300 border-indigo-500 leading-normal"><p class="p-3"><span><strong>k</strong> needs to be known (the number of clusters)</span></p></div></div></div></div></div><h3 id="what-about-k">What about <em>k</em>?</h3><p class="new">The BIG (and interesting) question to ask is 'how do we figure out how many
 clusters to look for?" The algorithm <em>needs</em> a <strong>k</strong>!</p><p class="new">The first tactic we already discussed: visualize all the possibilities. That
 won't work with high dimensional data or clusters that depend on more than
 2 attributes. You can do dimensional reduction (another algorithm; another
 lesson) to find the two attributes that explain the most amount of variance
 between the data instances.</p><p class="new">However, another technique is to use the model's inertia value. We won't 
necessarily pick the lowest inertia value across all <em>K</em> values, we want the 
lowest inertia for the least amount of clusters. After all having <em>K</em> equal 
the number of data points, would have a very low inertia score!</p><h2 id="data-preparation-taking-our-blinders-off-">Data Preparation, taking our blinders off üôà</h2><p class="new">The dataset we have been working with is the same dataset from the normalization 
lesson: the information on 261 different cars. Let's reload the un-anonymized dataset:</p><div class="ide code-starter clearfix"><pre><code>def build_dataset(as_is=False):
  p = Util.path_for_data('clean_cars.csv')
  # Importing the dataset
  df = pd.read_csv(p)
  if as_is:
    return df

  drop = ['brand', 'year'] #, 'cylinders']
  return df[df.columns.difference(drop)]
  
data_df = build_dataset()
print(data_df.head(5))</code></pre></div><p class="new">A few domain specific notes to the columns.</p><ul><li><p class="new">This dataset is already cleaned (the same from the normalization lesson)</p></li><li><p class="new">We dropped the column <code>brand</code> since this is a categorical attribute that will
be useful to see if any of the other attributes can help determine the brand
of the car.</p></li><li><p class="new">We dropped the <code>year</code> column since this value isn't intrinsic to the
engineering of the car.</p></li><li><p class="new">Another candidate for dropping is the 'number of cylinders'. Almost all
the cars are 4,6,or 8 cylinders. This natural cluster, may influence the
clusters.</p></li><li><p class="new">A good (and required) analysis is to experiment with keeping/dropping
these from the cluster analysis.</p></li></ul><p class="new">It's been too many words, let's start coding! Scikit-learn has a K-means
 algorithm.  </p><p class="new">Sklearn's k-means algorithm requires that the data is an array of points. 
As an example for data with three attributes, 4 data points (labeled p1 - p4) might look
 like: </p><pre><code>data = [(1.0, 1.4, 2.3), (2.0, 0.5, -1.0), (3.3, 4.0, -1.0), (0,0,0)]
#              p1               p2                p3            p4</code></pre><p class="new">For our running example, let's convert the pandas dataframe into a correctly 
formatted array of values (n-dimensional points of data) using numpy:</p><div class="ide code-starter clearfix"><pre><code>def convert_to_points(df):
    p = []
    for c in df.columns:
      pc = df[c]
      p.append(pc)
    
    points = np.stack(p, axis=1)
    return points

points = convert_to_points(data_df)
print(data_df.head(5))
print(points[0:5,:])</code></pre></div><div class="font-sans container mt-1 mb-4 "><div></div><div class="w-2/3 md:w-2/3"><div class="shadow-md"><div class="tab overflow-hidden border-t bg-green-200"><input class="absolute opacity-0" id="tab-multi-12" name="tabs" type="checkbox"/><label class="block p-3 leading-normal cursor-pointer" for="tab-multi-12"><span> Did you forgot <code>np.stack</code> how works? </span></label><div class="tab-content overflow-hidden border-l-2 bg-orange-300 border-indigo-500 leading-normal"><p class="p-3"><span><code>np.stack</code> rearranges the data based on the <code>axis</code> 
parameter: <img alt="npstack.png" class="iw400 border my-3 center" src="https://raw.githubusercontent.com/habermanUIUC/CodeStoryLessons/main/lessons/dmap/mlu/clustering/html/npstack.png"/></span></p></div></div></div></div></div><p class="new">For many problems in data science, a big part of the pipeline is converting
 your data from its native format to the format required to use libraries
 like sci-learn, pandas, matplotlib, etc.  Pay close attention to how each of
 the different libraries we use require the data to be slightly altered.</p><p class="new">It's also possible that (some) pandas data frame <em>can</em> be passed into the
 algorithm. However, this helps us separate the <em>need</em> to use pandas with
 sklearn. </p><h2 id="model-building">Model Building</h2><p class="new">Once we have our data as a list of points, we can use the <code>KMeans</code> class:</p><pre><code>    km = KMeans(n_clusters=K,    # how many clusters
                max_iter=300,    # iterate 300 times, for a 'single run'
                n_init=100,      # run it 100 times, picking the best one
                init='random',   # pick random centers (provide your own, 'k-means++')
                random_state=42, # pick None to be different each time
                tol=0.0001)      # when to declare convergence between two consecutive iterations</code></pre><p class="new">There are many <a href="https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html" target="_blank">parameters</a> 
 that can be configured for <code>KMeans</code>.  However, the above is a good starting
 point.</p><p class="new">Once that is done, you create/train the model using the <code>fit</code> method.  But
 instead of calling <code>transform</code>, we use <code>predict</code> instead.  There is still a
 <code>transform</code> method, but it is used to calculate the distance between cluster
 centers and the incoming data.  The <code>predict</code> will generate which cluster
 each data sample is assigned to.</p><div class="ide code-starter clearfix"><pre><code>import numpy as np
from sklearn.cluster import KMeans
import matplotlib
import matplotlib.pyplot as plt

def cluster_points(points, K=6):
    km = KMeans(n_clusters=K,    # how many clusters
                max_iter=300,    # iterate 300 times, for a 'single run'
                n_init=100,      # run it 100 times, picking the best one
                init='random',   # pick random centers (provide your own, 'k-means++')
                random_state=42, # pick None to be different each time
                tol=0.0001)      # when to declare convergence between two consecutive iterations
    
    # points needs to be like
    # [x1,y1], [x2, y2], etc
    
    # build the model
    km.fit(points)

    centers = km.cluster_centers_
    print(centers)
    
    # pass the same data through the model, 
    # predict will assign the point to a label (cluster number)
    labels = km.predict(points)
    return km, labels
    
km, labels = cluster_points(points)
print(len(km.cluster_centers_), set(labels))</code></pre></div><h3 id="centers-and-labels">Centers and Labels</h3><p class="new">As you can see, we have six cluster centers. Additionally, we also have an
 array (<code>labels</code>) that label each row as to what cluster they were assigned to.
In this case the 'labels' are simply the values <code>0 .. K-1</code>.</p><h4 id="a-cleaning-note">A cleaning note</h4><p class="new">Usually, things don't go so smoothly. But we spent a lot of time cleaning the
 cars dataset to make this step pain-free. Take a look at another dataset (to 
be used in the lesson component).</p><div class="ide code-starter clearfix"><pre><code>def unclean_data_demo():
  p = Util.path_for_data('anon.csv')
  tmp_df = pd.read_csv(p)[['a','b','c']]
  print(tmp_df.head(5))
  km, lbls = cluster_points(tmp_df)
  
unclean_data_demo()</code></pre></div><h3 id="data-preparation-a-reprise">Data Preparation (a reprise)</h3><p class="new">What happened?</p><pre><code>ValueError: Input contains NaN, infinity or a value too large for dtype('float64').</code></pre><p class="new">That error <code>NaN</code> is saying that some of the columns in our dataset don't have
 values.  We can clean these rows up by just removing them. Add the following line 
to make it all work</p><pre><code>    tmp_df = tmp_df.dropna()
    km, lbls = cluster_points(tmp_df)</code></pre><p class="new">Another note, is we are passing the dataframe directly to <code>cluster_points</code>. This 
won't always work, so buyer beware.</p><h2 id="hyper-parameters">Hyper Parameters</h2><p class="new">As you noticed, there are many parameters that you can adjust for building
 <code>KMeans</code> clustering models.  Most of the defaults are good bets and you can
 <a href="https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html" target="_blank">read</a> 
the documentation as well.</p><p class="new"><code>KMeans</code> will iterate <strong>max_iter</strong> times to build a model.  Then it will run
 the whole thing over again <strong>n_init</strong> times -- picking the very best 'fit'. The 
other two points to make: </p><ul><li><strong><em>fit</em></strong> was called to build the ML model.</li><li><strong><em>predict</em></strong> was called to pass the data through the model.</li></ul><p class="new">The <code>predict</code> method will take a point (or a set of points) and predict a label
 (e.g cluster center) for that point.</p><h2 id="drawing-our-clusters">Drawing our clusters</h2><p class="new">Printing out both the centers and the set of labels only goes so far (it's
 pretty boring too). It would be easier to see what's happening by plotting 
the centers, along with the data. Add the following code to the same cell
 with <code>plot_kmeans</code>:</p><div class="ide code-starter clearfix"><pre><code>import matplotlib
import matplotlib.pyplot as plt
import matplotlib.cm as cm

def plot_kmeans(points, centers, labels, x0=0, x1=1, columns=None):
    
    K = len(centers)
    colors=['#a6cee3','#1f78b4','#b2df8a','#33a02c','#fb9a99','#e31a1c','#fdbf6f','#ff7f00','#cab2d6']
    cmap = matplotlib.colors.ListedColormap(colors)
    if len(colors) &lt;= K:
      cmap = cm.Dark2
    
    fig, axes = plt.subplots(nrows=1, ncols=1, figsize=(6,6))
    for c_id in range(0, K):
        mask = labels == c_id
        c = cmap(c_id)
        axes.scatter(points[mask][:, x0], points[mask][:, x1], color=c, s=25)
        # mark the center
        axes.scatter(centers[c_id, x0], centers[c_id, x1], color=c, s=200, alpha=0.5, edgecolor='black')

    if columns is not None:
      axes.set_xlabel(columns[x0], fontsize=18)
      axes.set_ylabel(columns[x1], fontsize=16)
    axes.grid()</code></pre></div><p class="new">Once that is done, just add the call to the <code>plot_kmeans</code> function</p><pre><code>plot_kmeans(points, km.cluster_centers_, labels)</code></pre><div class="font-sans container mt-1 mb-4 "><div></div><div class="w-2/3 md:w-2/3"><div class="shadow-md"><div class="tab overflow-hidden border-t bg-green-200"><input class="absolute opacity-0" id="tab-multi-13" name="tabs" type="checkbox"/><label class="block p-3 leading-normal cursor-pointer" for="tab-multi-13"><span> Do you understand <code>points[mask][:,x0]</code> in the above code block? </span></label><div class="tab-content overflow-hidden border-l-2 bg-orange-300 border-indigo-500 leading-normal"><p class="p-3"><span>
<strong>mask</strong> is an array of booleans <br/><strong>points</strong><code>[mask]</code> is all the points where the <code>mask == True</code><br/>points<span>[mask][<strong>: ,  x0</strong>]</span><br/>of those points select all the rows (i.e. <code>:,</code> ) and column <code>x0</code><br/>this syntax is only available with numpy arrays</span></p></div></div></div></div></div><h1 class="section" id="section5">Finding <strong>K</strong> </h1><img alt="k6plot.png" class="iw300 border my-3 float-left mr-3" src="https://raw.githubusercontent.com/habermanUIUC/CodeStoryLessons/main/lessons/dmap/mlu/clustering/html/k6plot.png"/><p class="new">Your plot should be very similar to what is shown here. Each cluster is
 colored and the cluster centers are shown as bigger circles near/in each cluster. </p><p class="new">Our function uses a default <code>K=6</code> for its cluster count, but what is the optimal K to choose? </p><p class="new">We can take advantage of the objective function that <code>KMeans</code> is trying to
 minimize: The inertia or sum of intra cluster distances. Go ahead and add
 this line of code after you plot it:</p><pre><code>print(km.inertia_)</code></pre><p class="new">We can use this value of <code>inertia</code> along with the value of <code>K</code> and plot the
 resulting points.</p><div class="ide code-starter clearfix"><pre><code>def plot_scores(scores, y="inertia score"):
    fig, axes = plt.subplots(nrows=1, ncols=1)
    axes.plot(scores[:, 0], scores[:, 1], color='blue', marker='x')
    axes.set_ylabel(y, fontsize=18)
    axes.set_xlabel('K', fontsize=18)

def get_inertias(points):
  scores = []
  for k in range(2, 16):
     km, labels = cluster_points(points, K=k)
     scores.append((k, km.inertia_))
  return np.array(scores)
  
# be sure to comment this call out, before testing
# it will cause a timeout during any submissions
inertia_scores = get_inertias(points)
plot_scores(inertia_scores)</code></pre></div><h2 id="the-trick-is-in-the-elbow">The Trick is in the Elbow</h2><img alt="carElbow.png" class="iw300 border float-left my-2 mr-3" src="https://raw.githubusercontent.com/habermanUIUC/CodeStoryLessons/main/lessons/dmap/mlu/clustering/html/carElbow.png"/><p class="new">When looking at the plot of inertia vs K, you don't want the lowest inertia
 score, you want the smallest number of clusters for the best inertia. </p><p class="new">Looking at the graph, <code>k=3</code> (as well as <code>k=4</code>) is such a point. It's also 
called the 'elbow' method, because you are looking at where the graph bends (like an elbow). </p><img alt="elbow.png" class="iw300 clear-both float-left border my-2 mr-3" src="https://raw.githubusercontent.com/habermanUIUC/CodeStoryLessons/main/lessons/dmap/mlu/clustering/html/elbow.png"/><br/><p class="new">The next image (generated using a different dataset) shows that both 3 and 4 are 
good candidates for K and the 'elbow' is bit more pronounced. </p><p class="new">The issue with the 'elbow' method is that you need to visualize it.</p><h2 id="silhouette-scores">Silhouette Scores</h2><p class="new">Another metric is called the silhouette coefficient. It's more computationally
 complex (as the inertia score is already calculated for us). It is calculated 
for each data instance. The Silhouette Coefficient measures how closely related an 
object is to its own cluster against the other clusters.  For
 point <img alt="math?math=%5Clarge%20i" class="my-1 formula-inline" src="https://render.githubusercontent.com/render/math?math=%5Clarge%20i" style="display:inline-block"/>, the score is as follows:</p><img alt="math?math=%5CLarge%20S_i%20%3D%20(x_i-y_i)%2Fmax(x_i%2Cy_i)" class=" formula-block" src="https://render.githubusercontent.com/render/math?math=%5CLarge%20S_i%20%3D%20(x_i-y_i)%2Fmax(x_i%2Cy_i)"/><ul><li><img alt="math?math=%5Clarge%20x_i" class="my-1 formula-inline" src="https://render.githubusercontent.com/render/math?math=%5Clarge%20x_i" style="display:inline-block"/> is the mean distance to the point of the next closest cluster</li><li><img alt="math?math=%5Clarge%20y_i" class="my-1 formula-inline" src="https://render.githubusercontent.com/render/math?math=%5Clarge%20y_i" style="display:inline-block"/> is the mean <em>intra</em> cluster distance between <em>i</em> and other
points in the same cluster</li></ul><p class="new">The score can range from -1 to +1.</p><ul><li>+1: tight, well separated clusters: points are part of the correct cluster</li><li>0: mean of clusters are indifferent</li><li>-1: sprawling, overlapping clusters: points are assigned to the wrong cluster</li></ul><p class="new">Let's run the following code to plot the silhouette scores.  What value do
 you think K should be?</p><div class="ide code-starter clearfix"><pre><code>from sklearn.metrics import silhouette_score

def get_silhouette_scores(points):
  scores = []
  for k in range(2, 16):
     km, labels = cluster_points(points, K=k)
     s_score = silhouette_score(points,  km.labels_)
     scores.append((k, s_score))
  return np.array(scores)

# be sure to comment this call out, before testing
# it will cause a timeout during any submissions
sscores = get_silhouette_scores(points)
plot_scores(sscores, y='silhouette')</code></pre></div><h2 id="high-score-wins">High Score Wins</h2><img alt="carSilhouette.png" class="iw300 border float-left my-2 mr-3" src="https://raw.githubusercontent.com/habermanUIUC/CodeStoryLessons/main/lessons/dmap/mlu/clustering/html/carSilhouette.png"/><p class="new">Your graph should look similar to the following image to the left. Looking at 
the graph, it seems that having only 2 clusters performs well. </p><br class="clear-both"/><img alt="silhouette.png" class="iw300 border float-left my-2 mr-3" src="https://raw.githubusercontent.com/habermanUIUC/CodeStoryLessons/main/lessons/dmap/mlu/clustering/html/silhouette.png"/><p class="new">In other analysis, you may see a graph similar to the following, where the 
silhouette score peaks at a certain K (<code>K=3</code>). Even when seeing this result, it 
would be wise to try both <code>k=3</code> and <code>k=4</code> yourself.</p><h2 id="additional-metrics">Additional Metrics</h2><h3 id="dunn-index">Dunn Index</h3><p class="new">Dunn index is another metric by using which we can effectively evaluate the quality of clustering. 
The following metric basically represents a ratio between the minimum of inter-cluster distances 
and the maximum of intra-cluster distances. Unlike the inertia value, the Dunn index magnitude 
largely depends on the distance between clusters. 
In the most cases, to provide a high-quality clustering we want to maximize the Dunn index value</p><img alt="math?math=%5CLarge%20Dunn_%7Bindex%7D%3D%5Cfrac%7Bmin%5C%7Binter-cluster%20%5Cspace%20distances%5C%7D%7D%7Bmax%5C%7Bintra-cluster%20%5Cspace%20distances%5C%7D%7D" class=" formula-block" src="https://render.githubusercontent.com/render/math?math=%5CLarge%20Dunn_%7Bindex%7D%3D%5Cfrac%7Bmin%5C%7Binter-cluster%20%5Cspace%20distances%5C%7D%7D%7Bmax%5C%7Bintra-cluster%20%5Cspace%20distances%5C%7D%7D"/><p class="new">Want to maximize the Dunn Index:</p><ul><li>as clusters spread apart: numerator grows</li><li>as clusters become dense/compact: denominator shrinks</li></ul><h2 id="special-k3"><em>Special</em> <code>K=3</code></h2><p class="new">For our purposes, we will choose <code>K=3</code>. The silhouette score doesn't drop 
too much and it gives us a little more visual information to discus.  </p><div class="ide code-starter clearfix"><pre><code>def special_k():
    km, labels = cluster_points(points, K=3)
    print(len(km.cluster_centers_), set(labels))
    plot_kmeans(points, km.cluster_centers_, labels)

special_k()</code></pre></div><p class="new">Re-run your model using k=[2,3,4].  Decide which you think better fits the data. </p><h2 id="more-than-2-dimensions">More than 2 Dimensions</h2><img alt="k-anim.gif" class="iw300 mr-3 float-left border" src="https://raw.githubusercontent.com/habermanUIUC/CodeStoryLessons/main/lessons/dmap/mlu/clustering/html/k-anim.gif"/><p class="new">Don't forget (it's easy to when looking at the graphs) that the K-means is
 actually doing it's best to separate the clusters in high dimensional space. </p><p class="new">The animation shown is selecting just a few of the possible combinations to
 plot.</p><p class="new">You can actually graph <em>any two</em> dimensions and plot them to see
 where the centers lie!   </p><pre><code># you can pick any two to plot too
attributes = data_df.columns.tolist()
x0 = attributes.index('mpg')
x1 = attributes.index('time-to-60')
plot_kmeans(points, km.cluster_centers_, labels, x0,x1, attributes)</code></pre><h2 id="moving-centers-a-look-into-how-k-means-works">Moving Centers, a look into how K-means works</h2><img alt="kmeans-animation-cars.gif" class="iw500 border float-left mr-3" src="https://raw.githubusercontent.com/habermanUIUC/CodeStoryLessons/main/lessons/dmap/mlu/clustering/html/kmeans-animation-cars.gif"/><p class="new">When the K‚Ä¢Means algorithm initializes, the default algorithm picks random
 center points and slowly adjusts them. You can see this happening in the
 animated picture.  </p><p class="new">The points change colors as the cluster assignment changes. Around iteration 10, 
the cluster centers become stable (<code>k=4</code>). Also, the algorithm was initialized 
with very poor centers to make the moving centers obvious. </p><p class="new"><code>KMeans</code> can be initialized with random points (the default), specific centers or
 with the value 'k-means++' which attempts to select centers biased towards
 diversity (by spreading the initial centers evenly). </p><pre><code>km = KMeans(n_clusters=K, init='k-means++')</code></pre><h1 class="section" id="section6">External Measures for Cluster Evaluation</h1><p class="new">The previous validation metrics ('elbow', silhouette, Dunn) are about
 assessing the <em>internal</em> 'goodness' of the cluster. They help identify the ideal
 number of clusters.</p><p class="new">In <em>external</em> clustering validation, the focus is on evaluating the quality of the
 resulting clusters. It is used to select suitable clustering algorithm for a 
given data set or for comparing different clustering algorithms on the same dataset. </p><h2 id="scoring-against-a-labeled-dataset">Scoring against a labeled dataset</h2><p class="new">You can also use K‚Ä¢means with labeled data. You can use the labeled column to help
 score how well your model fits the underlying independent variables. The 
idea would be to look at each point and compare the true label it to the cluster 
label the model predicated. Having this count would be a good metric to compare 
models built with different hyper-parameters. </p><p class="new">For the car dataset, there are 3 manufacturers: US, Japan, Europe.  We can
 see how which instances get assigned to which clusters:</p><div class="ide code-starter clearfix"><pre><code>def get_distributions(df, points, K=3):
    km, labels = cluster_points(points, K=K)
    centers = km.cluster_centers_

    score = [{} for i in range(0, len(centers))]
    for idx, cluster_num in enumerate(labels):
        predict = cluster_num
        actual = df['brand'][idx]

        s = score[predict]
        v = s.get(actual, 0)
        s[actual] = v + 1

    return score

km, labels = cluster_points(points, K=3)
cars_df = build_dataset(as_is=True)
print(cars_df.groupby(['brand']).size())
print(get_distributions(cars_df, points, K=5))</code></pre></div><p class="new">Below is how to interpret the output (your cluster labels may be different):</p><pre><code>brand
0    161  US Cars
1     48  Europe Cars
2     52  Japan Cars

{0: 74},                Cluster 0 (all US Cars 74/161)
{1: 4, 0: 49, 2: 5},    Cluster 1 (most US Cars 49/161)
{2: 47, 1: 44, 0: 38}   Cluster 2 (equally shared)

US:      most in cluster 0, 74/161; rest split into 1, 2
Europe:  most in cluster 2  44/48
Japan:   most in cluster 2  47/52</code></pre><p class="new">One interesting note that even if you adjust K (3,4,5), the clusters don't
 separate enough to predict the country of origin.  Perhaps if you put the
 <code>brand</code> back into the cluster analysis, this might change.</p><p class="new">So in this case, the manufacturing label doesn't correspond closely with the
 clusters.</p><h2 id="adjusted-rand-index">Adjusted Rand Index</h2><p class="new">A more systematic way to evaluate the performance of a clustering algorithm
 (assuming you have the <em>true</em> labels) is the adjusted Rand index. It is a
 function that measures similarity between the labels (acting as the 'truth') 
and the predicted values (e.g. the labels). You can 
<a href="https://scikit-learn.org/stable/modules/clustering.html#adjusted-rand-score" target="_blank">read the details</a>.
The adjusted Rand index has the maximum value 1, and its expected value is 0 in 
the case of random clusters (ie poor clustering). The index will be less than 0 
if the result is less than the expected result.</p><div class="ide code-starter clearfix"><pre><code>import sklearn.metrics as metrics
labels_true = ['a','a','a',  'b','b','b']
labels_pred = [0,0,0, 1,1,1]
print(metrics.adjusted_mutual_info_score(labels_true, labels_pred))</code></pre></div><p class="new">As you can see, the 'truth' labels do not have to correspond to what the
 clustering algorithm is using for cluster labels.</p><p class="new">We can use this metric to see if our car clusters do a good job at predicting
 the <code>brand</code> assignment:</p><pre><code>score = metrics.adjusted_rand_score(cars_df['brand'].values, labels)
print('Index score', score)</code></pre><p class="new">As you can see this score confirms our intuition discussed above.</p><h1 class="section" id="section7">More than one way to cluster</h1><h2 id="limitations">Limitations</h2><p class="new">K-means works well in most situations, but it's important to understand when
 it will fail:
<img alt="circleData.png" class="iw300 border my-3 float-left mr-3" src="https://raw.githubusercontent.com/habermanUIUC/CodeStoryLessons/main/lessons/dmap/mlu/clustering/html/circleData.png"/></p><ul><li>Data that's not linear separable. If you can't draw a line to separate
the clusters, K-means will not work well. Concentric/Circular and spectral clusters cannot 
be properly separated.</li><li>Data that has no clusters.  If all your data is uniform, there's no
clusters. However, K-means will still find the centers of those points.</li><li>You pick the wrong K.</li><li>The metric is based on distance, it can't find non-distance based
relationships.</li><li>You forget to properly scale your attributes.</li></ul><h2 id="beyond-k-means">Beyond K-means</h2><img alt="hClustering.png" class="iw400 border my-3 float-right" src="https://raw.githubusercontent.com/habermanUIUC/CodeStoryLessons/main/lessons/dmap/mlu/clustering/html/hClustering.png"/><p class="new">The topic of clustering is rich with research and algorithms.  They are 
many different types of spatial clustering methods, including:</p><ul><li>Partitioning methods (including K-means)</li><li>Hierarchical clustering (builds by merging)</li><li>Fuzzy clustering (allows partial classification)</li><li>Density-based clustering (neighborhoods of points -- can find unique/nonlinear shapes)</li><li>Model-based clustering (assumes data is coming from a model of distributions)</li></ul><p class="new">Popular algorithms include:</p><ul><li>EM (Expectation Maximization) similar to K-means, but uses statistical
methods (probability) to find the likelihood of an observation belonging to a cluster.</li><li>Mean-Shift Algorithm.  Works by sliding windows.</li><li>DBSCAN (Density Based Spatial Clustering of Applications with Noise). You
can find circular and spectral clusters.</li><li>Hierarchical Clustering which builds hierarchies of information.</li></ul><p class="new">You can even change the algorithm that <code>KMeans</code> uses internally (see <code>algorithm</code> parameter)
There's a lot more to learn but this lesson gives you the background and
 vocabulary to incorporate other clustering algorithms in your own projects
 and research. </p><h1 class="section" id="section8">Lesson Assignment</h1><h2 id="robo-batter">ROBO-Batter</h2><img alt="roboBat.png" class="iw400 border my-3 float-right" src="https://raw.githubusercontent.com/habermanUIUC/CodeStoryLessons/main/lessons/dmap/mlu/clustering/html/roboBat.png"/><p class="new">In this lesson, you have been hired to program an artificial intelligent 
baseball batter (robo-agent).  </p><p class="new">Before each game, the robo-agent will be programmed based on who will be
 pitching. Your job is to run KMeans on the baseball pitching data and pick out the
 two factors that best separates the type of pitch that is being thrown. As a
 hitter, if you know the pitch type, you have enough information to adjust
 your bat swing to get a hit. </p><p class="new">So robo-agent will instantaneously be able to measure the two factors you pick
 as the ball leaves the pitcher's hand. It will know what pitch is coming
 (hopefully -- based on your expertise) and make the necessary adjustments to
 get a hit.</p><h2 id="the-data">The Data</h2><p class="new">In a previous lesson we worked with some baseball data. The dataset has been
 anonymized and has the only measurements that robo-agent can detect. 
Let's take a look:</p><div class="ide code-starter clearfix"><pre><code>import LessonUtil as Util
import pandas as pd

def get_robo_dataset():
  path = Util.path_for_data('robo.csv')
  df = pd.read_csv(path)
  return df

pitcher_df = get_robo_dataset()
pitcher_df.head()</code></pre></div><p class="new"> </p><p class="new">It's output should be as follows:</p><pre><code>     a        b          c         d     e    f
0  -1.504577  54.993072  6.927464  86.9  5.5  2560.0
1  -1.427146  54.849458  7.063881  94.0  5.7  2520.0
2  -1.418858  54.889659  6.952980  94.8  5.6  2618.0
3  -1.579721  55.387799  6.721339  80.4  5.1  3012.0
4  -1.596243  55.058817  6.936298  94.6  5.4  2663.0</code></pre><h2 id="the-pitcher">The Pitcher</h2><p class="new">In this situation we know the pitcher has only 4 different types of pitches: 
Fast Balls, Curve Balls, Sliders, and Change Ups.</p><h2 id="robo-batter">Robo Batter</h2><p class="new">You can pick any two columns to work with (the agent only has two sensors). 
You will cluster all the pitches using those two dimensions. The goal is to
 have separate homogeneous clusters (as much as possible).</p><h2 id="step-1">Step 1</h2><h3 id="experiment">Experiment</h3><p class="new">Work through this dataset, much like we did for the car data. Determine all the
 things you need to eventually cluster the data. You will want to analyze, visualize, 
hypothesis and test.  Before you move to step 2, you will want to know all
 the details of how to solve this problem using functions.</p><div class="ide code-starter clearfix"><pre><code></code></pre></div><h2 id="step-2">Step 2</h2><h3 id="robo-classes">Robo-Classes</h3><p class="new">Your solution will be written as a Python class <code>RoboBatter</code>. The entire solution must
 run using your class design. You will make most of the design decisions; 
However, here's how your class must work:</p><pre><code>path_to_data = Util.path_for_data('robo.csv') 

# the constructor takes a string (a path to the data file)
robo_batter = RoboBatter(path_to_data)

# the method cluster_pitches performs K-means on the data set
# it returns the model and the labels
km, labels = robo_batter.cluster_pitches()

# both km and labels are the only required attributes 
km = robo_batter.km
labels = robo_batter.labels</code></pre><p class="new">Note that this is a good technique to use when you first learn how to create
 classes.  Start with how the classes will be used (or how you want them to
 be used) and then fill out the details. It's a top down approach to
 software development.</p><div class="ide code-starter clearfix"><pre><code>class RoboBatter(object):
    pass</code></pre></div><p class="new">You can use other classes as well; but the API (how people will use your
 <code>RoboBatter</code> class is shown above.</p><h2 id="step-3">Step 3</h2><h3 id="scoring-your-robobatter">Scoring Your RoboBatter</h3><p class="new">The following class will be used to measure how well it clusters the data.</p><pre><code>class RoboTester(object):

  def __init__(self, batter):
     # reads in the true labels
  
  def get_distributions(self):
     # returns the cluster distributions
     # similar to function of the same name 

  def rand_index_score(self):
    # returns the adjusted_rand_score (sklearn) 
    # between the true labels, and batter.labels
    # Perfect labeling has a 1.0 score</code></pre><p class="new">This class is in the <code>LessonUtil</code> as well. You can use it as follows:</p><pre><code>import LessonUtil as Util
robo_batter = RoboBatter(Util.path_for_data('robo.csv'))
km, labels = robo_batter.cluster_pitches()

tester = Util.RoboTester(robo_batter)
print(tester.get_distributions()
print(tester.rand_index_score())</code></pre><h3 id="point-totals">Point Totals</h3><p class="new">The total points is based on how well the final clustering performs (based on the rand index score).</p><ul><li>The tests print out the rand score of your clustering algorithm -- although you should calculate it yourself.</li><li>For the lesson, 20 points are based on the performance. So even for an poor clustering job, can get you 80 total points.</li></ul><h2 id="extra-credit">Extra Credit</h2><p class="new">Note that extra credit is <em>only</em> available if the rand index &gt; 0.90.</p><p class="new">Add the method <code>predict_pitch</code> to your <code>RoboBatter</code> class. The method takes
 an array of measurements (in the same order as the columns of the data). And
 returns the abbreviation (shown below) for the pitch type. It will be called only after
 the <code>cluster_pitches</code> method has been called.</p><p class="new">It is used like this:</p><pre><code>p42 = "-1.5355167705159802,55.0975543168299,7.04587405132267,93.6,5.4,2588.0".split(',')
p42 = [float(v) for v in p42]

# testing the classifier
robo_batter.cluster_pitches()
pitch_type = robo_batter.predict_pitch(p42)
print(pitch_type == 'FF')</code></pre><p class="new">You can use the following abbreviations and knowledge of the distribution of
 pitch types (to help identify your cluster labels).</p><table><thead><tr><th>Pitch Type</th><th>Abbreviation</th><th>Count</th></tr></thead><tbody><tr><td>Fast Balls</td><td>FF</td><td>1617</td></tr><tr><td>Sliders</td><td>SL</td><td>929</td></tr><tr><td>Curve Balls</td><td>CU</td><td>560</td></tr><tr><td>Change Ups</td><td>CH</td><td>136</td></tr></tbody></table><h2 id="-before-testing-andor-submitting-">‚ö†Ô∏è Before Testing and/or Submitting ‚ö†Ô∏è</h2><p class="new">There's a few long running functions in this lesson.  You will have to
 comment them out before testing or submitting -- otherwise the auto-tester
 will time out.</p><ul><li>any calls to <code>get_silhouette_scores(points)</code> or <code>get_inertias(points)</code></li><li>any code you added that uses a significant amount of time (&gt;10 seconds) to
complete.</li></ul><h1>Test and Submit</h1><p>Once you have finished, you can download your code (via <code>ide.tester</code>) and upload that file to Gradescope (find lesson with tag <strong>clustering</strong>).</p><div class="my-4"><pre><code><strong># to list the tests available</strong><br/>print(ide.tester.list_tests())<br/><strong># to perform a specific test</strong><br/>print(ide.tester.test_functionality('name of test'))<br/><strong># to test your code (either works)</strong><br/>print(ide.tester.test_notebook())<br/>print(ide.tester.test_notebook(verbose=True))<br/><strong># to prepare and download your code</strong><br/>ide.tester.download_solution()</code></pre></div><div class="lesson-footer flex bg-gray-200 justify-center"><div class="lesson-footer-card displaycard bg-blue-200 border-t border-gray-400 max-w-2xl rounded overflow-hidden shadow-lg"><div class="px-6 py-4"><div class="title-text text-center font-bold text-xl">Clustering</div><p class="text-center text-gray-800 text-xl">finding patterns in a sea of data</p><div class="text-center mt-6 text-xl"><i aria-hidden="true" class="fas fa-tags"></i> any questions on Piazza with <span class="font-bold">clustering</span></div><div class="text-gray-700 text-base">¬†</div><div></div><div></div><div class="flex mt-4 border-t border-solid border-gray-500 justify-around bg-gray-200"><div class="text-gray-700 text-center px-4 m-2 text-sm">D.M. &amp; the üêç</div><div class="text-gray-700 text-center px-4 m-2 text-sm"><strong>Version:</strong> <!-- -->SP21</div></div><div class="text-gray-700 mt-2 text-center text-sm font-bold">All Rights Reserved Michael Haberman</div><div class="text-gray-700 text-center text-sm">Do not distribute this notebook</div></div></div></div><div>¬†</div><div class="ide code-starter clearfix"><pre><code># print(ide.tester.test_notebook()) 
# print(ide.tester.test_notebook(verbose=True)) 

# once you are ready -- run this 
# ide.tester.download_solution() 
</code></pre></div></div></div></body></html>