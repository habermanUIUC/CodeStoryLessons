{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data, Machines and the üêç \n",
    "<img src=\"https://raw.githubusercontent.com/habermanUIUC/CodeStoryLessons/main/lessons/dmap/text/word-embeddings/html/section00.png\" align=\"left\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"install\"></a>\n",
    "## Notebook Preparation for Lesson 1‚Ä¢2‚Ä¢3\n",
    "Each lesson will start with a similar template (given in the course schedule):  \n",
    "1. **save** to your google drive (copy to drive)<br/><img src=\"https://raw.githubusercontent.com/habermanUIUC/CodeStoryLessons/main/assets/images/colab/copy-to-drive.png\"/>\n",
    "2. **update** the NET_ID to be your netID (no need to include @illinois.edu)\n",
    "3. **run** the next cell to install the IDE. <img src=\"https://raw.githubusercontent.com/habermanUIUC/CodeStoryLessons/main/assets/images/colab/play-button.png\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "LESSON_ID = 'dmap:text:word-embeddings'   # keep this as is\n",
    "NET_ID    = 'CHANGE_ME' # CHANGE_ME to your netID (keep the quotes)\n",
    "\n",
    "def install_ide(net_id, lesson_id):\n",
    "  import sys\n",
    "  if 'codestories' not in sys.modules:\n",
    "      print('installing modules')\n",
    "      !pip install git+https://mehaberman@bitbucket.org/mehaberman/codestories.git --upgrade &> install.log\n",
    "  \n",
    "  from codestories.cs.CodeStories import CodeStory\n",
    "  return CodeStory(net_id, lesson_id)\n",
    "\n",
    "ide = install_ide(NET_ID, LESSON_ID)\n",
    "print(ide.welcome())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson Word Embeddings\n",
    "(hit ‚ñ∂ to read the first part of the lessonÔ∏è)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run to read the next section\n",
    "ide.reader.view_section(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Words as Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run to read the next section\n",
    "ide.reader.view_section(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this cell will take a long time to run\n",
    "# you only need to run it once per session\n",
    "def load_model(use_large=False):\n",
    "    import LessonUtil as Util\n",
    "    import spacy\n",
    "    if spacy.__version__ != \"2.2.4\":\n",
    "        print(\"WARNING, spacy version may not have vectors\")\n",
    "    return Util.load_model(use_large)\n",
    "\n",
    "# keep this as is\n",
    "nlp = load_model(use_large=True)\n",
    "print('Total Words', len(nlp.vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run to read the next section\n",
    "ide.reader.view_section(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_demo(md):\n",
    "    # retrieve words from the English model vocabulary\n",
    "    cat = md.vocab['cat']\n",
    "\n",
    "    # print the dimension of word vectors\n",
    "    print('vector length:', len(cat.vector))\n",
    "\n",
    "    # print the word vector\n",
    "    print('cat:', cat.vector)\n",
    "  \n",
    "word_demo(nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run to read the next section\n",
    "ide.reader.view_section(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat = nlp.vocab['cat']\n",
    "dog = nlp.vocab['dog']\n",
    "car = nlp.vocab['car']\n",
    "print('The similarity between dog and dog:', dog.similarity(dog))\n",
    "print('The similarity between dog and car:', dog.similarity(car))\n",
    "print('The similarity between dog and cat:', dog.similarity(cat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run to read the next section\n",
    "ide.reader.view_section(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def spacy_sim(a,b):\n",
    "    return -1 \n",
    "\n",
    "print(spacy_sim(nlp.vocab['dog'], nlp.vocab['car']))\n",
    "# print(ide.tester.test_function(spacy_sim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run to read the next section\n",
    "ide.reader.view_section(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "def homemade_sim(a,b):\n",
    "    return np.dot(a, b)/(np.linalg.norm(a)*np.linalg.norm(b))\n",
    "\n",
    "def demo_vector_math(md):\n",
    "    man = md.vocab['father'].vector\n",
    "    woman = md.vocab['mother'].vector\n",
    "    d1 = man - woman\n",
    "\n",
    "    uncle = md.vocab['uncle'].vector\n",
    "    aunt = md.vocab['aunt'].vector\n",
    "    d2 = uncle - aunt\n",
    "\n",
    "    print('d1 and d2 close:', homemade_sim(d1, d2) > 0.70)\n",
    "\n",
    "demo_vector_math(nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run to read the next section\n",
    "ide.reader.view_section(12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_words_except(md, word):\n",
    "    # get all words in the vocabulary\n",
    "    # each must have a word vector\n",
    "    # don't include the exception\n",
    "    return [w for w in md.vocab if w.has_vector and w.is_lower and w.lower_ != word.lower_]  \n",
    "\n",
    "def most_similar(md, name, topn=10):\n",
    "\n",
    "    word = md.vocab[name]\n",
    "    \n",
    "    allwords = all_words_except(md, word)\n",
    "    \n",
    "    # sort words by similarity in descending order\n",
    "    out = sorted(allwords, key=lambda w: word.similarity(w), reverse=True)  \n",
    "    \n",
    "    # slicing to the rescue\n",
    "    return out[:topn]\n",
    "\n",
    "def demo_close_words(md, word):\n",
    "    neighbors = most_similar(md, word)\n",
    "    print([w.text for w in neighbors])\n",
    "\n",
    "# this can take a long time to run\n",
    "demo_close_words(nlp, 'car')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run to read the next section\n",
    "ide.reader.view_section(14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "cos = lambda v1, v2: np.dot(v1, v2)/(np.linalg.norm(v1) * np.linalg.norm(v2))\n",
    "def all_words_except(md, exclude_set):\n",
    "\n",
    "    # the valid function ensures we don't include any of the words \n",
    "    # that are part of the equation (input)\n",
    "    def valid(w, exclude):\n",
    "        if w.has_vector and w.is_lower:\n",
    "            for t in exclude:\n",
    "                if w.lower_.find(t) >= 0: \n",
    "                    return False\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    return [w for w in md.vocab if valid(w, exclude_set)]\n",
    "\n",
    "def find_closest(md, vector, exclude, topn=50):\n",
    "    working_set = all_words_except(md, exclude)\n",
    "    candidates = sorted(working_set, key=lambda w: cos(vector, w.vector), reverse=True)\n",
    "    return candidates[:topn]\n",
    "\n",
    "def demo_word_math(md):\n",
    "    vector = md.vocab['math'].vector + md.vocab['symbol'].vector\n",
    "    answer = find_closest(md, vector, ['math', 'symbol'])\n",
    "    print(answer[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run to read the next section\n",
    "ide.reader.view_section(16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# type&run the above example/exercise in this cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run to read the next section\n",
    "ide.reader.view_section(18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# type&run the above example/exercise in this cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run to read the next section\n",
    "ide.reader.view_section(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_dimensions(md, labels):\n",
    "    from sklearn.manifold import TSNE\n",
    "    import numpy as np\n",
    "  \n",
    "    data = np.array([md.vocab[w].vector for w in labels])\n",
    "    # reduce to two\n",
    "    tsne_model = TSNE(n_components=2)\n",
    "    data_2d = tsne_model.fit_transform(data)\n",
    "  \n",
    "    return data_2d\n",
    "\n",
    "def plot_results(data_2d, labels):\n",
    "    import matplotlib\n",
    "    import matplotlib.pyplot as plt\n",
    "  \n",
    "    # plot the 2d vectors and show their labels\n",
    "    fig, axes = plt.subplots()\n",
    "    axes.scatter(data_2d[:, 0], data_2d[:, 1], s=100)\n",
    "    for i, txt in enumerate(labels):\n",
    "        axes.annotate(txt, (data_2d[i,0], data_2d[i,1]), xytext=(2, 3), textcoords='offset points')\n",
    "    axes.grid()\n",
    "    return fig\n",
    "\n",
    "def show_vector_space(md):\n",
    "    labels = ['king', 'man', 'queen', 'woman']\n",
    "    data = reduce_dimensions(md, labels)\n",
    "    fig = plot_results(data, labels)\n",
    "\n",
    "show_vector_space(nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run to read the next section\n",
    "ide.reader.view_section(22)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One Hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run to read the next section\n",
    "ide.reader.view_section(23)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson Assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run to read the next section\n",
    "ide.reader.view_section(24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_fit(md, result, words, topn=15):\n",
    "    # md is the nlp model\n",
    "    # result is a vector you're trying to get close to\n",
    "    # words is a list of spacy objects/words (3 of them) that\n",
    "    # need to be excluded (e.g nlp.vocab['man'])\n",
    "  \n",
    "    # return the topn candidates, in order so index 0 is the best fit\n",
    "    # it should always have at least one word in it\n",
    "    return []\n",
    "    \n",
    "def find_analogy(md, three_words):\n",
    "     # md is the nlp model\n",
    "     # three_words is a list of strings (e.g. ['king', 'man', 'woman'])\n",
    "\n",
    "     # return the text (a string) for the word that meets the analogy \n",
    "     # it should use find_best_fit\n",
    "     return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run to read the next section\n",
    "ide.reader.view_section(26)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test and Submit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run to read the next section\n",
    "ide.reader.view_section(27)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(ide.tester.test_notebook()) \n",
    "# print(ide.tester.test_notebook(verbose=True)) \n",
    "\n",
    "# once you are ready -- run this \n",
    "# ide.tester.download_solution()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Word Embeddings",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language": "python",
  "story": {
   "auth_token": "BmuLglRCIBDvPTa3qLjdradpaRqgyvXacbx0QH9YZ2U=",
   "authorship_tag": "AB",
   "chapters": 29,
   "name": "Word Embeddings",
   "parser": {},
   "root": "https://raw.githubusercontent.com/habermanUIUC/CodeStoryLessons/main/lessons",
   "tag": "dmap:text:word-embeddings"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
