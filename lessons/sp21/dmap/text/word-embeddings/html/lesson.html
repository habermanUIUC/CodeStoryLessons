<!DOCTYPE html><html lang='en'><head><title>Word Embeddings</title><meta charset="utf-8"><style>/*! normalize.css v8.0.1 | MIT License | github.com/necolas/normalize.css */html{line-height:1.15;-webkit-text-size-adjust:100%}body{margin:0}h1{font-size:2em;margin:.67em 0}pre{font-family:monospace,monospace;font-size:1em}a{background-color:transparent}strong{font-weight:bolder}code{font-family:monospace,monospace;font-size:1em}img{border-style:none}input{font-family:inherit;font-size:100%;line-height:1.15;margin:0}input{overflow:visible}[type=checkbox]{box-sizing:border-box;padding:0}::-webkit-file-upload-button{-webkit-appearance:button;font:inherit}h1,h2,h3,h4,p,pre{margin:0}ul{list-style:none;margin:0;padding:0}html{font-family:system-ui,-apple-system,BlinkMacSystemFont,"Segoe UI",Roboto,"Helvetica Neue",Arial,"Noto Sans",sans-serif,"Apple Color Emoji","Segoe UI Emoji","Segoe UI Symbol","Noto Color Emoji";line-height:1.5}*,::after,::before{box-sizing:border-box;border-width:0;border-style:solid;border-color:#e2e8f0}img{border-style:solid}input:-ms-input-placeholder{color:#a0aec0}input::-ms-input-placeholder{color:#a0aec0}input::-moz-placeholder{color:#a0aec0}h1,h2,h3,h4{font-size:inherit;font-weight:inherit}a{color:inherit;text-decoration:inherit}input{padding:0;line-height:inherit;color:inherit}code,pre{font-family:Menlo,Monaco,Consolas,"Liberation Mono","Courier New",monospace}img{display:block;vertical-align:middle}img{max-width:100%;height:auto}.container{width:100%}@media (min-width:640px){.container{max-width:640px}}@media (min-width:768px){.container{max-width:768px}}@media (min-width:1024px){.container{max-width:1024px}}@media (min-width:1280px){.container{max-width:1280px}}.bg-gray-200{--bg-opacity:1;background-color:#edf2f7;background-color:rgba(237,242,247,var(--bg-opacity))}.bg-gray-300{--bg-opacity:1;background-color:#e2e8f0;background-color:rgba(226,232,240,var(--bg-opacity))}.bg-orange-300{--bg-opacity:1;background-color:#fbd38d;background-color:rgba(251,211,141,var(--bg-opacity))}.bg-green-200{--bg-opacity:1;background-color:#c6f6d5;background-color:rgba(198,246,213,var(--bg-opacity))}.bg-blue-200{--bg-opacity:1;background-color:#bee3f8;background-color:rgba(190,227,248,var(--bg-opacity))}.bg-blue-300{--bg-opacity:1;background-color:#90cdf4;background-color:rgba(144,205,244,var(--bg-opacity))}.border-gray-400{--border-opacity:1;border-color:#cbd5e0;border-color:rgba(203,213,224,var(--border-opacity))}.border-gray-500{--border-opacity:1;border-color:#a0aec0;border-color:rgba(160,174,192,var(--border-opacity))}.border-indigo-500{--border-opacity:1;border-color:#667eea;border-color:rgba(102,126,234,var(--border-opacity))}.rounded{border-radius:.25rem}.rounded-full{border-radius:9999px}.border-solid{border-style:solid}.border{border-width:1px}.border-l-2{border-left-width:2px}.border-t{border-top-width:1px}.cursor-pointer{cursor:pointer}.block{display:block}.inline-block{display:inline-block}.flex{display:flex}.justify-center{justify-content:center}.justify-around{justify-content:space-around}.float-left{float:left}.clearfix:after{content:"";display:table;clear:both}.font-sans{font-family:system-ui,-apple-system,BlinkMacSystemFont,"Segoe UI",Roboto,"Helvetica Neue",Arial,"Noto Sans",sans-serif,"Apple Color Emoji","Segoe UI Emoji","Segoe UI Symbol","Noto Color Emoji"}.font-serif{font-family:Georgia,Cambria,"Times New Roman",Times,serif}.font-semibold{font-weight:600}.font-bold{font-weight:700}.h-64{height:16rem}.text-xs{font-size:.75rem}.text-sm{font-size:.875rem}.text-base{font-size:1rem}.text-xl{font-size:1.25rem}.leading-none{line-height:1}.leading-normal{line-height:1.5}.m-2{margin:.5rem}.my-4{margin-top:1rem;margin-bottom:1rem}.mt-1{margin-top:.25rem}.mt-2{margin-top:.5rem}.mr-2{margin-right:.5rem}.mb-2{margin-bottom:.5rem}.mr-3{margin-right:.75rem}.mb-3{margin-bottom:.75rem}.mt-4{margin-top:1rem}.mb-4{margin-bottom:1rem}.mt-6{margin-top:1.5rem}.max-w-sm{max-width:24rem}.max-w-2xl{max-width:42rem}.object-contain{-o-object-fit:contain;object-fit:contain}.opacity-0{opacity:0}.overflow-hidden{overflow:hidden}.p-1{padding:.25rem}.p-3{padding:.75rem}.py-1{padding-top:.25rem;padding-bottom:.25rem}.py-2{padding-top:.5rem;padding-bottom:.5rem}.px-3{padding-left:.75rem;padding-right:.75rem}.py-4{padding-top:1rem;padding-bottom:1rem}.px-4{padding-left:1rem;padding-right:1rem}.px-6{padding-left:1.5rem;padding-right:1.5rem}.pl-3{padding-left:.75rem}.absolute{position:absolute}.shadow-md{box-shadow:0 4px 6px -1px rgba(0,0,0,.1),0 2px 4px -1px rgba(0,0,0,.06)}.shadow-lg{box-shadow:0 10px 15px -3px rgba(0,0,0,.1),0 4px 6px -2px rgba(0,0,0,.05)}.shadow-inner{box-shadow:inset 0 2px 4px 0 rgba(0,0,0,.06)}.text-left{text-align:left}.text-center{text-align:center}.text-black{--text-opacity:1;color:#000;color:rgba(0,0,0,var(--text-opacity))}.text-gray-700{--text-opacity:1;color:#4a5568;color:rgba(74,85,104,var(--text-opacity))}.text-gray-800{--text-opacity:1;color:#2d3748;color:rgba(45,55,72,var(--text-opacity))}.whitespace-no-wrap{white-space:nowrap}.w-1\/2{width:50%}.w-2\/3{width:66.666667%}.w-full{width:100%}@media (min-width:768px){.md\:w-2\/3{width:66.666667%}}.text-tiny{font-size:.5rem!important}body{color:#000!important;font-size:1.25rem!important}.main-content{max-width:900px}.lesson{padding-left:15px!important;padding-right:10px!important;--bg-opacity:1;background-color:#edf2f7;background-color:rgba(237,242,247,var(--bg-opacity))}.main-content,html{font-family:Arial,Georgia,Verdana,"Times New Roman"!important}.lesson-footer-card,.lesson-overview-card{font-family:"Times New Roman"!important}.lesson-footer{margin-top:50px;margin-top:20px}.lesson ul{list-style-position:inside;list-style-type:none;margin-left:1em}.lesson ul li{padding-left:1em;padding-right:5px}.lesson ul li::before{content:"‚Ä¢";padding-right:5px}span{white-space:nowrap}p.new{padding-top:0;padding-bottom:.5em}p.new+p{padding-top:.5em}h1,h2,h3,h4{font-weight:700;margin-top:.25em!important;margin-bottom:.05em!important;font-family:Georgia,Cambria,"Times New Roman",Times,serif!important}h1{font-size:2em!important;clear:both;color:#000!important}div+h1,h2{margin-top:0!important}h2{margin-top:.5em!important;font-size:1.5em!important;clear:both;color:#8b0000!important}h3{font-size:1.25em!important;clear:both;color:#006400!important}h4{font-size:1em!important;clear:both;color:#00008b!important}ul{margin-bottom:30px}p.new a{text-decoration:underline}.lesson a{text-decoration:underline;color:#00f}.title-text{font-size:2rem}.shadow-lg{box-shadow:0 10px 15px -3px rgba(0,0,0,.1),0 4px 6px -2px rgba(0,0,0,.5)}img.formula-block{margin-left:auto;margin-right:auto;margin-top:.25rem;margin-bottom:.75rem}img.small{width:33.333333%;margin-left:1.25rem}img.border{border:1px solid #021a40;margin-top:.5rem;margin-bottom:.75rem}code{font-size:smaller}pre code{font-size:15px}pre code:not(.line-number){background:#f4f4f4;font-family:monospace;font-size:15px;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;cursor:default;touch-action:none;-webkit-touch-callout:none;-webkit-tap-highlight-color:transparent;clear:both;border:1px solid #ddd;color:#666;page-break-inside:avoid;display:block;min-width:840px;max-width:840px;overflow:scroll;line-height:1.6;margin-bottom:1.6em;padding:1em 1.5em;-moz-tab-size:2;-o-tab-size:2;tab-size:2;word-wrap:break-word;white-space:pre-wrap;border-left:3px solid #f36d33}div.code-starter>pre code{border-left:3px solid #fdff44!important;background-image:radial-gradient(rgba(0,150,0,.75),#000 120%);color:#fff;font:.9rem Inconsolata,monospace}div.code-starter>pre code::after{content:"\a$_"}.tab{font-size:1rem;border-color:#8c6728}.tab-content{max-height:0;max-width:100%;transition:max-height .35s}.tab input:checked~.tab-content{max-height:100vh}.tab input:checked+label{padding:1rem;border-left-width:2px;border-color:#6574cd;background-color:#f8fafc;color:#6574cd}.tab label::after{float:right;right:0;top:0;display:block;width:1em;height:1.5em;line-height:1.5;font-size:1rem;text-align:center;transition:all .35s}.tab input[type=checkbox]+label::after{content:"+";font-weight:700;border-width:1px;border-radius:9999px;border-color:#8c6728}.tab input[type=checkbox]:checked+label::after{transform:rotate(315deg);background-color:#6574cd;color:#f8fafc}</style>
<script src="https://kit.fontawesome.com/7efc4bcee2.js" crossOrigin="anonymous"></script>
<script>
    let stateCheck = setInterval(function(){
      if (document.readyState === 'complete') {
        clearInterval(stateCheck);
        let s1 = document.getElementById('start');
        // console.log('doc is ready', s1);
        if (s1) {
           s1.setAttribute('tabindex', '-1');
           s1.focus(); 
           s1.scrollIntoView({behavior: 'smooth'}); 
           setTimeout(function(){s1.blur()}, 500);
           // console.log('focus set');
        }
      }
    }, 200);
    </script>
</head><body class="lesson"><div class="main-content lesson bg-gray-200 text-black p-1 pl-3 font-serif"><div class="md-inner">
<div id="start" class="section">&nbsp;</div><h1 class="overview"></h1><div class="lesson-overview bg-gray-200 flex justify-center"><div class="text-center px-4 py-2 m-2"><div class="lesson-overview-card displaycard bg-blue-200 max-w-sm rounded overflow-hidden shadow-lg"><div>¬†</div><img alt="Text" class="object-contain h-64 w-full" src="https://raw.githubusercontent.com/habermanUIUC/CodeStoryLessons/main/lessons/dmap/text/word-embeddings/html/TextTripping-sm.png"/><div class="px-6 py-4"><div class="title-text text-center leading-none font-bold text-xl">Word Embeddings</div><p class="text-center mt-2 text-gray-800 text-xl">text by numbers</p><div class="text-gray-700 text-base">¬†</div><div class="text-center mb-3"><span class="inline-block bg-gray-300 rounded-full px-3 py-1 text-sm font-semibold text-gray-700 mr-2">#text</span><span class="inline-block bg-gray-300 rounded-full px-3 py-1 text-sm font-semibold text-gray-700 mr-2">#spacy</span></div><div class="flex border-t border-solid border-gray-500 shadow-inner justify-around bg-blue-300"><div class="text-gray-700 text-center px-4 m-2 text-sm"><span class="whitespace-no-wrap">D.M. &amp; the üêç</span></div><div class="text-gray-700 text-center px-4 m-2 text-sm"><span class="whitespace-no-wrap"><strong>Version:</strong> <!-- -->SP21</span></div></div><div class="text-gray-700 mt-1 text-center text-tiny">All Rights Reserved</div></div></div></div><div class="text-center px-4 py-2 m-2 w-1/2"><div class="displaycard bg-gray-200 max-w-sm rounded overflow-hidden shadow-lg"><div class="px-6 py-4 text-left"><div class="text-center font-bold text-xl">Word Embeddings<br/><div><span>prerequisites</span><div class="text-center text-xs mb-2">(start only after finishing)</div><p class="max-w-sm text-gray-800 text-sm">‚¶ø <strong>info490 NLP</strong></p><p class="max-w-sm text-gray-800 text-sm">‚¶ø <strong>tfidf</strong></p></div></div></div><div class="px-6 py-4 text-left text-gray-800"><div class="text-center font-bold text-xl">Colab Notes</div><p class="max-w-sm text-sm">1. <strong>Copy</strong> this notebook <img alt="copy2drive.png" class="inline-block" src="https://raw.githubusercontent.com/habermanUIUC/CodeStoryLessons/main/lessons/dmap/text/word-embeddings/html/copy2drive.png"/></p><p class="max-w-sm text-sm">2. <strong>Update</strong> the <strong><code>NET_ID</code></strong> in the notebook</p><p class="max-w-sm text-gray-800 text-sm">3. <strong>Hit ‚ñ∂Ô∏è¬†</strong> to install the INFO 490 IDE</p><div class="text-center font-bold text-xl">¬†</div><div class="text-center font-bold text-xl">Jupyter/PyCharm Notes</div><p class="max-w-sm text-gray-800 text-sm text-left">The testing framework does <strong>not work</strong> (at this time) for Jupyter  notebooks or local code development.</p></div></div></div></div><h1 class="section" id="section1">Word Embeddings</h1><p class="new">As we first saw in the tf‚Ä¢idf lesson, in order for a computer to work with
text, we needed to first convert the text/words into a numerical representation. 
The algorithm for tf‚Ä¢idf is based on word frequency counts. However, coming up 
with a meaningful 'number' that captures the semantic difference between
 words is difficult.  </p><h2 id="edit-distance">Edit Distance</h2><p class="new">As an example of using a numeric value to signal the difference between words
 is spellcheckers. For example, they can use
<img alt="editDistance.png" class="border small float-left mr-3" src="https://raw.githubusercontent.com/habermanUIUC/CodeStoryLessons/main/lessons/dmap/text/word-embeddings/html/editDistance.png"/> 'edit' distance between two words to determine how similar they are.  The edit 
 distance is how many modifications (letter updates, additions, subtractions) are needed 
 to get from one word to the other.<br/> This allows one to offer possible corrections for a misspelled word. For example, 
 each of the following pairs have a 1 edit distance: </p><ul><li>capital, capitol (change 'a' to 'o')</li><li>war, warm (add an 'm')</li><li>brand, band (drop the 'r')</li></ul><p class="new">Of course, edit distance is incapable of capturing meaning between words (the
 edit distance of 4 between <em>warm</em> and <em>mild</em> is meaningless
 in trying to evaluate the distance in the semantics).</p><h2 id="word-embeddings-word-vectors">Word Embeddings, Word Vectors</h2><p class="new">A majority of NLP (natural language processing) techniques attempt to make
sense of (understand and derive meaning) text.  In order to do this, an
 important processing step is to convert each word into a representation that
  computers and algorithms can use.</p><p class="new">The result of converting a word/token into a number or list of numbers is a 
<strong>word embedding</strong>.  Each word is represented by a vector of real numbers
 (e.g. floats) -- the interpretation of those numbers depends on the model or
  algorithm used. In most cases, those numbers are not intuitive or readily
   accessible.</p><p class="new">  As an example the following vector may represent the word 'dog' with 4 attributes.</p><img alt="wordVector.png" class="border mb-3" src="https://raw.githubusercontent.com/habermanUIUC/CodeStoryLessons/main/lessons/dmap/text/word-embeddings/html/wordVector.png"/><p class="new">Word vectors are synonymous with word embeddings. You may also see the word 'row vector' 
to describe a word embedding. Additionally these vectors can be called 
a <em>k-dimensional</em> vector or a vector with k columns, components, attributes, 
features, traits, variables, or dimensions.</p><p class="new">For most algorithms that produce word embeddings, the actual meaning or
 interpretation of the columns (i.e. attributes) is unknown.  This is
  especially true of many machine learning (ML) algorithms that produce 
  a model or representation that best 'fits' its data.  The numbers/vectors, etc. that 
  are generated cannot be interpreted in any meaningful way.  It's one of the 
  reasons why machine learning models in many situations are described as a black box. </p><p class="new">For example, below are two word vectors for dog and car:
<img alt="dogCarVector.png" class="border" src="https://raw.githubusercontent.com/habermanUIUC/CodeStoryLessons/main/lessons/dmap/text/word-embeddings/html/dogCarVector.png"/></p><p class="new">In this case the meaning of the columns (generated from an ML algorithm) could 
be related to the concepts of 'fast', 'pet', 'animal', 'transportation', but 
we really have no way of knowing. </p><h1 class="section" id="section2">Words as Vectors</h1><p class="new">Representing words as vectors of numbers opens up the world of linear algebra
 (working with vectors and matrices) to working with text. We won't necessarily
 require you to know linear algebra (we'll try to introduce the 
 concepts as needed), but if you want to know why the underlying math works, 
 taking a course in linear algebra will add to your data science expertise.</p><p class="new">The following are a few good (and free) references: </p><pre><code> ‚Ä¢ http://joshua.smcvt.edu/linearalgebra/book.pdf
 ‚Ä¢ https://www.deeplearningbook.org/contents/linear_algebra.html</code></pre><h2 id="spacy-word-vectors">Spacy Word Vectors</h2><p class="new">The NLP package <a href="https://spacy.io/" target="_blank">spacy</a> includes the ability to work with
 word vectors.  In order to use spacy, you will need to download and install
  a language model -- a model trained using a certain corpus (like a news
   feed, Wikipedia, etc).  The following can take a few minutes to run. You
    will need to do so each time you start a session.</p><p class="new">The code cell below installs the spacy models, for the most part all the examples
 work with either the <code>en_core_web_md</code> (100MB of data) or <code>en_core_web_lg</code> (850MB).
 If you are low on patience, you can set <code>use_large=False</code>.</p><div class="ide code-starter clearfix"><pre><code># this cell will take a long time to run
# you only need to run it once per session
def load_model(use_large=False):
    import LessonUtil as Util
    import spacy
    if spacy.__version__ != "2.2.4":
        print("WARNING, spacy version may not have vectors")
    return Util.load_model(use_large)

# keep this as is
nlp = load_model(use_large=True)
print('Total Words', len(nlp.vocab))</code></pre></div><p class="new">The function <code>load_model</code> just hands off the task to the <code>LessonUtil</code> module.
It essentially does the following:</p><pre><code>    import spacy                   # not always needed
    import en_core_web_md as model # the medium size model is fast
    model.load()                   # this takes a long time</code></pre><p class="new">Once the model is loaded, we can do some simple things with word vectors:</p><div class="ide code-starter clearfix"><pre><code>def word_demo(md):
    # retrieve words from the English model vocabulary
    cat = md.vocab['cat']

    # print the dimension of word vectors
    print('vector length:', len(cat.vector))

    # print the word vector
    print('cat:', cat.vector)
  
word_demo(nlp)</code></pre></div><h2 id="vector-dimensions">Vector Dimensions</h2><p class="new">Did you notice what <code>cat.vector</code> printed?  That's 300 dimensions that
 contribute to the meaning of the word cat.  Each word in the spacy model has
 300 attributes.  We really don't know the meaning of each of these
 attributes, but the machine learning algorithm trained the model to output
 300 features for each word.</p><h2 id="distance-between-words-the-reprise">Distance Between Words (the reprise)</h2><p class="new">Now that we have vectors for words, we can use linear algebra and vector space models to analyze the relationship between words.
In many data science and machine learning tasks, distance between two
 items can be used to indicate similarity or to evaluate the 'cost' or 'fit' of
 a model.  </p><p class="new">Although we will have a separate lesson on distance metrics, as we saw in 
the tf‚Ä¢idf lesson, one of the main metrics for working with vectors is cosine
 similarity. This metric relies on normalizing the two input vectors so that
 longer vectors (in magnitude) don't over influence the calculation 
(e.g. just because a document contains more words doesn't make it more important).</p><p class="new">We can visualize cosine similarity (the value of ùõ≥ in the image below) to show
the distance between to words (the value d, represents euclidean distance).
<img alt="eucos.png" class="ma-4 mt-6 border small" src="https://raw.githubusercontent.com/habermanUIUC/CodeStoryLessons/main/lessons/dmap/text/word-embeddings/html/eucos.png"/></p><p class="new">Using the cosine distance formula: <br/><img alt="math?math=%5CLarge%20cosine%5C_similarity(A%2C%20B)%20%3D%20%5Cfrac%7BA%20%5Ccdot%20B%7D%7B%5Cleft%20%5C%7C%20A%20%5Cright%20%5C%7C%5Cleft%20%5C%7C%20B%20%5Cright%20%5C%7C%7D" class=" formula-block" src="https://render.githubusercontent.com/render/math?math=%5CLarge%20cosine%5C_similarity(A%2C%20B)%20%3D%20%5Cfrac%7BA%20%5Ccdot%20B%7D%7B%5Cleft%20%5C%7C%20A%20%5Cright%20%5C%7C%5Cleft%20%5C%7C%20B%20%5Cright%20%5C%7C%7D"/><br/>We can also calculate it using numpy (assuming both A and B are defined):</p><pre><code>import numpy as np
cosine_A_B = np.dot(A, B)/(np.linalg.norm(A) * np.linalg.norm(B))</code></pre><p class="new">As we saw in the tf‚Ä¢idf lessons, the L2 norm (<code>np.linalg.norm</code>) is just the square
 root of the sum of the squared components:</p><pre><code>def simple_l2(vector):
    ssq = 0
    for v in vector:
        ssq += v * v 
    return np.sqrt(ssq)

def show_dog_norms(md):
    dog = md.vocab['dog']
    print(np.linalg.norm(dog.vector))
    print(dog.vector_norm)
    print(simple_l2(dog.vector))

show_dog_norms(nlp)</code></pre><p class="new">However, the spacy <code>similarity</code> method does this calculation:</p><div class="ide code-starter clearfix"><pre><code>cat = nlp.vocab['cat']
dog = nlp.vocab['dog']
car = nlp.vocab['car']
print('The similarity between dog and dog:', dog.similarity(dog))
print('The similarity between dog and car:', dog.similarity(car))
print('The similarity between dog and cat:', dog.similarity(cat))</code></pre></div><h3 id="exercise-">Exercise </h3><p class="new">Implement the function <code>spacy_sim</code> which uses numpy to calculate the
cosine similarity between the two spacy tokens. You can use either 
<code>np.linalg.norm</code> or <code>&lt;spacy_word&gt;.vector_norm</code></p><div class="ide code-starter clearfix"><pre><code>import numpy as np
def spacy_sim(a,b):
    return -1 

print(spacy_sim(nlp.vocab['dog'], nlp.vocab['car']))
# print(ide.tester.test_function(spacy_sim))</code></pre></div><h4 id="before-you-test-">Before you test ...</h4><p class="new">Since the models have to be loaded into the auto-grader, each test will take 
about 30 seconds to a minute.  You should make sure to write your own tests
 first before using the auto-grader.</p><pre><code># to get a list of tests for this lesson
print(ide.tester.list_tests())</code></pre><h2 id="the-meaning-of-similar">The Meaning of Similar</h2><p class="new">As noted before, the range of sim will be from -1 to +1.  A value of 1 is a
 perfect match (same word), but a -1 does not mean 'opposite'. 
Word embeddings do not capture antonyms. 
The vectors captures the word-context from the trained corpus. 
So it highly depends on what corpus was used to build the word vectors. 
The cosine similarly of antonyms are generally high. 
The cosine similarity of (fast,slow) (high, low) should be quite high 
because they (most likely) occur in the same context.</p><p class="new">So similarity is co-occurrence and dissimilarity means the words do not appear 
within the same context (usually).</p><pre><code>cold = nlp.vocab['cold']
hot = nlp.vocab['hot']
print(spacy_sim(hot, cold))</code></pre><p class="new">What do you notice?
</p><h2 id="word-math">Word Math</h2><p class="new">With word vectors you can do simple math with them to determine if the
 'difference' between pairs of words have similar meaning. </p><p class="new">In the next example, we show that the difference between <strong>father</strong> and <strong>mother</strong> 
is very close to the difference between <strong>uncle</strong> and <strong>aunt</strong></p><ul><li>d1 is a vector that represents the difference between father and mother</li><li>d2 is a vector that represents the difference between uncle and aunt</li><li>if d1 and d2 are close in vector space (e.g &gt; 0.70), they are similar</li></ul><div class="ide code-starter clearfix"><pre><code>def homemade_sim(a,b):
    return np.dot(a, b)/(np.linalg.norm(a)*np.linalg.norm(b))

def demo_vector_math(md):
    man = md.vocab['father'].vector
    woman = md.vocab['mother'].vector
    d1 = man - woman

    uncle = md.vocab['uncle'].vector
    aunt = md.vocab['aunt'].vector
    d2 = uncle - aunt

    print('d1 and d2 close:', homemade_sim(d1, d2) &gt; 0.70)

demo_vector_math(nlp)</code></pre></div><h2 id="finding-similar-words">Finding Similar Words</h2><p class="new">Using the same math, we can build upon that to find all the words that are
 closest to a specific word.</p><div class="ide code-starter clearfix"><pre><code>def all_words_except(md, word):
    # get all words in the vocabulary
    # each must have a word vector
    # don't include the exception
    return [w for w in md.vocab if w.has_vector and w.is_lower and w.lower_ != word.lower_]  

def most_similar(md, name, topn=10):

    word = md.vocab[name]
    
    allwords = all_words_except(md, word)
    
    # sort words by similarity in descending order
    out = sorted(allwords, key=lambda w: word.similarity(w), reverse=True)  
    
    # slicing to the rescue
    return out[:topn]

def demo_close_words(md, word):
    neighbors = most_similar(md, word)
    print([w.text for w in neighbors])

# this can take a long time to run
demo_close_words(nlp, 'car')</code></pre></div><p class="new">The above code can take a long time to run since we are looking at <strong>all</strong> the
 words in the corpus (see <code>all_words_except</code>).</p><p class="new">Be sure you understand the process that is being shown. What's so interesting
 (exciting!!) is that the algorithm used to create these vectors had only 
the text as its input.  No dictionary, thesaurus, or experts were used. 
In the next lesson, we will use some of these algorithms.</p><h2 id="model-matters">Model Matters</h2><p class="new">The words you get back will depend on which spacy model you are using. 
If you aren't getting the results you expect, it could be that the model 
you are using isn't trained on the same data you have or it's not suited for your task.</p><h2 id="word-play">Word Play</h2><p class="new">Perhaps 'coolest' feature of using linear algebra on word vectors is we can
 use this math to find word analogies.</p><p class="new">For example, let's fill in the blank:</p><p class="new"><strong>Math</strong> + <strong>Symbols</strong> is ‚ùì‚ùì‚ùì‚ùì</p><p class="new">We can use the similarity functionality we just build to find the word.</p><div class="ide code-starter clearfix"><pre><code>cos = lambda v1, v2: np.dot(v1, v2)/(np.linalg.norm(v1) * np.linalg.norm(v2))
def all_words_except(md, exclude_set):

    # the valid function ensures we don't include any of the words 
    # that are part of the equation (input)
    def valid(w, exclude):
        if w.has_vector and w.is_lower:
            for t in exclude:
                if w.lower_.find(t) &gt;= 0: 
                    return False
            return True
        return False

    return [w for w in md.vocab if valid(w, exclude_set)]

def find_closest(md, vector, exclude, topn=50):
    working_set = all_words_except(md, exclude)
    candidates = sorted(working_set, key=lambda w: cos(vector, w.vector), reverse=True)
    return candidates[:topn]

def demo_word_math(md):
    vector = md.vocab['math'].vector + md.vocab['symbol'].vector
    answer = find_closest(md, vector, ['math', 'symbol'])
    print(answer[0].text)</code></pre></div><p class="new">Be sure to take the time to understand what is happening when you call <code>demo_word_math()</code>:
The big issue is that it takes a long time to run these since <code>all_words_except</code> is looking at all the words in the vocabulary.
Also, it's important to remove the words math, symbol from the vocabulary, since those words will match perfectly.</p><p class="new">The function <code>valid</code> also removes words that begin with 'math' (e.g
 mathematics, etc).  It's not a perfect solution, but it's enough for the demonstration.</p><pre><code>demo_word_math(nlp)</code></pre><p class="new">What did you get ?</p><p class="new">Does it make sense for <strong>Math</strong> + <strong>Symbols</strong> is ‚ùì‚ùì‚ùì‚ùì</p><p class="new">Depending on the data used to build the model (tweeter, news, Wikipedia, etc) 
you may have to adjust your results. For example, using the <code>en_core_web_md</code> 
model gives different results.</p><h2 id="-the-most-famous-word-analogy-of-all-">‚ô™‚ô´ The Most Famous Word Analogy of All ‚ô¨‚ô¨</h2><img alt="kmwBlur.png" class="border small float-left mb-3 mr-3" src="https://raw.githubusercontent.com/habermanUIUC/CodeStoryLessons/main/lessons/dmap/text/word-embeddings/html/kmwBlur.png"/><p class="new">One of the popular examples of math with word vectors is to solve this equation:</p><p class="new"><strong>Man</strong> is to <strong>Woman</strong> as <strong>King</strong> is to ‚ùì‚ùì‚ùì‚ùì</p><p class="new">Before reading on, grab some paper and see if you can come up with the math
 (just addition and subtraction) to solve it (it's fun!).</p><h3 id="-"> </h3><div class="font-sans container mt-1 mb-4 "><div></div><div class="w-2/3 md:w-2/3"><div class="shadow-md"><div class="tab overflow-hidden border-t bg-green-200"><input class="absolute opacity-0" id="tab-multi-1" name="tabs" type="checkbox"/><label class="block p-3 leading-normal cursor-pointer" for="tab-multi-1"><span> This can be solved using the vector math: </span></label><div class="tab-content overflow-hidden border-l-2 bg-orange-300 border-indigo-500 leading-normal"><p class="p-3"><span><code>result = king - man + woman</code></span></p></div></div></div></div></div><h3 id="dont-go-past-">Don't Go Past </h3><p class="new">Make sure you understand the logic behind that vector math.  It's like saying
take the difference between ('woman' - 'man') and add that to 'king'. </p><p class="new">Let's use our vector math and code this analogy up:</p><pre><code>def king_is_to_what(md):
    vector = md.vocab['king'].vector - md.vocab['man'].vector + md.vocab['woman'].vector
    answer = find_closest(md, vector, ['king', 'man', 'woman'])
    print(answer[0].text)
  
king_is_to_what(nlp)</code></pre><div class="ide code-starter clearfix"><pre><code></code></pre></div><p class="new">Be sure to code this up and run it (it's also tested).</p><h2 id="ill-have-some-with-that-">I'll have some üçü‚ùìwith that. </h2><p class="new">Most of the world knows about french fries, but other countries may have
 another word (or similar food) to them. Let's use this machinery to help us 
out in case we go to Canada on vacation.</p><p class="new">Do Canadians even eat fries ?</p><p class="new">To find the equivalent of US fries in Canada, set up the following equation:</p><pre><code>def where_are_my_fries(md):
    vector = md.vocab['canada'].vector - md.vocab['usa'].vector + md.vocab['fries'].vector
    ex = ['canad', 'us', 'frie']
    answer = find_closest(md, vector, ex)
    print([a.text for a in answer[0:10]])

where_are_my_fries(nlp)</code></pre><div class="ide code-starter clearfix"><pre><code></code></pre></div><h2 id="-really-">üçî Really ?</h2><p class="new">Luckily, we know burgers seems a bit strange. Much like tweaking with stopwords, 
we can decide to remove some words that might have been affected by the training.
 In most situations, this isn't a scalable solution, but we're hungry and we 
want Canadian fries. </p><p class="new">In order to narrow down the results, you need to remove synonyms for fries
 (potatoes) and remove any reference to hamburgers (probably due to the training data):</p><pre><code>ex = ['canad', 'us', 'frie', 'bacon', 'potato', 'burger', 'steak']</code></pre><p class="new">Add those to the exceptions list above and re-run your code.</p><p class="new">If you inspect the top 10, you will find poutine, cheese, and gravy (the
 Canadian delicacy)! Although the need to tweak the results, shows the
 importance of data used to build the word vectors.</p><h2 id="visualizing-word-vectors">Visualizing Word Vectors</h2><img alt="king.png" class="border small float-left mr-3" src="https://raw.githubusercontent.com/habermanUIUC/CodeStoryLessons/main/lessons/dmap/text/word-embeddings/html/king.png"/><p class="new">Since each word vector contains hundreds of dimensions (300 for spacy), it's
impossible to visualize without projecting the high dimensions into lower
 dimensional space (i.e. 2 or 3).  </p><p class="new">Although we will have a separate lesson on how to transform a data-set with many
 dimensions to one with only a few of its 'most important' dimensions, we will 
use something called t-Distributed Stochastic Neighbor Embedding (t-SNE for
 short). The t-SNE algorithm is particularly well suited for the visualization of 
high-dimensional datasets.  </p><p class="new">Don't worry too much about the actual code (it's not important for this lesson).
You <em>should</em> be able to read it. The main point is to see where the 'words' are 
in 2D space and their relative distances to each other. </p><div class="ide code-starter clearfix"><pre><code>def reduce_dimensions(md, labels):
    from sklearn.manifold import TSNE
    import numpy as np
  
    data = np.array([md.vocab[w].vector for w in labels])
    # reduce to two
    tsne_model = TSNE(n_components=2)
    data_2d = tsne_model.fit_transform(data)
  
    return data_2d

def plot_results(data_2d, labels):
    import matplotlib
    import matplotlib.pyplot as plt
  
    # plot the 2d vectors and show their labels
    fig, axes = plt.subplots()
    axes.scatter(data_2d[:, 0], data_2d[:, 1], s=100)
    for i, txt in enumerate(labels):
        axes.annotate(txt, (data_2d[i,0], data_2d[i,1]), xytext=(2, 3), textcoords='offset points')
    axes.grid()
    return fig

def show_vector_space(md):
    labels = ['king', 'man', 'queen', 'woman']
    data = reduce_dimensions(md, labels)
    fig = plot_results(data, labels)

show_vector_space(nlp)</code></pre></div><p class="new">If the following line is a bit confusing: <code>data_2d[:, 0], data_2d[:, 1]</code>, 
now's a good time to review array slicing from INFO 490.  We will be using
 that syntax many times.</p><p class="new">You can see that there is indeed similar 'distances' between the points. Another fun 
<a href="https://projector.tensorflow.org/" target="_blank">visualization tool</a> for word vectors is with using TensorFlow. 
TensorFlow is a Google product to create, use (and learn about) machine learning algorithms easily. 
You can even use TensorFlow to create word embeddings --assuming you have 
access to a large corpus to train on.  Our next lesson is on word2vec, one of
 the most popular implementations for building and using word vectors.</p><h1 class="section" id="section3">One Hot Encoding</h1><p class="new">Algorithms that produce word vector models -- like word2vec and GloVe (more
 next lesson) usually input their 'words' as vectors as well.  However, they
use something called one-hot encoding.  </p><p class="new">One hot encoding simply assigns either a 1 or a 0 to an attribute if that
 attribute is present.  For example, in the following sentence: 
'a word is known by the company it keeps' would have the following matrix
 representation:</p><pre><code>        a word is known by the company it keeps
a       1 0    0  0     0   0  0       0  0
word    0 1    0  0     0   0  0       0  0
is      0 0    1  0     0   0  0       0  0
known   0 0    0  1     0   0  0       0  0
by      0 0    0  0     1   0  0       0  0
the     0 0    0  0     0   1  0       0  0
company 0 0    0  0     0   0  1       0  0
it      0 0    0  0     0   0  0       1  0
keeps   0 0    0  0     0   0  0       0  1</code></pre><p class="new">The rows are are the unique set of words in the corpus. The columns are the corpus. 
The big issue of course is that for a large corpus, the matrix is <em>sparse</em>
meaning most of the values are zero.  Hence, the usually need a more efficient
way to model this representation in memory.</p><p class="new">Note that one hot encoding only captures location information.  While word 
vectors capture 'meaning' and location.  They enable deep learning models as
 well since they can be used as input into these algorithms.</p><h2 id="bonus-vectors">Bonus Vectors</h2><p class="new">As with a lot of machine learning models, many will be too big to load into
 notebooks.  However, if you want to experiment with other word vectors 
(on your own computer -- they are too big to load into a notebook), 
the following models have been trained using Wikipedia. </p><ul><li><a href="https://fasttext.cc/docs/en/english-vectors.html" target="_blank">https://fasttext.cc/docs/en/english-vectors.html</a></li></ul><h1 class="section" id="section4">Lesson Assignment </h1><p class="new">This lesson uses a significant amount of compute resources. The <code>ide.tester</code> 
skips almost all the tests.  You will need to use gradescope for final
 testing and credit. </p><p class="new">However, you have enough experience to create your own tests. Once the model
 is loaded, each function shouldn't take too long to run in the notebook.</p><p class="new">Using what you learned in this lesson, you are going to create a function named
<code>find_analogy</code>.  This function takes 2 arguments:</p><ul><li>the nlp object to use to get the spacy word vectors</li><li>an array of 3 strings (used as words)</li></ul><p class="new">The function returns a string: </p><ul><li>name/text of the closest word that completes the analogy</li></ul><h3 id="example-use">example use</h3><pre><code>print(find_analogy(nlp, ['france', 'paris', 'rome']))</code></pre><p class="new">This should print <code>Italy</code> or <code>italy</code> (case will not matter for an accepted answer). 
One thing to remember is the order of the words given to <code>find_analogy</code> will matter. The
 same math formula is used so these analogies are 'hard' coded with how the
 vector math will work.</p><h3 id="implementation-details">implementation details</h3><p class="new">A few notes:</p><ul><li>you can model much of your code after the examples.  However, many of
the examples have assumptions that may not be true</li><li>write at least one helper function, <code>find_best_fit</code>, for <code>find_analogy</code> to use</li><li>this will make your code more readable, manageable, and testable</li><li>use the same <strong><em>logic</em></strong> to exclude words as <code>all_words_except</code></li></ul><div class="ide code-starter clearfix"><pre><code>def find_best_fit(md, result, words, topn=15):
    # md is the nlp model
    # result is a vector you're trying to get close to
    # words is a list of spacy objects/words (3 of them) that
    # need to be excluded (e.g nlp.vocab['man'])
  
    # return the topn candidates, in order so index 0 is the best fit
    # it should always have at least one word in it
    return []
    
def find_analogy(md, three_words):
     # md is the nlp model
     # three_words is a list of strings (e.g. ['king', 'man', 'woman'])

     # return the text (a string) for the word that meets the analogy 
     # it should use find_best_fit
     return None</code></pre></div><h4 id="test-cases">Test Cases</h4><p class="new">A few test cases to try:</p><pre><code>print(find_analogy(nlp, ['king', 'man', 'woman']))      
# --&gt; queen
print(find_analogy(nlp, ['Illinois', 'Springfield', 'Lansing']))  
# --&gt; michigan
print(find_analogy(nlp, ['walked', 'walk', 'go'])) 
# --&gt; want past tense of go (went)</code></pre><p class="new">Note that order will matter:</p><pre><code>print(find_analogy(nlp, ['pizza', 'Italy', 'Mexico'])) # YES tacos
print(find_analogy(nlp, ['Italy', 'pizza', 'Mexico'])) # NO SPAIN</code></pre><p class="new">If you start playing with these word vectors, remember, not all word
 analogies will work:</p><pre><code># fastest, (furthest in top 10)
print(find_analogy(nlp, ['quickest', 'quick', 'far'])) 

# raptor??, (bird not even in top 10) 
print(find_analogy(nlp, ['mammal', 'dog', 'eagle']))</code></pre><h2 id="snail-testing">Snail Testing</h2><p class="new">Both <code>find_analogy</code> and <code>find_best_match</code> take a bit (almost a minute) to test.
The testing software has load the spacy model into memory -- and we all know
 how long that can take.  Please, work on getting your code to work first in
 the notebook before using the <code>ide.tester</code> functionality.  If your analogies
 are not working here, they won't pass the tests anyway.</p><pre><code># print(ide.tester.test_function(find_analogy))
# print(ide.tester.test_function(find_best_match))</code></pre><h2 id="optimization-is-the-root-of-evil-but-">Optimization is the root of evil, but ...</h2><p class="new">Although you always want correctness over efficiency, once things are
 working there are a few places you can look to make your code faster: </p><ul><li>rather than sorting the entire array of words and the distances/similarities,
you can only sort the words worth keeping (a much smaller list)</li></ul><h1>Test and Submit</h1><p>Once you have finished, you can download your code (via <code>ide.tester</code>) and upload that file to Gradescope (find lesson with tag <strong>word-embeddings</strong>).</p><div class="my-4"><pre><code><strong># to list the tests available</strong><br/>print(ide.tester.list_tests())<br/><strong># to perform a specific test</strong><br/>print(ide.tester.test_functionality('name of test'))<br/><strong># to test your code (either works)</strong><br/>print(ide.tester.test_notebook())<br/>print(ide.tester.test_notebook(verbose=True))<br/><strong># to prepare and download your code</strong><br/>ide.tester.download_solution()</code></pre></div><div class="lesson-footer flex bg-gray-200 justify-center"><div class="lesson-footer-card displaycard bg-blue-200 border-t border-gray-400 max-w-2xl rounded overflow-hidden shadow-lg"><div class="px-6 py-4"><div class="title-text text-center font-bold text-xl">Word Embeddings</div><p class="text-center text-gray-800 text-xl">text by numbers</p><div class="text-center mt-6 text-xl"><i aria-hidden="true" class="fas fa-tags"></i> any questions on Piazza with <span class="font-bold">word-embeddings</span></div><div class="text-gray-700 text-base">¬†</div><div></div><div></div><div class="flex mt-4 border-t border-solid border-gray-500 justify-around bg-gray-200"><div class="text-gray-700 text-center px-4 m-2 text-sm">D.M. &amp; the üêç</div><div class="text-gray-700 text-center px-4 m-2 text-sm"><strong>Version:</strong> <!-- -->SP21</div></div><div class="text-gray-700 mt-2 text-center text-sm font-bold">All Rights Reserved Michael Haberman</div><div class="text-gray-700 text-center text-sm">Do not distribute this notebook</div></div></div></div><div>¬†</div><div class="ide code-starter clearfix"><pre><code># print(ide.tester.test_notebook()) 
# print(ide.tester.test_notebook(verbose=True)) 

# once you are ready -- run this 
# ide.tester.download_solution() 
</code></pre></div></div></div></body></html>