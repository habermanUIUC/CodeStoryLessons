<!DOCTYPE html><html lang='en'><head><title>Distance Metrics</title><meta charset="utf-8"><style>/*! normalize.css v8.0.1 | MIT License | github.com/necolas/normalize.css */html{line-height:1.15;-webkit-text-size-adjust:100%}body{margin:0}h1{font-size:2em;margin:.67em 0}pre{font-family:monospace,monospace;font-size:1em}a{background-color:transparent}strong{font-weight:bolder}code{font-family:monospace,monospace;font-size:1em}img{border-style:none}input{font-family:inherit;font-size:100%;line-height:1.15;margin:0}input{overflow:visible}[type=checkbox]{box-sizing:border-box;padding:0}::-webkit-file-upload-button{-webkit-appearance:button;font:inherit}h1,h2,h3,h4,p,pre{margin:0}html{font-family:system-ui,-apple-system,BlinkMacSystemFont,"Segoe UI",Roboto,"Helvetica Neue",Arial,"Noto Sans",sans-serif,"Apple Color Emoji","Segoe UI Emoji","Segoe UI Symbol","Noto Color Emoji";line-height:1.5}*,::after,::before{box-sizing:border-box;border-width:0;border-style:solid;border-color:#e2e8f0}img{border-style:solid}input:-ms-input-placeholder{color:#a0aec0}input::-ms-input-placeholder{color:#a0aec0}input::-moz-placeholder{color:#a0aec0}h1,h2,h3,h4{font-size:inherit;font-weight:inherit}a{color:inherit;text-decoration:inherit}input{padding:0;line-height:inherit;color:inherit}code,pre{font-family:Menlo,Monaco,Consolas,"Liberation Mono","Courier New",monospace}img{display:block;vertical-align:middle}img{max-width:100%;height:auto}.container{width:100%}@media (min-width:640px){.container{max-width:640px}}@media (min-width:768px){.container{max-width:768px}}@media (min-width:1024px){.container{max-width:1024px}}@media (min-width:1280px){.container{max-width:1280px}}.bg-gray-200{--bg-opacity:1;background-color:#edf2f7;background-color:rgba(237,242,247,var(--bg-opacity))}.bg-gray-300{--bg-opacity:1;background-color:#e2e8f0;background-color:rgba(226,232,240,var(--bg-opacity))}.bg-orange-300{--bg-opacity:1;background-color:#fbd38d;background-color:rgba(251,211,141,var(--bg-opacity))}.bg-green-200{--bg-opacity:1;background-color:#c6f6d5;background-color:rgba(198,246,213,var(--bg-opacity))}.bg-blue-200{--bg-opacity:1;background-color:#bee3f8;background-color:rgba(190,227,248,var(--bg-opacity))}.bg-blue-300{--bg-opacity:1;background-color:#90cdf4;background-color:rgba(144,205,244,var(--bg-opacity))}.border-gray-400{--border-opacity:1;border-color:#cbd5e0;border-color:rgba(203,213,224,var(--border-opacity))}.border-gray-500{--border-opacity:1;border-color:#a0aec0;border-color:rgba(160,174,192,var(--border-opacity))}.border-indigo-500{--border-opacity:1;border-color:#667eea;border-color:rgba(102,126,234,var(--border-opacity))}.rounded{border-radius:.25rem}.rounded-full{border-radius:9999px}.border-solid{border-style:solid}.border{border-width:1px}.border-l-2{border-left-width:2px}.border-t{border-top-width:1px}.cursor-pointer{cursor:pointer}.block{display:block}.inline-block{display:inline-block}.flex{display:flex}.justify-center{justify-content:center}.justify-around{justify-content:space-around}.float-left{float:left}.clearfix:after{content:"";display:table;clear:both}.font-sans{font-family:system-ui,-apple-system,BlinkMacSystemFont,"Segoe UI",Roboto,"Helvetica Neue",Arial,"Noto Sans",sans-serif,"Apple Color Emoji","Segoe UI Emoji","Segoe UI Symbol","Noto Color Emoji"}.font-serif{font-family:Georgia,Cambria,"Times New Roman",Times,serif}.font-semibold{font-weight:600}.font-bold{font-weight:700}.h-64{height:16rem}.text-xs{font-size:.75rem}.text-sm{font-size:.875rem}.text-base{font-size:1rem}.text-xl{font-size:1.25rem}.leading-none{line-height:1}.leading-normal{line-height:1.5}.m-2{margin:.5rem}.my-1{margin-top:.25rem;margin-bottom:.25rem}.my-3{margin-top:.75rem;margin-bottom:.75rem}.my-4{margin-top:1rem;margin-bottom:1rem}.mt-1{margin-top:.25rem}.mt-2{margin-top:.5rem}.mr-2{margin-right:.5rem}.mb-2{margin-bottom:.5rem}.mr-3{margin-right:.75rem}.mb-3{margin-bottom:.75rem}.mt-4{margin-top:1rem}.mb-4{margin-bottom:1rem}.mt-6{margin-top:1.5rem}.max-w-sm{max-width:24rem}.max-w-2xl{max-width:42rem}.object-contain{-o-object-fit:contain;object-fit:contain}.opacity-0{opacity:0}.overflow-hidden{overflow:hidden}.p-1{padding:.25rem}.p-2{padding:.5rem}.p-3{padding:.75rem}.py-1{padding-top:.25rem;padding-bottom:.25rem}.py-2{padding-top:.5rem;padding-bottom:.5rem}.px-3{padding-left:.75rem;padding-right:.75rem}.py-4{padding-top:1rem;padding-bottom:1rem}.px-4{padding-left:1rem;padding-right:1rem}.px-6{padding-left:1.5rem;padding-right:1.5rem}.pl-3{padding-left:.75rem}.absolute{position:absolute}.shadow-md{box-shadow:0 4px 6px -1px rgba(0,0,0,.1),0 2px 4px -1px rgba(0,0,0,.06)}.shadow-lg{box-shadow:0 10px 15px -3px rgba(0,0,0,.1),0 4px 6px -2px rgba(0,0,0,.05)}.shadow-inner{box-shadow:inset 0 2px 4px 0 rgba(0,0,0,.06)}.text-left{text-align:left}.text-center{text-align:center}.text-black{--text-opacity:1;color:#000;color:rgba(0,0,0,var(--text-opacity))}.text-gray-700{--text-opacity:1;color:#4a5568;color:rgba(74,85,104,var(--text-opacity))}.text-gray-800{--text-opacity:1;color:#2d3748;color:rgba(45,55,72,var(--text-opacity))}.whitespace-no-wrap{white-space:nowrap}.w-1\/2{width:50%}.w-2\/3{width:66.666667%}.w-full{width:100%}@media (min-width:768px){.md\:w-2\/3{width:66.666667%}}.text-tiny{font-size:.5rem!important}body{color:#000!important;font-size:1.25rem!important}.main-content{max-width:900px}.lesson{padding-left:15px!important;padding-right:10px!important;--bg-opacity:1;background-color:#edf2f7;background-color:rgba(237,242,247,var(--bg-opacity))}.main-content,html{font-family:Arial,Georgia,Verdana,"Times New Roman"!important}.lesson-footer-card,.lesson-overview-card{font-family:"Times New Roman"!important}.lesson-footer{margin-top:50px;margin-top:20px}span{white-space:nowrap}p.new{padding-top:0;padding-bottom:.5em}p.new+p{padding-top:.5em}h1,h2,h3,h4{font-weight:700;margin-top:.25em!important;margin-bottom:.05em!important;font-family:Georgia,Cambria,"Times New Roman",Times,serif!important}h1{font-size:2em!important;clear:both;color:#000!important}div+h1,h2{margin-top:0!important}h2{margin-top:.5em!important;font-size:1.5em!important;clear:both;color:#8b0000!important}h3{font-size:1.25em!important;clear:both;color:#006400!important}h4{font-size:1em!important;clear:both;color:#00008b!important}.lesson a{text-decoration:underline;color:#00f}.title-text{font-size:2rem}.shadow-lg{box-shadow:0 10px 15px -3px rgba(0,0,0,.1),0 4px 6px -2px rgba(0,0,0,.5)}img.formula-block{margin-left:auto;margin-right:auto;margin-top:.25rem;margin-bottom:.75rem}img.formula-inline{margin-top:.25rem;margin-bottom:.25rem}img.center{-o-object-position:center;object-position:center;margin-left:auto;margin-right:auto}img.small{width:33.333333%;margin-left:1.25rem}img.border{border:1px solid #021a40;margin-top:.5rem;margin-bottom:.75rem}img.iw600{height:auto;width:auto;max-width:600px}img.iw500{height:auto;width:auto;max-width:500px}img.iw300{height:auto;width:auto;max-width:300px}img.iw200{height:auto;width:auto;max-width:200px}img.iw100{height:auto;width:auto;max-width:100px}code{font-size:smaller}pre code{font-size:15px}pre code:not(.line-number){background:#f4f4f4;font-family:monospace;font-size:15px;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;cursor:default;touch-action:none;-webkit-touch-callout:none;-webkit-tap-highlight-color:transparent;clear:both;border:1px solid #ddd;color:#666;page-break-inside:avoid;display:block;min-width:840px;max-width:840px;overflow:scroll;line-height:1.6;margin-bottom:1.6em;padding:1em 1.5em;-moz-tab-size:2;-o-tab-size:2;tab-size:2;word-wrap:break-word;white-space:pre-wrap;border-left:3px solid #f36d33}div.code-starter>pre code{border-left:3px solid #fdff44!important;background-image:radial-gradient(rgba(0,150,0,.75),#000 120%);color:#fff;font:.9rem Inconsolata,monospace}div.code-starter>pre code::after{content:"\a$_"}.tab{font-size:1rem;border-color:#8c6728}.tab-content{max-height:0;max-width:100%;transition:max-height .35s}.tab input:checked~.tab-content{max-height:100vh}.tab input:checked+label{padding:1rem;border-left-width:2px;border-color:#6574cd;background-color:#f8fafc;color:#6574cd}.tab label::after{float:right;right:0;top:0;display:block;width:1em;height:1.5em;line-height:1.5;font-size:1rem;text-align:center;transition:all .35s}.tab input[type=checkbox]+label::after{content:"+";font-weight:700;border-width:1px;border-radius:9999px;border-color:#8c6728}.tab input[type=checkbox]:checked+label::after{transform:rotate(315deg);background-color:#6574cd;color:#f8fafc}</style>
<script src="https://kit.fontawesome.com/7efc4bcee2.js" crossOrigin="anonymous"></script>
<script>
    let stateCheck = setInterval(function(){
      if (document.readyState === 'complete') {
        clearInterval(stateCheck);
        let s1 = document.getElementById('start');
        // console.log('doc is ready', s1);
        if (s1) {
           s1.setAttribute('tabindex', '-1');
           s1.focus(); 
           s1.scrollIntoView({behavior: 'smooth'}); 
           setTimeout(function(){s1.blur()}, 500);
           // console.log('focus set');
        }
      }
    }, 200);
    </script>
</head><body class="lesson"><div class="main-content lesson bg-gray-200 text-black p-1 pl-3 font-serif"><div class="md-inner">
<div id="start" class="section">&nbsp;</div><h1 class="overview"></h1><div class="lesson-overview bg-gray-200 flex justify-center"><div class="text-center px-4 py-2 m-2"><div class="lesson-overview-card displaycard bg-blue-200 max-w-sm rounded overflow-hidden shadow-lg"><div>¬†</div><img alt="Text" class="object-contain h-64 w-full" src="https://raw.githubusercontent.com/habermanUIUC/CodeStoryLessons/main/lessons/dmap/ml/distance/html/MachineLearningV1-sm.png"/><div class="px-6 py-4"><div class="title-text text-center leading-none font-bold text-xl">Distance Metrics</div><p class="text-center mt-2 text-gray-800 text-xl">Far being the cost</p><div class="text-gray-700 text-base">¬†</div><div class="text-center mb-3"><span class="inline-block bg-gray-300 rounded-full px-3 py-1 text-sm font-semibold text-gray-700 mr-2">#machine learning</span></div><div class="flex border-t border-solid border-gray-500 shadow-inner justify-around bg-blue-300"><div class="text-gray-700 text-center px-4 m-2 text-sm"><span class="whitespace-no-wrap">D.M. &amp; the üêç</span></div><div class="text-gray-700 text-center px-4 m-2 text-sm"><span class="whitespace-no-wrap"><strong>Version:</strong> <!-- -->SP21</span></div></div><div class="text-gray-700 mt-1 text-center text-tiny">All Rights Reserved</div></div></div></div><div class="text-center px-4 py-2 m-2 w-1/2"><div class="displaycard bg-gray-200 max-w-sm rounded overflow-hidden shadow-lg"><div class="px-6 py-4 text-left"><div class="text-center font-bold text-xl">Distance Metrics<br/><div><span>prerequisites</span><div class="text-center text-xs mb-2">(start only after finishing)</div><p class="max-w-sm text-gray-800 text-sm">‚¶ø <strong>mlprep</strong></p></div></div></div><div class="px-6 py-4 text-left text-gray-800"><div class="text-center font-bold text-xl">Colab Notes</div><p class="max-w-sm text-sm">1. <strong>Copy</strong> this notebook <img alt="copy2drive.png" class="inline-block" src="https://raw.githubusercontent.com/habermanUIUC/CodeStoryLessons/main/lessons/dmap/ml/distance/html/copy2drive.png"/></p><p class="max-w-sm text-sm">2. <strong>Update</strong> the <strong><code>NET_ID</code></strong> in the notebook</p><p class="max-w-sm text-gray-800 text-sm">3. <strong>Hit ‚ñ∂Ô∏è¬†</strong> to install the INFO 490 IDE</p><div class="text-center font-bold text-xl">¬†</div><div class="text-center font-bold text-xl">Jupyter/PyCharm Notes</div><p class="max-w-sm text-gray-800 text-sm text-left">The testing framework does <strong>not work</strong> (at this time) for Jupyter  notebooks or local code development.</p></div></div></div></div><h1 class="section" id="section1">Distance Metrics</h1><p class="new">Almost all machine learning algorithms use metrics while building their models.
In general, there are two kinds of metrics: those for <u>model evaluation</u> and
 those for <u>data evaluation</u>.</p><h2 id="model-evaluation">Model Evaluation</h2><p class="new">These metrics help evaluate and compare different models. It could be different 
versions of the same algorithm (e.g. hyper-parameter tuning) or completely 
different algorithms (neural network model vs a Bayesian model). These metrics 
also help evaluate the "cost" during the building of a model. 
Each epoch (iteration of a model build) needs to be evaluated as well. There's no need to continue 
the search for a better model, if the current one is 'close enough'.  </p><p class="new">You may have heard of <strong>loss functions</strong>, <strong>cost functions</strong>, <strong>fitness</strong>, 
<strong>optimizing the objective function</strong> can refer to metrics at this stage. The objective
   function is the function you want to minimize (or maximize). Some 
    reserve the term loss function for a single training example and cost
     function for the entire training set.</p><p class="new">Specific metrics to evaluate a model will be discussed in detail for the
 lesson dedicated to the algorithm.</p><h2 id="data-relationship-evaluation-distance">Data Relationship Evaluation: <em>distance</em></h2><p class="new">These metrics help determine the relationship between different instances in a
 dataset. For example, in tf‚Ä¢idf, each word had a value to reflect how 'important' 
 it was to a document.</p><p class="new">This lesson is focusing on the latter:  where we want to know which metrics
 can be used to measure the 'distance' between elements/points in the
  dataset.</p><p class="new">Algorithms for classification (K-Nearest Neighbors (KNN)), clustering (K-Means), 
and document retrieval all rely on some sort of metric to help
 determine distance or similarity between instances/data points.</p><h2 id="mathematical-primer">Mathematical Primer</h2><p class="new">The cost of learning about distance metrics involves taking the time to
 understand some of the math behind the metrics. Sometimes the biggest
  barrier is just being able to read the nomenclature and symbols of the
   underlying formulas. </p><p class="new">Let's start by looking at the familiar two dimensional graph (a.k.a. euclidean, 
Cartesian coordinates) for 2 points. Each point has two (hence two-dimensional) components: 
an <code>x</code> value and a <code>y</code> value. 
<img alt="points.png" class="iw300 center border" src="https://raw.githubusercontent.com/habermanUIUC/CodeStoryLessons/main/lessons/dmap/ml/distance/html/points.png"/> </p><p class="new">Symbolically we can write <img alt="math?math=%5Clarge%20p_1%20%3D%5C%7Bx_1%2Cy_1%5C%7D" class="my-1 formula-inline" src="https://render.githubusercontent.com/render/math?math=%5Clarge%20p_1%20%3D%5C%7Bx_1%2Cy_1%5C%7D" style="display:inline-block"/> and <img alt="math?math=%5Clarge%20p_2%20%3D%5C%7Bx_2%2Cy_2%5C%7D" class="my-1 formula-inline" src="https://render.githubusercontent.com/render/math?math=%5Clarge%20p_2%20%3D%5C%7Bx_2%2Cy_2%5C%7D" style="display:inline-block"/> </p><p class="new">Both <code>p1</code> and <code>p2</code> can also be called vectors (we'll discuss the meaning soon) and 
can be written using 'vector' notation: 
<img alt="math?math=%5CLarge%20%5Cvec%7Bp_1%7D%3D%5C%7Bx_1%2Cy_1%5C%7D" class=" formula-block" src="https://render.githubusercontent.com/render/math?math=%5CLarge%20%5Cvec%7Bp_1%7D%3D%5C%7Bx_1%2Cy_1%5C%7D"/></p><p class="new">Each 'dimension' is usually written using the same symbol. So rather than having 
an <code>x</code> attribute and a <code>y</code> attribute, a common sub-scripted letter (usually <code>u</code>, <code>v</code>, <code>w</code>) 
is used:</p><img alt="math?math=%5CLarge%20p_1%3D%5Cvec%7Bu%7D%3D%5C%7Bu_1%2Cu_2%5C%7D" class=" formula-block" src="https://render.githubusercontent.com/render/math?math=%5CLarge%20p_1%3D%5Cvec%7Bu%7D%3D%5C%7Bu_1%2Cu_2%5C%7D"/><img alt="math?math=%5CLarge%20p_2%3D%5Cvec%7Bv%7D%3D%5C%7Bv_1%2Cv_2%5C%7D" class=" formula-block" src="https://render.githubusercontent.com/render/math?math=%5CLarge%20p_2%3D%5Cvec%7Bv%7D%3D%5C%7Bv_1%2Cv_2%5C%7D"/><p class="new">Sometimes the ^ (i.e. 'hat') symbol is used to signal the value as a vector
 as well. Also take note that <img alt="math?math=%5Clarge%20u_1" class="my-1 formula-inline" src="https://render.githubusercontent.com/render/math?math=%5Clarge%20u_1" style="display:inline-block"/> and <img alt="math?math=%5Clarge%20v_1" class="my-1 formula-inline" src="https://render.githubusercontent.com/render/math?math=%5Clarge%20v_1" style="display:inline-block"/> are the <code>x</code> attributes and 
<img alt="math?math=%5Clarge%20u_2" class="my-1 formula-inline" src="https://render.githubusercontent.com/render/math?math=%5Clarge%20u_2" style="display:inline-block"/> and <img alt="math?math=%5Clarge%20v_2" class="my-1 formula-inline" src="https://render.githubusercontent.com/render/math?math=%5Clarge%20v_2" style="display:inline-block"/> are the <code>y</code> attributes.  Each subscript is
 the same dimension across all the vectors.</p><h2 id="multiple-dimensions">Multiple Dimensions</h2><p class="new">The nice thing about this notation is that it easily extends into multiple
 dimensions.  We don't have to worry about using different letters (x,y,z). Each 
 dimension is indicated by a number instead.</p><img alt="math?math=%5CLarge%20%5Cvec%7Bu%7D%20%3D%20%5Chat%7Bu%7D%3D%5C%7Bu_1%2Cu_2%2Cu_3%2C...%2Cu_%7Bn-1%7D%2Cu_n%5C%7D" class=" formula-block" src="https://render.githubusercontent.com/render/math?math=%5CLarge%20%5Cvec%7Bu%7D%20%3D%20%5Chat%7Bu%7D%3D%5C%7Bu_1%2Cu_2%2Cu_3%2C...%2Cu_%7Bn-1%7D%2Cu_n%5C%7D"/><img alt="math?math=%5CLarge%20%5Cvec%7Bv%7D%20%3D%20%5Chat%7Bv%7D%3D%5C%7Bv_1%2Cv_2%2Cv_3%2C...%2Cv_%7Bn-1%7D%2Cv_n%5C%7D" class=" formula-block" src="https://render.githubusercontent.com/render/math?math=%5CLarge%20%5Cvec%7Bv%7D%20%3D%20%5Chat%7Bv%7D%3D%5C%7Bv_1%2Cv_2%2Cv_3%2C...%2Cv_%7Bn-1%7D%2Cv_n%5C%7D"/><p class="new">Both <code>u</code> and <code>v</code> are n-dimensional vectors.  You can think of a dimension as
 simply an attribute rather than trying to visualize a high-dimensional space.</p><p class="new">Each vector represents the same attribute across all data instances.  It is
 also possible to have a vector represent all the attributes of a single
  instance.  It's important to know what the vector represents -- and it
   usually depends on the problem or context.</p><h2 id="math-on-vectors">Math on Vectors</h2><p class="new">Once our data is expressed in vector form, we can do math on them
 (specifically linear algebra). The one caveat is that all the data (as vectors) needs to
  have the same number of dimensions. If you want to <em>add</em> <code>u</code> and <code>v</code>, they both 
  need to have the same number of dimensions (components, attributes).</p><h1 class="section" id="section2">Physics, Math, and Data Vectors</h1><h2 id="euclidean-vectors">Euclidean Vectors</h2><img alt="vector2.png" class="border iw200 float-left mr-3" src="https://raw.githubusercontent.com/habermanUIUC/CodeStoryLessons/main/lessons/dmap/ml/distance/html/vector2.png"/><p class="new">One stumbling point is that different fields will use the word vector differently. 
In mathematics, vector represents direction/orientation and magnitude (and 'position' isn't a quantity). </p><p class="new"> 
 In physics, the word vector usually refers to a physical observation. Examples 
 might be velocity, torque, and acceleration. 
<img alt="vector.png" class="iw300 float-left mr-3" src="https://raw.githubusercontent.com/habermanUIUC/CodeStoryLessons/main/lessons/dmap/ml/distance/html/vector.png"/>These vectors are called Euclidean vectors. Since position isn't considered, 
these vectors are always anchored at the origin (0,0).  </p><h2 id="data-vectors">Data Vectors</h2><p class="new">In computer science the word vector may be used to refer to a data type (e.g. arrays) 
or a type of graphics. Even in Python:</p><pre><code>some_list = [0,1,2,3,4]</code></pre><p class="new">The variable <code>some_list</code> can be called (a one dimensional list with 5 elements) 
or a vector with 5 attributes (e.g. 5-dimensional; one row). </p><p class="new">In data science the word <strong>feature vector</strong> is used to discuss instances with <code>n</code> 
dimensional attributes. </p><p class="new">It indeed can be confusing. Vector based mathematics is deep and wide -- there 
are several courses that are centered on defining and describing properties of 
using vectors. Rather than going deep into the properties of vector math, we 
will try to stay applicable to our needs. If you have time, taking a course in 
linear algebra will be well worth the investment.</p><h1 class="section" id="section3">Too many normals ü§î</h1><p class="new">We have discussed <em>normalizing</em> data, and you may have heard of using an <em>L1 norm</em>, 
<em>L2 norm</em>, <em>normalizing</em> a vector, and even calculating a <em>vector norm</em>. 
It's all so confusing!  The word normal is indeed multi-dimensional.  Let's
 start with some simple definitions.</p><h2 id="l1-norm">L1 Norm</h2><h3 id="l1-single-vector-norm">L1 Single Vector Norm</h3><p class="new">The L1 Norm for a single vector is just the sum of the absolute values of the components.</p><img alt="math?math=%5CLarge%20L1%3D%7B%5Cleft%5CVert%7B%7D%7Bu%7D%5Cright%5CVert%7B%7D%7D_1%20%3D%20%5Csum_%7Bi%3D1%7D%5En%7Cu_i%7C" class=" formula-block" src="https://render.githubusercontent.com/render/math?math=%5CLarge%20L1%3D%7B%5Cleft%5CVert%7B%7D%7Bu%7D%5Cright%5CVert%7B%7D%7D_1%20%3D%20%5Csum_%7Bi%3D1%7D%5En%7Cu_i%7C"/><h2 id="l2-norm">L2 Norm</h2><h3 id="l2-single-vector-norm">L2 Single Vector Norm</h3><p class="new">The L2 Norm for a single vector is just the square root of the summed squared component values.</p><img alt="math?math=%5CLarge%20L2%3D%7B%5Cleft%5CVert%7B%7D%7Bu%7D%5Cright%5CVert%7B%7D%7D_2%20%3D%20%5Csqrt%7B%5Csum_%7Bi%3D1%7D%5En%7B(u_i)%7D%5E2%7D" class=" formula-block" src="https://render.githubusercontent.com/render/math?math=%5CLarge%20L2%3D%7B%5Cleft%5CVert%7B%7D%7Bu%7D%5Cright%5CVert%7B%7D%7D_2%20%3D%20%5Csqrt%7B%5Csum_%7Bi%3D1%7D%5En%7B(u_i)%7D%5E2%7D"/><p class="new">The L2 Norm is also called the <strong><em>length</em></strong> or <strong><em>magnitude</em></strong> of a vector. </p><p class="new">Sometimes it's seen as <img alt="math?math=%5Clarge%20%5Cleft%7Cu%5Cright%7C" class="my-1 formula-inline" src="https://render.githubusercontent.com/render/math?math=%5Clarge%20%5Cleft%7Cu%5Cright%7C" style="display:inline-block"/> or <img alt="math?math=%5Clarge%20%5Cleft%5CVert%7B%7D%7Bu%7D%5Cright%5CVert%7B%7D" class="my-1 formula-inline" src="https://render.githubusercontent.com/render/math?math=%5Clarge%20%5Cleft%5CVert%7B%7D%7Bu%7D%5Cright%5CVert%7B%7D" style="display:inline-block"/></p><p class="new">We can use numpy (and scipy) to demonstrate these norms</p><div class="ide code-starter clearfix"><pre><code>import numpy as np
from numpy import linalg as LA

values = np.array([x for x in range(0,10)])
l1_norm = LA.norm(values, ord=1) 
l2_norm = LA.norm(values, ord=2) 
ld_norm = LA.norm(values)  # default is ord==2
print('L1', l1_norm)
print('L2', l2_norm, ld_norm)</code></pre></div><p class="new"> </p><p class="new">For completeness sake the <code>L0</code> norm is the number of non-zero components in a vector.
Both L1 and the L2 norms are used in their respective distance formulas</p><h1 class="section" id="section4">Distance Formulas</h1><p class="new">With both the L1 and L2 Norms defined, we also have our first two distance
 formulas (which work for any number of dimensions).  In these examples, we
 are assuming that each vector contains the relevent attributes of one instance.</p><h2 id="l1taxicab-distance">L1/Taxicab Distance</h2><h3 id="l1-distance-norm">L1 Distance Norm</h3><p class="new">The L1 <em>distance</em> norm is also called Manhattan (or Taxicab) norm. It is the sum of the
 absolute difference of the components of the vector.  In the image below, the
  distance is simply <img alt="math?math=%5Clarge%20%7Cx_2%20-%20x_1%7C%20%2B%20%7Cy_2%20-%20y_1%7C" class="my-1 formula-inline" src="https://render.githubusercontent.com/render/math?math=%5Clarge%20%7Cx_2%20-%20x_1%7C%20%2B%20%7Cy_2%20-%20y_1%7C" style="display:inline-block"/></p><img alt="L1.png" class="border iw300 center" src="https://raw.githubusercontent.com/habermanUIUC/CodeStoryLessons/main/lessons/dmap/ml/distance/html/L1.png"/><p class="new">We can express this in vector notation as</p><img alt="math?math=%5CLarge%20L1_%7Bnorm%7D%3D%7B%5Cleft%5CVert%7B%7D%7BD%7D%5Cright%5CVert%7B%7D%7D_1%20%3D%20%5Csum_%7Bi%3D1%7D%5En%7Cu_i-v_i%7C" class=" formula-block" src="https://render.githubusercontent.com/render/math?math=%5CLarge%20L1_%7Bnorm%7D%3D%7B%5Cleft%5CVert%7B%7D%7BD%7D%5Cright%5CVert%7B%7D%7D_1%20%3D%20%5Csum_%7Bi%3D1%7D%5En%7Cu_i-v_i%7C"/><h2 id="l2euclidean-distance-">L2/Euclidean Distance </h2><h3 id="l2-distance-norm">L2 Distance Norm</h3><p class="new">The L2 <em>distance</em> norm (also called the Euclidean norm) is the square root
 of sum of the squared differences:</p><img alt="L2.png" class="border iw300 center" src="https://raw.githubusercontent.com/habermanUIUC/CodeStoryLessons/main/lessons/dmap/ml/distance/html/L2.png"/><p class="new">This is the familiar <img alt="math?math=%5Clarge%20a%5E2%20%2B%20b%5E2%20%3D%20c%5E2" class="my-1 formula-inline" src="https://render.githubusercontent.com/render/math?math=%5Clarge%20a%5E2%20%2B%20b%5E2%20%3D%20c%5E2" style="display:inline-block"/> formula. We can express this in 
vector notation as </p><img alt="math?math=%5CLarge%20L2%3D%7B%5Cleft%5CVert%7B%7D%7BD%7D%5Cright%5CVert%7B%7D%7D_2%20%3D%20%5Csqrt%7B%5Csum_%7Bi%3D1%7D%5En%7B(u_i-v_i)%7D%5E2%7D" class=" formula-block" src="https://render.githubusercontent.com/render/math?math=%5CLarge%20L2%3D%7B%5Cleft%5CVert%7B%7D%7BD%7D%5Cright%5CVert%7B%7D%7D_2%20%3D%20%5Csqrt%7B%5Csum_%7Bi%3D1%7D%5En%7B(u_i-v_i)%7D%5E2%7D"/><p class="new">Note that these quantities are defined for <em>any number</em> of dimensions.</p><h2 id="cosine-distance-">Cosine Distance </h2><p class="new">Although we have explained the cosine metric before (e.g. tf‚Ä¢idf), it's a good 
time to discuss it in greater detail. The cosine distance (or similarity) 
metric uses the L2 Norm in its formula. In the image below, the angle between the two
 vectors (ùú≠) is the measurement we are interested in.  </p><img alt="COS.png" class="border iw300 center" src="https://raw.githubusercontent.com/habermanUIUC/CodeStoryLessons/main/lessons/dmap/ml/distance/html/COS.png"/><p class="new">By using some geometric properties (and who doesn't long for their high-school 
geometry class), we can find the <code>cos(ùú≠)</code> using the the formula below:</p><img alt="math?math=%5CLarge%20cos(%5Ctheta)%20%3D%20%5Cfrac%7B%5Csum_%7Bi%3D1%7D%5En%7B(u_i%20%5Ctimes%20v_i)%7D%7D%7B%5Csqrt%7B%5Csum_%7Bi%3D1%7D%5En%7Bu_i%7D%5E2%7D%20%5Csqrt%7B%5Csum_%7Bi%3D1%7D%5En%7Bv_i%7D%5E2%7D%7D" class=" formula-block" src="https://render.githubusercontent.com/render/math?math=%5CLarge%20cos(%5Ctheta)%20%3D%20%5Cfrac%7B%5Csum_%7Bi%3D1%7D%5En%7B(u_i%20%5Ctimes%20v_i)%7D%7D%7B%5Csqrt%7B%5Csum_%7Bi%3D1%7D%5En%7Bu_i%7D%5E2%7D%20%5Csqrt%7B%5Csum_%7Bi%3D1%7D%5En%7Bv_i%7D%5E2%7D%7D"/><p class="new">If you look carefully, you will notice the denominator is just the L2norm for u an v</p><img alt="math?math=%5CLarge%20cos(%5Ctheta)%20%3D%20%5Cfrac%7B%5Csum_%7Bi%3D1%7D%5En%7B(u_i%20%5Ctimes%20v_i)%7D%7D%7B%5Cleft%5CVert%7B%7D%7Bu%7D%5Cright%5CVert%7B%7D_2%20%5Cleft%5CVert%7B%7D%7Bv%7D%5Cright%5CVert%7B%7D_2%7D" class=" formula-block" src="https://render.githubusercontent.com/render/math?math=%5CLarge%20cos(%5Ctheta)%20%3D%20%5Cfrac%7B%5Csum_%7Bi%3D1%7D%5En%7B(u_i%20%5Ctimes%20v_i)%7D%7D%7B%5Cleft%5CVert%7B%7D%7Bu%7D%5Cright%5CVert%7B%7D_2%20%5Cleft%5CVert%7B%7D%7Bv%7D%5Cright%5CVert%7B%7D_2%7D"/><h3 id="dot-product">Dot Product</h3><p class="new">In linear algebra, the dot product (also called the <em>inner product</em>) of two vectors 
is the result of summing the component-by-component multiplication:</p><img alt="math?math=%5CLarge%20u%20%5Ccdot%20v%20%3D%20(u_1%2C%20..%20%2C%20u_n)%20%5Ccdot%20(v_1%2C%20..%20%2C%20v_n)%20%3D%20u_1%20v_1%20%2B%20..%20%20%2B%20u_n%20v_n" class=" formula-block" src="https://render.githubusercontent.com/render/math?math=%5CLarge%20u%20%5Ccdot%20v%20%3D%20(u_1%2C%20..%20%2C%20u_n)%20%5Ccdot%20(v_1%2C%20..%20%2C%20v_n)%20%3D%20u_1%20v_1%20%2B%20..%20%20%2B%20u_n%20v_n"/><pre><code>u = [2, 4, 6,]
v = [3, 5, 7,]
u‚Ä¢v = (2*3) + (4*5) + (6*7)
u‚Ä¢v = 6 + 20 + 42 = 68</code></pre><p class="new">So now we can rewrite the cos formula as
<img alt="math?math=%5CLarge%20cos(%5Ctheta)%20%3D%20%5Cfrac%7B%20u%20%5Ccdot%20v%20%7D%7B%20%5C%7C%20u%20%5C%7C%20%5C%7C%20v%20%5C%7C%20%7D" class=" formula-block" src="https://render.githubusercontent.com/render/math?math=%5CLarge%20cos(%5Ctheta)%20%3D%20%5Cfrac%7B%20u%20%5Ccdot%20v%20%7D%7B%20%5C%7C%20u%20%5C%7C%20%5C%7C%20v%20%5C%7C%20%7D"/> </p><p class="new">In fact, looking even closer, we can replace everything with dot products:</p><img alt="math?math=%5CLarge%20%5Cfrac%7Bu%20%5Cbullet%20v%7D%7B%20%5Csqrt%7Bu%20%5Cbullet%20u%7D%20%5Csqrt%7Bv%20%5Cbullet%20v%7D%7D" class=" formula-block" src="https://render.githubusercontent.com/render/math?math=%5CLarge%20%5Cfrac%7Bu%20%5Cbullet%20v%7D%7B%20%5Csqrt%7Bu%20%5Cbullet%20u%7D%20%5Csqrt%7Bv%20%5Cbullet%20v%7D%7D"/><p class="new">The following code demonstrates this (not a proof :)</p><div class="ide code-starter clearfix"><pre><code>import numpy as np
from numpy import linalg as LA
from scipy.spatial import distance

def cosine_similarity_v1(x, y):
    return np.dot(x,y)/(LA.norm(x) * LA.norm(y))

def cosine_similarity_v2(x, y):
    return np.dot(x, y) / (np.sqrt(np.dot(x, x)) * np.sqrt(np.dot(y, y)))
    
def cosine_similarity_v3(x, y):
    return 1 - distance.cosine(x,y)

u = np.array([2, 4, 6,])
v = np.array([3, 5, 7,])
d1 = cosine_similarity_v1(u,v)
d2 = cosine_similarity_v2(u,v)
d3 = cosine_similarity_v3(u,v)
print(d1, d2, d3)

from sklearn.metrics.pairwise import cosine_similarity
print(cosine_similarity(u.reshape(1,3),v.reshape(1,3)))</code></pre></div><h3 id="the-range-of-values">The Range of Values</h3><p class="new">Although we want the value of ùõ≥, we take the cosine of that angle to map the
 values to the [-1, 1] range. This provides the benefit of being able to restrict
 the range of output.</p><p class="new">Take a look at the cosine graph to re-familiarize yourself with the range of the cosine function:
<img alt="cosine.png" class="ma-4 mt-6 border small" src="https://raw.githubusercontent.com/habermanUIUC/CodeStoryLessons/main/lessons/dmap/ml/distance/html/cosine.png"/></p><p class="new">A value of 1 (ùõ≥=0), means the vectors are completely aligned.
When the dot product is 0, it means the vectors are perpendicular to each
 other (geometrically speaking).</p><h4 id="word-similarity">Word Similarity</h4><p class="new">When we were discussing word similarity in spaCy (words with 'similar' 
embeddings), those close to 0 were very similar words (fri and fries) or
 words that co-occurred often (Italy and coffee).</p><img alt="words.png" class="border center iw600" src="https://raw.githubusercontent.com/habermanUIUC/CodeStoryLessons/main/lessons/dmap/ml/distance/html/words.png"/><h3 id="vector-normalization">Vector Normalization</h3><p class="new">In tf‚Ä¢idf scoring, there was an option to 'normalize' the document vectors. This 
was done so that longer documents (which will have more words) didn't become 
'more important' just because of their length.</p><p class="new">In vector normalization, we take the vector (the components of the vector) 
and divide each by the 'length' of the vector. However, length in this
 context <em>is</em> the L2 norm. This comes from terminology of vectors having
 both length and magnitude. </p><div class="ide code-starter clearfix"><pre><code>def norm_demo():
    # normalizing a vector
    u = np.array([1,2,3])
    l2_norm = LA.norm(u)  # sometimes called the 'length' of the vector
    unit_vector = u/l2_norm
    return unit_vector
    
print(norm_demo())</code></pre></div><p class="new">When you do vector normalization, you are essentially creating a <em>unit</em> vector.
A unit vector has length/magnitude (i.e. L2 norm) of 1.0:</p><div class="ide code-starter clearfix"><pre><code>unit_vector = norm_demo()
# a unit vector has length 1
print(LA.norm(unit_vector))</code></pre></div><p class="new">You can even use sklearn's preprocessing module to do vector normalization:</p><div class="ide code-starter clearfix"><pre><code>import sklearn.preprocessing as pre

v = np.array([0,3,-4])
unit_norm = pre.Normalizer(norm='l2').fit_transform([v])
print(unit_norm)

print(pre.normalize([v], norm='l2'))</code></pre></div><h2 id="the-normal-vector">The ‚ùùnormal‚ùû vector</h2><img alt="normal_vector.png" class="border iw100 float-left mr-3" src="https://raw.githubusercontent.com/habermanUIUC/CodeStoryLessons/main/lessons/dmap/ml/distance/html/normal_vector.png"/><p class="new">Just when you thought there's enough normal to go around, the phrase 'vector
 normal' adds yet another path. A normal vector is one that is perpendicular to a
 surface (sometimes called orthogonal).  Many image shading algorithms rely
 on vector normals to understand orientation.</p><h2 id="minkowski-distance">Minkowski Distance</h2><p class="new">Another distance metric is the generalization of the Euclidean metric. 
Rather than taking the square root, we take the <code>n</code>th root. It's named the 
Minkowski distance and it serves well for managing points in high dimensional space.</p><img alt="math?math=%5CLarge%20%5Csqrt%5Bp%5D%7B%20%7B(u_1-v_1)%7D%5Ep%20%2B%20%7B(u_2-v_2)%7D%5Ep%20%2B%20..%20%2B%20%7B(u_n-v_n)%7D%5Ep%7D" class=" formula-block" src="https://render.githubusercontent.com/render/math?math=%5CLarge%20%5Csqrt%5Bp%5D%7B%20%7B(u_1-v_1)%7D%5Ep%20%2B%20%7B(u_2-v_2)%7D%5Ep%20%2B%20..%20%2B%20%7B(u_n-v_n)%7D%5Ep%7D"/><p class="new">In the KNN lesson, this was the <code>p</code> value for the model:</p><pre><code>knn = KNeighborsClassifier(n_neighbors=11, p=2) # p == 2 euclidean</code></pre><h2 id="pearsons-correlation-similarity">Pearson's correlation similarity</h2><p class="new">Person's correlation coefficient, a common statistic for linear regression, 
can also be used to measure how two items are correlated.</p><img alt="math?math=%5CLarge%20r_%7Buv%7D%20%3D%20%5Cfrac%7B%5Csum_%7Bi%3D1%7D%5En(u_i%20-%20%5Coverline%7Bu%7D)(v_i%20-%20%5Coverline%7Bv%7D)%7D%7B%5Csqrt%7B%5Csum_%7Bi%3D1%7D%5En(u_i%20-%20%5Coverline%7Bu%7D)%5E2%7D%5Csqrt%7B%5Csum_%7Bi%3D1%7D%5En(v_i%20-%20%5Coverline%7Bv%7D)%5E2%7D%7D" class=" formula-block" src="https://render.githubusercontent.com/render/math?math=%5CLarge%20r_%7Buv%7D%20%3D%20%5Cfrac%7B%5Csum_%7Bi%3D1%7D%5En(u_i%20-%20%5Coverline%7Bu%7D)(v_i%20-%20%5Coverline%7Bv%7D)%7D%7B%5Csqrt%7B%5Csum_%7Bi%3D1%7D%5En(u_i%20-%20%5Coverline%7Bu%7D)%5E2%7D%5Csqrt%7B%5Csum_%7Bi%3D1%7D%5En(v_i%20-%20%5Coverline%7Bv%7D)%5E2%7D%7D"/><p class="new">It's range is from -1 (negative correlation) to 0 (no correlation) to +1 (positive 
correlation). The higher the correlation, the higher the similarity.
<img alt="r.png" class="center my-3 iw600" src="https://raw.githubusercontent.com/habermanUIUC/CodeStoryLessons/main/lessons/dmap/ml/distance/html/r.png"/></p><h1 class="section" id="section5">Distance Metrics with Scipy</h1><p class="new">The distance metrics discussed (and a few more) are all available in scipy.</p><div class="ide code-starter clearfix"><pre><code>from scipy.spatial import distance
import scipy.stats

# defining the points
point_1 = (1, 2, 3)
point_2 = (4, 5, 6)

l1_dist = distance.cityblock(point_1, point_2)
print('Manhattan Distance {}, {} = {:.4f}'.format(point_1, point_2, l1_dist))

l2_dist = distance.euclidean(point_1, point_2)
print('Euclidean Distance {}, {} = {:.4f}'.format(point_1, point_2, l2_dist))

ln_dist = distance.minkowski(point_1, point_2, p=3)
print('Minkowski Distance {}, {} = {:.4f}'.format(point_1, point_2, ln_dist))

r, p_value = scipy.stats.pearsonr(point_1, point_2)
print('Pearson r {}, {} = {:.4f}'.format(point_1, point_2, r))</code></pre></div><h1 class="section" id="section6">Vector Math</h1><p class="new">We'll keep this section short, but it's important to know what's happening when 
you are working with vectors.</p><h2 id="multiplying-vectors">Multiplying Vectors</h2><p class="new">As mentioned, the dot product (or inner product) is when you multiply two vectors 
together. It is defined as </p><img alt="math?math=%5CLarge%20u%20%5Ccdot%20v%20%3D%20(u_1%2C%20...%2C%20u_n)%20%5Ccdot%20(v_1%2C%20...%2C%20v_n)%20%3D%20u_1%20v_1%20%2B%20...%20%2B%20u_n%20v_n" class=" formula-block" src="https://render.githubusercontent.com/render/math?math=%5CLarge%20u%20%5Ccdot%20v%20%3D%20(u_1%2C%20...%2C%20u_n)%20%5Ccdot%20(v_1%2C%20...%2C%20v_n)%20%3D%20u_1%20v_1%20%2B%20...%20%2B%20u_n%20v_n"/><h3 id="column-x-row">Column x Row</h3><p class="new">The dot product whose value is a scalar (1 dimensional) can be thought of as
 a row vector multiplied by a column vector. </p><img alt="rowXcol.png" class="center iw500" src="https://raw.githubusercontent.com/habermanUIUC/CodeStoryLessons/main/lessons/dmap/ml/distance/html/rowXcol.png"/><div class="ide code-starter clearfix"><pre><code>u = np.array([2,3,4])
v = np.array([6,4,3])
print(np.dot(u,v))   # output is scalar
print(np.inner(u,v)) # same as dot</code></pre></div><h3 id="transposing-vectors">Transposing Vectors</h3><p class="new">When you transpose a vector, you change it from 'row' form to 'column' form
 (and vica-versa). Each numpy vector has a <code>.T</code> attribute which represents
 the transpose:</p><img alt="transpose.png" class="center iw500" src="https://raw.githubusercontent.com/habermanUIUC/CodeStoryLessons/main/lessons/dmap/ml/distance/html/transpose.png"/><div class="ide code-starter clearfix"><pre><code>u = np.array([[2,3,4]])
print(u)
print(u.T)
row = u.reshape(1,-1)
col = u.reshape(-1,1)
print(row)
print(col)</code></pre></div><p class="new">You can now see that a vector multiplied by it's transpose is the square of
 the L2 norm.  So using the transpose is sometimes included in both the
 definitions of dot product and L2 definitions:</p><div class="ide code-starter clearfix"><pre><code>def all_the_same():
 u = np.array([2,3,4])
 row = u.reshape(1,-1)
 print(row)
 print(row.T)

 print(np.sqrt(np.dot(row, row.T)))
 print(np.sqrt(np.dot(u, u.T)))
 print(LA.norm(u))
 print(np.sqrt(np.sum(u * u.T)))</code></pre></div><p class="new"> </p><h3 id="row-x-column">Row x Column</h3><p class="new">Taking a column vector and multiplying it by a row vector, actually
 produces a matrix (rows and columns)</p><img alt="colXrow.png" class="center iw500 mb-4" src="https://raw.githubusercontent.com/habermanUIUC/CodeStoryLessons/main/lessons/dmap/ml/distance/html/colXrow.png"/><pre><code>u = np.array([2,3,4])
v = np.array([6,4,3])

row = u.reshape(1,-1)
col = v.reshape(-1,1)
print(row)
print(col)

print(np.multiply(col, row)) # output is a matrix
print(np.outer(v,u))         # same but with vectors</code></pre><h1 class="section" id="section7">Matrix Math</h1><p class="new">Until we start working on more complex data, we'll keep the review on matrix
 math even shorter.</p><h2 id="matrix-multiplication">Matrix Multiplication</h2><p class="new">The rules for multiplying matrices are similar (but there are restrictions on
 the shapes of what matrices can be multiplied together.</p><img alt="matrixMult.png" class="center iw500 mb-4" src="https://raw.githubusercontent.com/habermanUIUC/CodeStoryLessons/main/lessons/dmap/ml/distance/html/matrixMult.png"/><h2 id="transposing-matrices">Transposing Matrices</h2><p class="new">Similarly, you can transpose a matrix as well:</p><img alt="transpose2.png" class="center iw300" src="https://raw.githubusercontent.com/habermanUIUC/CodeStoryLessons/main/lessons/dmap/ml/distance/html/transpose2.png"/><h2 id="cross-product--and-so-much-more">Cross Product ... (and so much more)</h2><p class="new">Another <em>kind</em> of vector multiplication is the <em>cross product</em>. The result of 
a cross product is another vector (sometimes called a vector product) that is at
 right angles to both. </p><img alt="cross.png" class="center iw300" src="https://raw.githubusercontent.com/habermanUIUC/CodeStoryLessons/main/lessons/dmap/ml/distance/html/cross.png"/><p class="new">You can solve for the perpendicular distance from a line to a point (useful
 in linear regression), by calculating a cross product between the different
 line segments.</p><p class="new">There's certainly more to discuss when you dabble or dive in linear algebra. But 
we will re-visit more topics as they become necessary to understand a situation.</p><h1 class="section" id="section8">Review</h1><div class="font-sans container mt-1 mb-4 "><p>üéóBefore you go, you should <strong>know</strong>:</p><div class="w-2/3 md:w-2/3"><div class="shadow-md"><div class="tab overflow-hidden border-t bg-green-200"><input class="absolute opacity-0" id="tab-multi-40" name="tabs" type="checkbox"/><label class="block p-3 leading-normal cursor-pointer" for="tab-multi-40"><span> What is the L1 norm? </span></label><div class="tab-content overflow-hidden border-l-2 bg-orange-300 border-indigo-500 leading-normal"><p class="p-3">No Answer</p></div></div><div class="tab overflow-hidden border-t bg-green-200"><input class="absolute opacity-0" id="tab-multi-41" name="tabs" type="checkbox"/><label class="block p-3 leading-normal cursor-pointer" for="tab-multi-41"><span> What is the L2 norm? </span></label><div class="tab-content overflow-hidden border-l-2 bg-orange-300 border-indigo-500 leading-normal"><p class="p-3">No Answer</p></div></div><div class="tab overflow-hidden border-t bg-green-200"><input class="absolute opacity-0" id="tab-multi-42" name="tabs" type="checkbox"/><label class="block p-3 leading-normal cursor-pointer" for="tab-multi-42"><span> What does normalizing a vector mean? </span></label><div class="tab-content overflow-hidden border-l-2 bg-orange-300 border-indigo-500 leading-normal"><p class="p-3">No Answer</p></div></div><div class="tab overflow-hidden border-t bg-green-200"><input class="absolute opacity-0" id="tab-multi-43" name="tabs" type="checkbox"/><label class="block p-3 leading-normal cursor-pointer" for="tab-multi-43"><span> What is the range of the cosine distance? </span></label><div class="tab-content overflow-hidden border-l-2 bg-orange-300 border-indigo-500 leading-normal"><p class="p-3">No Answer</p></div></div></div></div></div><h1 class="section" id="section9">Lesson Assignment</h1><p class="new">We'll be using a few of these metrics in other lessons.  Be sure you are
 comfortable with the material. You can submit your notebook to gradescope
 to acknowledge you read the material.</p><h1>Test and Submit</h1><p>Once you have finished, you can download your code (via <code>ide.tester</code>) and upload that file to Gradescope (find lesson with tag <strong>distance</strong>).</p><div class="my-4"><pre><code><strong># to list the tests available</strong><br/>print(ide.tester.list_tests())<br/><strong># to perform a specific test</strong><br/>print(ide.tester.test_functionality('name of test'))<br/><strong># to test your code (either works)</strong><br/>print(ide.tester.test_notebook())<br/>print(ide.tester.test_notebook(verbose=True))<br/><strong># to prepare and download your code</strong><br/>ide.tester.download_solution()</code></pre></div><div class="lesson-footer flex bg-gray-200 justify-center"><div class="lesson-footer-card displaycard bg-blue-200 border-t border-gray-400 max-w-2xl rounded overflow-hidden shadow-lg"><div class="px-6 py-4"><div class="title-text text-center font-bold text-xl">Distance Metrics</div><p class="text-center text-gray-800 text-xl">Far being the cost</p><div class="text-center mt-6 text-xl"><i aria-hidden="true" class="fas fa-tags"></i> any questions on Piazza with <span class="font-bold">distance</span></div><div class="text-gray-700 text-base">¬†</div><div></div><div class="text-gray-700 text-base">References and Additional Readings</div><div class="text-xs p-2 border border-solid border-gray-500 bg-gray-300"> <div class="text-gray-700 px-4 m-2">‚Ä¢ <!-- --> <a href="https://www.mathsisfun.com/algebra/vectors.html" target="_blank">https://www.mathsisfun.com/algebra/vectors.html</a></div><div class="text-gray-700 px-4 m-2">‚Ä¢ <!-- --> <a href="https://medium.com/@Jernfrost/why-does-matrix-multiplication-work-the-way-it-does-7a8ed9739254" target="_blank">https://medium.com/@Jernfrost/why-does-matrix-multiplication-work-the-way-it-does-7a8ed9739254</a></div><div class="text-gray-700 px-4 m-2">‚Ä¢ <!-- --> <a href="https://towardsdatascience.com/why-is-the-inner-product-of-orthogonal-vectors-zero-88469043decf" target="_blank">https://towardsdatascience.com/why-is-the-inner-product-of-orthogonal-vectors-zero-88469043decf</a></div><div class="text-gray-700 px-4 m-2">‚Ä¢ <!-- --> <a href="https://www.mathsisfun.com/algebra/vectors-dot-product.html" target="_blank">https://www.mathsisfun.com/algebra/vectors-dot-product.html</a></div></div><div class="flex mt-4 border-t border-solid border-gray-500 justify-around bg-gray-200"><div class="text-gray-700 text-center px-4 m-2 text-sm">D.M. &amp; the üêç</div><div class="text-gray-700 text-center px-4 m-2 text-sm"><strong>Version:</strong> <!-- -->SP21</div></div><div class="text-gray-700 mt-2 text-center text-sm font-bold">All Rights Reserved Michael Haberman</div><div class="text-gray-700 text-center text-sm">Do not distribute this notebook</div></div></div></div><div>¬†</div><div class="ide code-starter clearfix"><pre><code># print(ide.tester.test_notebook()) 
# print(ide.tester.test_notebook(verbose=True)) 

# once you are ready -- run this 
# ide.tester.download_solution() 
</code></pre></div></div></div></body></html>