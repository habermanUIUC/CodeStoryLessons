<!DOCTYPE html><html lang='en'><head><title>KNN</title><meta charset="utf-8"><style>/*! normalize.css v8.0.1 | MIT License | github.com/necolas/normalize.css */html{line-height:1.15;-webkit-text-size-adjust:100%}body{margin:0}h1{font-size:2em;margin:.67em 0}pre{font-family:monospace,monospace;font-size:1em}a{background-color:transparent}strong{font-weight:bolder}code{font-family:monospace,monospace;font-size:1em}img{border-style:none}::-webkit-file-upload-button{-webkit-appearance:button;font:inherit}h1,h2,h3,p,pre{margin:0}ol,ul{list-style:none;margin:0;padding:0}html{font-family:system-ui,-apple-system,BlinkMacSystemFont,"Segoe UI",Roboto,"Helvetica Neue",Arial,"Noto Sans",sans-serif,"Apple Color Emoji","Segoe UI Emoji","Segoe UI Symbol","Noto Color Emoji";line-height:1.5}*,::after,::before{box-sizing:border-box;border-width:0;border-style:solid;border-color:#e2e8f0}img{border-style:solid}h1,h2,h3{font-size:inherit;font-weight:inherit}a{color:inherit;text-decoration:inherit}code,pre{font-family:Menlo,Monaco,Consolas,"Liberation Mono","Courier New",monospace}img{display:block;vertical-align:middle}img{max-width:100%;height:auto}.bg-gray-200{--bg-opacity:1;background-color:#edf2f7;background-color:rgba(237,242,247,var(--bg-opacity))}.bg-gray-300{--bg-opacity:1;background-color:#e2e8f0;background-color:rgba(226,232,240,var(--bg-opacity))}.bg-blue-200{--bg-opacity:1;background-color:#bee3f8;background-color:rgba(190,227,248,var(--bg-opacity))}.bg-blue-300{--bg-opacity:1;background-color:#90cdf4;background-color:rgba(144,205,244,var(--bg-opacity))}.border-gray-500{--border-opacity:1;border-color:#a0aec0;border-color:rgba(160,174,192,var(--border-opacity))}.rounded{border-radius:.25rem}.rounded-full{border-radius:9999px}.border-solid{border-style:solid}.border-t{border-top-width:1px}.inline-block{display:inline-block}.flex{display:flex}.justify-center{justify-content:center}.justify-around{justify-content:space-around}.float-right{float:right}.float-left{float:left}.clearfix:after{content:"";display:table;clear:both}.clear-both{clear:both}.font-serif{font-family:Georgia,Cambria,"Times New Roman",Times,serif}.font-semibold{font-weight:600}.font-bold{font-weight:700}.h-64{height:16rem}.text-xs{font-size:.75rem}.text-sm{font-size:.875rem}.text-base{font-size:1rem}.text-xl{font-size:1.25rem}.leading-none{line-height:1}.m-2{margin:.5rem}.my-1{margin-top:.25rem;margin-bottom:.25rem}.mt-1{margin-top:.25rem}.mt-2{margin-top:.5rem}.mr-2{margin-right:.5rem}.mb-2{margin-bottom:.5rem}.mr-3{margin-right:.75rem}.mb-3{margin-bottom:.75rem}.ml-3{margin-left:.75rem}.max-w-sm{max-width:24rem}.object-contain{-o-object-fit:contain;object-fit:contain}.overflow-hidden{overflow:hidden}.p-1{padding:.25rem}.py-1{padding-top:.25rem;padding-bottom:.25rem}.py-2{padding-top:.5rem;padding-bottom:.5rem}.px-3{padding-left:.75rem;padding-right:.75rem}.py-4{padding-top:1rem;padding-bottom:1rem}.px-4{padding-left:1rem;padding-right:1rem}.px-6{padding-left:1.5rem;padding-right:1.5rem}.pl-3{padding-left:.75rem}.shadow-lg{box-shadow:0 10px 15px -3px rgba(0,0,0,.1),0 4px 6px -2px rgba(0,0,0,.05)}.shadow-inner{box-shadow:inset 0 2px 4px 0 rgba(0,0,0,.06)}.text-left{text-align:left}.text-center{text-align:center}.text-black{--text-opacity:1;color:#000;color:rgba(0,0,0,var(--text-opacity))}.text-gray-700{--text-opacity:1;color:#4a5568;color:rgba(74,85,104,var(--text-opacity))}.text-gray-800{--text-opacity:1;color:#2d3748;color:rgba(45,55,72,var(--text-opacity))}.whitespace-no-wrap{white-space:nowrap}.w-1\/2{width:50%}.w-full{width:100%}.text-tiny{font-size:.5rem!important}body{color:#000!important;font-size:1.25rem!important}.main-content{max-width:900px}.lesson{padding-left:15px!important;padding-right:10px!important;--bg-opacity:1;background-color:#edf2f7;background-color:rgba(237,242,247,var(--bg-opacity))}.main-content,html{font-family:Arial,Georgia,Verdana,"Times New Roman"!important}.lesson-overview-card{font-family:"Times New Roman"!important}.lesson ol{list-style-type:decimal;list-style-position:inside;margin-left:1em}.lesson ul{list-style-position:inside;list-style-type:none;margin-left:1em}.lesson ul li{padding-left:1em;padding-right:5px}.lesson ul li::before{content:"‚Ä¢";padding-right:5px}span{white-space:nowrap}p.new{padding-top:0;padding-bottom:.5em}p.new+p{padding-top:.5em}h1,h2,h3{font-weight:700;margin-top:.25em!important;margin-bottom:.05em!important;font-family:Georgia,Cambria,"Times New Roman",Times,serif!important}h1{font-size:2em!important;clear:both;color:#000!important}div+h1,h2{margin-top:0!important}h2{margin-top:.5em!important;font-size:1.5em!important;clear:both;color:#8b0000!important}h3{font-size:1.25em!important;clear:both;color:#006400!important}ul{margin-bottom:30px}p.new a{text-decoration:underline}.lesson a{text-decoration:underline;color:#00f}.title-text{font-size:2rem}.shadow-lg{box-shadow:0 10px 15px -3px rgba(0,0,0,.1),0 4px 6px -2px rgba(0,0,0,.5)}img.formula-block{margin-left:auto;margin-right:auto;margin-top:.25rem;margin-bottom:.75rem}img.formula-inline{margin-top:.25rem;margin-bottom:.25rem}img.border2{border:1px solid #94add4;margin-top:.5rem;margin-bottom:.75rem}img.iw400{height:auto;width:auto;max-width:400px}img.iw200{height:auto;width:auto;max-width:200px}code{font-size:smaller}pre code{font-size:15px}pre code:not(.line-number){background:#f4f4f4;font-family:monospace;font-size:15px;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;cursor:default;touch-action:none;-webkit-touch-callout:none;-webkit-tap-highlight-color:transparent;clear:both;border:1px solid #ddd;color:#666;page-break-inside:avoid;display:block;min-width:840px;max-width:840px;overflow:scroll;line-height:1.6;margin-bottom:1.6em;padding:1em 1.5em;-moz-tab-size:2;-o-tab-size:2;tab-size:2;word-wrap:break-word;white-space:pre-wrap;border-left:3px solid #f36d33}div.code-starter>pre code{border-left:3px solid #fdff44!important;background-image:radial-gradient(rgba(0,150,0,.75),#000 120%);color:#fff;font:.9rem Inconsolata,monospace}div.code-starter>pre code::after{content:"\a$_"}</style>
<script src="https://kit.fontawesome.com/7efc4bcee2.js" crossOrigin="anonymous"></script>
<script>
    let stateCheck = setInterval(function(){
      if (document.readyState === 'complete') {
        clearInterval(stateCheck);
        let s1 = document.getElementById('start');
        // console.log('doc is ready', s1);
        if (s1) {
           s1.setAttribute('tabindex', '-1');
           s1.focus(); 
           s1.scrollIntoView({behavior: 'smooth'}); 
           setTimeout(function(){s1.blur()}, 500);
           // console.log('focus set');
        }
      }
    }, 200);
    </script>
</head><body class="lesson"><div class="main-content lesson bg-gray-200 text-black p-1 pl-3 font-serif"><div class="md-inner">
<div id="start" class="section">&nbsp;</div><h1 class="overview"></h1><div class="lesson-overview bg-gray-200 flex justify-center"><div class="text-center px-4 py-2 m-2"><div class="lesson-overview-card displaycard bg-blue-200 max-w-sm rounded overflow-hidden shadow-lg"><div>¬†</div><img alt="Text" class="object-contain h-64 w-full" src="https://raw.githubusercontent.com/habermanUIUC/CodeStoryLessons/main/lessons/dmap/mls/knn/html/MachineLearningV2-sm.png"/><div class="px-6 py-4"><div class="title-text text-center leading-none font-bold text-xl">KNN</div><p class="text-center mt-2 text-gray-800 text-xl">Won‚Äôt you be my Neighbor?</p><div class="text-gray-700 text-base">¬†</div><div class="text-center mb-3"><span class="inline-block bg-gray-300 rounded-full px-3 py-1 text-sm font-semibold text-gray-700 mr-2">#machine learning</span></div><div class="flex border-t border-solid border-gray-500 shadow-inner justify-around bg-blue-300"><div class="text-gray-700 text-center px-4 m-2 text-sm"><span class="whitespace-no-wrap">D.M. &amp; the üêç</span></div><div class="text-gray-700 text-center px-4 m-2 text-sm"><span class="whitespace-no-wrap"><strong>Version:</strong> <!-- -->SP21</span></div></div><div class="text-gray-700 mt-1 text-center text-tiny">All Rights Reserved</div></div></div></div><div class="text-center px-4 py-2 m-2 w-1/2"><div class="displaycard bg-gray-200 max-w-sm rounded overflow-hidden shadow-lg"><div class="px-6 py-4 text-left"><div class="text-center font-bold text-xl">KNN<br/><div><span>prerequisites</span><div class="text-center text-xs mb-2">(start only after finishing)</div><p class="max-w-sm text-gray-800 text-sm">‚¶ø <strong>mlprep</strong></p></div></div></div><div class="px-6 py-4 text-left text-gray-800"><div class="text-center font-bold text-xl">Colab Notes</div><p class="max-w-sm text-sm">1. <strong>Copy</strong> this notebook <img alt="copy2drive.png" class="inline-block" src="https://raw.githubusercontent.com/habermanUIUC/CodeStoryLessons/main/lessons/dmap/mls/knn/html/copy2drive.png"/></p><p class="max-w-sm text-sm">2. <strong>Update</strong> the <strong><code>NET_ID</code></strong> in the notebook</p><p class="max-w-sm text-gray-800 text-sm">3. <strong>Hit ‚ñ∂Ô∏è¬†</strong> to install the INFO 490 IDE</p><div class="text-center font-bold text-xl">¬†</div><div class="text-center font-bold text-xl">Jupyter/PyCharm Notes</div><p class="max-w-sm text-gray-800 text-sm text-left">The testing framework does <strong>not work</strong> (at this time) for Jupyter  notebooks or local code development.</p></div></div></div></div><h1 class="section" id="section1">KNN</h1><h2 id="wont-you-be-my-neighbor">Won‚Äôt you be my Neighbor?</h2><p class="new">K-Nearest Neighbors or (a.k.a KNN) is an algorithm that provides a good 
access point into machine learning.  The algorithm itself is not too
 difficult, but in order for it to perform well, it does require some of the
  same principals used across many different techniques. It lends itself 
  to understanding the entire process (the ML pipeline) from preparing the data to 
  the necessity of splitting the data into training the testing sets. It also 
  provides a great context in which we can discuss a lot of machine learning vocabulary.</p><p class="new">The ubiquity of KNN provides an accessible benchmark for which to compare other more complex
  classifiers as well. However, after you see how it works, you may ask yourself 
  'How is this even machine learning?'  We will <em>learn</em> about that too.</p><h3 id="supervised">Supervised</h3><p class="new">KNN is a supervised ML technique that is used mostly for classification. The 
goal of KNN is to determine which class an unseen instance (or data point) belongs to. 
It can also be used for regression as well (predicting a numeric value).  It is 
supervised in that it needs labeled data to indicate for which class each instance belongs.</p><h3 id="voting-on-similarity">Voting on Similarity</h3><p class="new">The KNN algorithm essentially picks the closest <em>K</em> neighbors to an unseen
 observation (the unlabeled data point). These <em>K</em> neighbors (K is odd) then
  vote to declare to whom the new point is more similar.  Majority voting is
   used.  For example, if K is 5, the 5 closest neighbors are selected.  If 3
    of them are 'yellow', then the new point is deemed 'yellow'.</p><p class="new">Here's <em>very</em> rough outline of how the algorithm works:</p><pre><code>K=5
# init with some large distances 
top5 = [2**32 for i in range(0,K)] 
for element in dataset:
   dist = calculate_distance(element, new_instance)
   add_if_closer(element, dist, top5)
best = get_majority(top5)</code></pre><h3 id="metrics-on-similarity">Metrics on Similarity</h3><p class="new">We will have a dedicated lesson on distance metrics, but a common one for KNN
 is to use euclidean distance.  The distance between each attribute is
  calculated, squared and summed up.  The square root of the result is taken
   as a proxy for how close two points are.  Of course, this implies the
    attributes themselves are numeric.  </p><img alt="math?math=%5CLarge%20d(u%2C%20v)%20%3D%20%5Csqrt%7B%5Cleft(u_1%20-%20v_1%20%5Cright)%5E2%20%2B%20%5Cdotsc%20%2B%20%5Cleft(u_n%20-%20v_n%20%5Cright)%5E2%7D" class=" formula-block" src="https://render.githubusercontent.com/render/math?math=%5CLarge%20d(u%2C%20v)%20%3D%20%5Csqrt%7B%5Cleft(u_1%20-%20v_1%20%5Cright)%5E2%20%2B%20%5Cdotsc%20%2B%20%5Cleft(u_n%20-%20v_n%20%5Cright)%5E2%7D"/><p class="new">This is also known as the L2 Distance:
  <img alt="math?math=%5CLarge%20L2%3D%7B%5Cleft%5CVert%7B%7D%7BD%7D%5Cright%5CVert%7B%7D%7D_2%20%3D%20%5Csqrt%7B%5Csum_%7Bi%3D1%7D%5En%7B(u_i-v_i)%7D%5E2%7D" class=" formula-block" src="https://render.githubusercontent.com/render/math?math=%5CLarge%20L2%3D%7B%5Cleft%5CVert%7B%7D%7BD%7D%5Cright%5CVert%7B%7D%7D_2%20%3D%20%5Csqrt%7B%5Csum_%7Bi%3D1%7D%5En%7B(u_i-v_i)%7D%5E2%7D"/></p><p class="new">The L2 distance scales well to using multiple attributes and is easy to compute.</p><p class="new">K-Nearest Neighbors assumes that similar items will be close together (in
 high dimensional space). So those items whose have the same attributes in
  common (or very close to each other), will be of the same class -- 
"Birds of a feather, flock together" or "Show me your friends, and I'll tell you who you are".</p><h2 id="a-k5-walk-through">A <em>K</em>5 Walk through</h2><img alt="voronoi1.png" class="float-left mr-3 iw200 border2" src="https://raw.githubusercontent.com/habermanUIUC/CodeStoryLessons/main/lessons/dmap/mls/knn/html/voronoi1.png"/><p class="new">The following is a visual walk through of KNN with <code>K=5</code>.  KNN will use the
 dataset shown to the left on which to train.</p><ul><li>19 data points (8 red, 11 blue)</li></ul><br class="clear-both"/><img alt="voronoi2.png" class="float-left mr-3 iw200 border2" src="https://raw.githubusercontent.com/habermanUIUC/CodeStoryLessons/main/lessons/dmap/mls/knn/html/voronoi2.png"/><p class="new">We then ask the model to classify a new point </p><ul><li>shown as the orange circle</li><li>the model needs to respond with either 'red' or 'blue':</li></ul><br class="clear-both"/><img alt="withOrange.png" class="float-left mr-3 iw200 border2" src="https://raw.githubusercontent.com/habermanUIUC/CodeStoryLessons/main/lessons/dmap/mls/knn/html/withOrange.png"/><p class="new">KNN would then select the following points</p><ul><li>the 5 closest points are selected (i.e. <code>K=5</code>) based on L2 distance</li><li>the majority vote is 'blue'</li><li>the new point gets assigned the label 'blue'</li></ul><h2 id="decision-boundaries">Decision Boundaries</h2><img alt="hyperplane.png" class="float-right ml-3 iw400 border2" src="https://raw.githubusercontent.com/habermanUIUC/CodeStoryLessons/main/lessons/dmap/mls/knn/html/hyperplane.png"/><p class="new">The goal of many ML algorithms is to generate a decision boundary (e.g. line, 
hyperplane, surface) that partitions the instances into different spaces (one 
for each class).</p><p class="new">If a point lies on a boundary, it's not clear which class it belongs to. The 
boundary itself is ambiguous as the output label of a classifier could be either class.</p><p class="new">If the decision surface is a hyperplane, then the classification problem is linear, 
and the classes are said to be <strong><em>linearly separable</em></strong>.</p><h3 id="k1-decision-boundaries">K1 Decision Boundaries</h3><img alt="voronoi3.png" class="float-left mr-3 iw400 border2" src="https://raw.githubusercontent.com/habermanUIUC/CodeStoryLessons/main/lessons/dmap/mls/knn/html/voronoi3.png"/><p class="new">Going back to our example, if K were 1, each point would have it's own decision
 boundary.  In fact a Voronoi diagram partitions a space in such a way that
  each region shares characteristics with KNN.</p><p class="new">In this example, each point defines a region such that if another point fell into 
its region, it would be the closest point to the new point. Each data point 
has a region for which it will vote for itself. </p><p class="new">KNN creates a decision boundary for classification.  In fact it can create 
very complicated precise decision boundaries and is one of the reasons why 
over-fitting can be an issue.  </p><h2 id="special-k">Special K</h2><img alt="specialK.jpeg" class="float-left mr-3 iw200" src="https://raw.githubusercontent.com/habermanUIUC/CodeStoryLessons/main/lessons/dmap/mls/knn/html/specialK.jpeg"/><p class="new">You maybe now be asking, "so how to pick the right K?".  Differnt <em>K</em> values
 will result in different decision boundaries.</p><p class="new"><em>K</em> for this algorithm is a hyper-parameter -- a parameter that is not
 learned by the algorithm.  Hyper-parameters are selected through experience, 
 rules-of-thumb (best practices) and experimentation.</p><h3 id="start-with-sqrt">start with <code>sqrt</code></h3><p class="new">A good starting point is having K be equal to the square root of the number
 of instances in the data: <img alt="math?math=%5Clarge%20K%3Dint(%5Csqrt%7Bn%7D)" class="my-1 formula-inline" src="https://render.githubusercontent.com/render/math?math=%5Clarge%20K%3Dint(%5Csqrt%7Bn%7D)" style="display:inline-block"/></p><h3 id="make-it-odd">make it odd</h3><p class="new">However if K is even, you will decrease it by one.  Since voting is done by
 majority rules, an odd K will relieve you from breaking ties.</p><p class="new">Another adjustment is to make sure the K isn't equal to the number of classes
 you have. In a multi-class situation, you can still end up with ties. For 
 example, if you have 3 classes and K is three:</p><ol start="1"><li>red</li><li>green</li><li>blue</li></ol><p class="new">Who would win?
So K must not be a multiple of the number of classes in your labels. </p><h3 id="finding-k">finding K</h3><p class="new">Once we understand how to evaluate the KNN results, you can use a metric and
 just iterate over different values of K, selecting the one that gives you
  the best performance.</p><h1 class="section" id="section2">Enough is Enough, where's the code?</h1><p class="new">With <em>all</em> of that behind us, let's take a look at how we can use sklern to
 <em>easily</em> get KNN up and running.</p><p class="new">We will use a popular dataset from the UCI Machine Learning Repository -- 
although the original dataset seems to be <a href="https://archive.ics.uci.edu/ml/support/diabetes" target="_blank">missing</a>.</p><p class="new">That dataset is available via the <code>LessonUtil</code> module:</p><div class="ide code-starter clearfix"><pre><code>
</code></pre></div><p class="new"> </p><p class="new"> 
For this example we are using the 
# Testing and Training
importance of splitting the data</p><h1 class="section" id="section3">Model Evaluation</h1><p class="new">Confusion Matrix F1
accuracy vs precision, etc</p><h3 id="non-parametric">Non parametric</h3><p class="new">Although KNN does indeed have a hyper-parameter (i.e. <em>K</em>), because it makes
 no assumptions on distribution of the underlying data (is it normal, bi
 -modal, random) it is considered to be a <em>non-parametric</em> algorithm.  In
  this context, the word 'parameter/parametric' refers to any necessary
   parameters needed to describe the data it can work with.
   Some learning models assume a Gaussian distribution and if the underlying
    data were non-Gaussian, the algorithm would make poor predictions (or
     even dangerous ones -- depending on the situation).</p><h3 id="instance-based">Instance Based</h3><p class="new">KNN does not explicitly learn a model that can be used later to make
 predictions.  In stead in memorizes (or uses) the training instances as its
  knowledge base.  So KNN is trivial to build (it costs nothing), but can be
   computational expensive (and have a large memory footprint) to use.</p><h1 class="section" id="section4">Where's the learning?o</h1><p class="new">It doesn't really "learn" from the training data, it just compares predictors 
to the nearby training data.</p><p class="new">That is one way to learn, don't you agree? It's called lazy learning. 
The type of learning we typically think of is called eager learning (the one that 
actually involves learning), but both are considered learning methods.</p><p class="new">Edit: I remember a nice analogy from my professor: think of writing an exam 
next week where you can bring whatever you want. The lazy way, the KNN way, 
is to bring all the exercises you did in class and look for similar tasks when 
you start the exam in order to answer the questions. The eager way involves 
studying the material up front. Both are valid methods to approach the exam.</p><p class="new">It is a lazy learner because it does not have a training phase but rather 
memorizes the training dataset. All computations are delayed until classification.</p><p class="new">Case-Based Learning Algorithm -The algorithm uses raw training instances from the 
problem domain to make predictions and is often referred to as an 
instance based or case-based learning algorithm. 
Case-based learning implies that KNN does not explicitly learn a model. 
Rather it memorizes the training instances/cases which are then used as ‚Äúknowledge‚Äù 
for the prediction phase. Given an input, when we ask the algorithm to predict a label, 
it will make use of the memorized training instances to give out an answer.</p><p class="new">KNN is a non-parametric and lazy learning algorithm. 
Non-parametric means there is no assumption for underlying data distribution. 
In other words, the model structure determined from the dataset</p><p class="new">from <a href="https://sebastianraschka.com/faq/docs/lazy-knn.html" target="_blank">https://sebastianraschka.com/faq/docs/lazy-knn.html</a>
K-NN is a lazy learner because it doesn‚Äôt learn a discriminative function from the training data but ‚Äúmemorizes‚Äù the training dataset instead.
 the logistic regression algorithm learns its model weights (parameters) during training time. In contrast, there is no training time in K-NN. Although this may sound very convenient, this property doesn‚Äôt come without a cost: The ‚Äúprediction‚Äù step in K-NN is relatively expensive! Each time we want to make a prediction, K-NN is searching for the nearest neighbor(s) in the entire training set! (Note that there are certain tricks such as BallTrees and KDtrees to speed this up a bit.)
 o</p><h1 class="section" id="section5">decision surface</h1><p class="new">Recommendation systems</p><h1 class="section" id="section6">bias vs variance</h1><p class="new"><a href="https://medium.com/datadriveninvestor/bias-and-variance-in-machine-learning-51fdd38d1f86" target="_blank">https://medium.com/datadriveninvestor/bias-and-variance-in-machine-learning-51fdd38d1f86</a>
<a href="https://towardsdatascience.com/understanding-the-bias-variance-tradeoff-165e6942b229" target="_blank">https://towardsdatascience.com/understanding-the-bias-variance-tradeoff-165e6942b229</a></p></div></div></body></html>