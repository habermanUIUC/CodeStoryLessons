<!DOCTYPE html><html lang='en'><head><title>Neural Networks (part1)</title><meta charset="utf-8"><style>/*! normalize.css v8.0.1 | MIT License | github.com/necolas/normalize.css */html{line-height:1.15;-webkit-text-size-adjust:100%}body{margin:0}h1{font-size:2em;margin:.67em 0}pre{font-family:monospace,monospace;font-size:1em}a{background-color:transparent}strong{font-weight:bolder}code{font-family:monospace,monospace;font-size:1em}img{border-style:none}::-webkit-file-upload-button{-webkit-appearance:button;font:inherit}h1,h2,h3,h4,p,pre{margin:0}ul{list-style:none;margin:0;padding:0}html{font-family:system-ui,-apple-system,BlinkMacSystemFont,"Segoe UI",Roboto,"Helvetica Neue",Arial,"Noto Sans",sans-serif,"Apple Color Emoji","Segoe UI Emoji","Segoe UI Symbol","Noto Color Emoji";line-height:1.5}*,::after,::before{box-sizing:border-box;border-width:0;border-style:solid;border-color:#e2e8f0}img{border-style:solid}h1,h2,h3,h4{font-size:inherit;font-weight:inherit}a{color:inherit;text-decoration:inherit}code,pre{font-family:Menlo,Monaco,Consolas,"Liberation Mono","Courier New",monospace}img{display:block;vertical-align:middle}img{max-width:100%;height:auto}.bg-gray-200{--bg-opacity:1;background-color:#edf2f7;background-color:rgba(237,242,247,var(--bg-opacity))}.bg-gray-300{--bg-opacity:1;background-color:#e2e8f0;background-color:rgba(226,232,240,var(--bg-opacity))}.bg-blue-200{--bg-opacity:1;background-color:#bee3f8;background-color:rgba(190,227,248,var(--bg-opacity))}.bg-blue-300{--bg-opacity:1;background-color:#90cdf4;background-color:rgba(144,205,244,var(--bg-opacity))}.border-gray-400{--border-opacity:1;border-color:#cbd5e0;border-color:rgba(203,213,224,var(--border-opacity))}.border-gray-500{--border-opacity:1;border-color:#a0aec0;border-color:rgba(160,174,192,var(--border-opacity))}.rounded{border-radius:.25rem}.rounded-full{border-radius:9999px}.border-solid{border-style:solid}.border{border-width:1px}.border-t{border-top-width:1px}.inline-block{display:inline-block}.flex{display:flex}.justify-center{justify-content:center}.justify-around{justify-content:space-around}.float-right{float:right}.float-left{float:left}.clearfix:after{content:"";display:table;clear:both}.font-serif{font-family:Georgia,Cambria,"Times New Roman",Times,serif}.font-semibold{font-weight:600}.font-bold{font-weight:700}.h-64{height:16rem}.text-xs{font-size:.75rem}.text-sm{font-size:.875rem}.text-base{font-size:1rem}.text-xl{font-size:1.25rem}.leading-none{line-height:1}.m-2{margin:.5rem}.my-1{margin-top:.25rem;margin-bottom:.25rem}.my-4{margin-top:1rem;margin-bottom:1rem}.mt-1{margin-top:.25rem}.mt-2{margin-top:.5rem}.mr-2{margin-right:.5rem}.mb-2{margin-bottom:.5rem}.mr-3{margin-right:.75rem}.mb-3{margin-bottom:.75rem}.ml-3{margin-left:.75rem}.mt-4{margin-top:1rem}.mt-6{margin-top:1.5rem}.max-w-sm{max-width:24rem}.max-w-2xl{max-width:42rem}.object-contain{-o-object-fit:contain;object-fit:contain}.overflow-hidden{overflow:hidden}.p-1{padding:.25rem}.p-2{padding:.5rem}.p-4{padding:1rem}.py-1{padding-top:.25rem;padding-bottom:.25rem}.py-2{padding-top:.5rem;padding-bottom:.5rem}.px-3{padding-left:.75rem;padding-right:.75rem}.py-4{padding-top:1rem;padding-bottom:1rem}.px-4{padding-left:1rem;padding-right:1rem}.px-6{padding-left:1.5rem;padding-right:1.5rem}.pl-3{padding-left:.75rem}.shadow-lg{box-shadow:0 10px 15px -3px rgba(0,0,0,.1),0 4px 6px -2px rgba(0,0,0,.05)}.shadow-inner{box-shadow:inset 0 2px 4px 0 rgba(0,0,0,.06)}.text-left{text-align:left}.text-center{text-align:center}.text-black{--text-opacity:1;color:#000;color:rgba(0,0,0,var(--text-opacity))}.text-gray-700{--text-opacity:1;color:#4a5568;color:rgba(74,85,104,var(--text-opacity))}.text-gray-800{--text-opacity:1;color:#2d3748;color:rgba(45,55,72,var(--text-opacity))}.whitespace-no-wrap{white-space:nowrap}.w-1\/2{width:50%}.w-full{width:100%}.text-tiny{font-size:.5rem!important}body{color:#000!important;font-size:1.25rem!important}.main-content{max-width:900px}.lesson{padding-left:15px!important;padding-right:10px!important;--bg-opacity:1;background-color:#edf2f7;background-color:rgba(237,242,247,var(--bg-opacity))}.main-content,html{font-family:Arial,Georgia,Verdana,"Times New Roman"!important}.lesson-footer-card,.lesson-overview-card{font-family:"Times New Roman"!important}.lesson-footer{margin-top:50px;margin-top:20px}.lesson ul{list-style-position:inside;list-style-type:none;margin-left:1em}.lesson ul li{padding-left:1em;padding-right:5px}.lesson ul li::before{content:"‚Ä¢";padding-right:5px}span{white-space:nowrap}p.new{padding-top:0;padding-bottom:.5em}p.new+p{padding-top:.5em}h1,h2,h3,h4{font-weight:700;margin-top:.25em!important;margin-bottom:.05em!important;font-family:Georgia,Cambria,"Times New Roman",Times,serif!important}h1{font-size:2em!important;clear:both;color:#000!important}div+h1,h2{margin-top:0!important}h2{margin-top:.5em!important;font-size:1.5em!important;clear:both;color:#8b0000!important}h3{font-size:1.25em!important;clear:both;color:#006400!important}h4{font-size:1em!important;clear:both;color:#00008b!important}ul{margin-bottom:30px}p.new a{text-decoration:underline}.lesson a{text-decoration:underline;color:#00f}.title-text{font-size:2rem}.shadow-lg{box-shadow:0 10px 15px -3px rgba(0,0,0,.1),0 4px 6px -2px rgba(0,0,0,.5)}img.formula-block{margin-left:auto;margin-right:auto;margin-top:.25rem;margin-bottom:.75rem}img.formula-inline{margin-top:.25rem;margin-bottom:.25rem}img.center{-o-object-position:center;object-position:center;margin-left:auto;margin-right:auto}img.border2{border:1px solid #94add4;margin-top:.5rem;margin-bottom:.75rem}img.iw600{height:auto;width:auto;max-width:600px}img.iw400{height:auto;width:auto;max-width:400px}img.iw500{height:auto;width:auto;max-width:500px}img.iw300{height:auto;width:auto;max-width:300px}img.iw200{height:auto;width:auto;max-width:200px}img.iw100{height:auto;width:auto;max-width:100px}code{font-size:smaller}pre code{font-size:15px}pre code:not(.line-number){background:#f4f4f4;font-family:monospace;font-size:15px;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;cursor:default;touch-action:none;-webkit-touch-callout:none;-webkit-tap-highlight-color:transparent;clear:both;border:1px solid #ddd;color:#666;page-break-inside:avoid;display:block;min-width:840px;max-width:840px;overflow:scroll;line-height:1.6;margin-bottom:1.6em;padding:1em 1.5em;-moz-tab-size:2;-o-tab-size:2;tab-size:2;word-wrap:break-word;white-space:pre-wrap;border-left:3px solid #f36d33}div.code-starter>pre code{border-left:3px solid #fdff44!important;background-image:radial-gradient(rgba(0,150,0,.75),#000 120%);color:#fff;font:.9rem Inconsolata,monospace}div.code-starter>pre code::after{content:"\a$_"}</style>
<script src="https://kit.fontawesome.com/7efc4bcee2.js" crossOrigin="anonymous"></script>
<script>
    let stateCheck = setInterval(function(){
      if (document.readyState === 'complete') {
        clearInterval(stateCheck);
        let s1 = document.getElementById('start');
        // console.log('doc is ready', s1);
        if (s1) {
           s1.setAttribute('tabindex', '-1');
           s1.focus(); 
           s1.scrollIntoView({behavior: 'smooth'}); 
           setTimeout(function(){s1.blur()}, 500);
           // console.log('focus set');
        }
      }
    }, 200);
    </script>
</head><body class="lesson"><div class="main-content lesson bg-gray-200 text-black p-1 pl-3 font-serif"><div class="md-inner">
<div id="start" class="section">&nbsp;</div><h1 class="overview"></h1><div class="lesson-overview bg-gray-200 flex justify-center"><div class="text-center px-4 py-2 m-2"><div class="lesson-overview-card displaycard bg-blue-200 max-w-sm rounded overflow-hidden shadow-lg"><div>¬†</div><img alt="Text" class="object-contain h-64 w-full" src="https://raw.githubusercontent.com/habermanUIUC/CodeStoryLessons/main/lessons/dmap/mls/nn1/html/MachineLearningV2-sm.png"/><div class="px-6 py-4"><div class="title-text text-center leading-none font-bold text-xl">Neural Networks (part1)</div><p class="text-center mt-2 text-gray-800 text-xl">The Perceptron and the MLP</p><div class="text-gray-700 text-base">¬†</div><div class="text-center mb-3"><span class="inline-block bg-gray-300 rounded-full px-3 py-1 text-sm font-semibold text-gray-700 mr-2">#machine learning</span></div><div class="flex border-t border-solid border-gray-500 shadow-inner justify-around bg-blue-300"><div class="text-gray-700 text-center px-4 m-2 text-sm"><span class="whitespace-no-wrap">D.M. &amp; the üêç</span></div><div class="text-gray-700 text-center px-4 m-2 text-sm"><span class="whitespace-no-wrap"><strong>Version:</strong> <!-- -->SP21</span></div></div><div class="text-gray-700 mt-1 text-center text-tiny">All Rights Reserved</div></div></div></div><div class="text-center px-4 py-2 m-2 w-1/2"><div class="displaycard bg-gray-200 max-w-sm rounded overflow-hidden shadow-lg"><div class="px-6 py-4 text-left"><div class="text-center font-bold text-xl">Neural Networks (part1)<br/><div><span>prerequisites</span><div class="text-center text-xs mb-2">(start only after finishing)</div><p class="max-w-sm text-gray-800 text-sm">‚¶ø <strong>mlprep</strong></p><p class="max-w-sm text-gray-800 text-sm">‚¶ø <strong>Python classes</strong></p><p class="max-w-sm text-gray-800 text-sm">‚¶ø <strong>linear regression</strong></p><p class="max-w-sm text-gray-800 text-sm">‚¶ø <strong>gradient descent</strong></p></div></div></div><div class="px-6 py-4 text-left text-gray-800"><div class="text-center font-bold text-xl">Colab Notes</div><p class="max-w-sm text-sm">1. <strong>Copy</strong> this notebook <img alt="copy2drive.png" class="inline-block" src="https://raw.githubusercontent.com/habermanUIUC/CodeStoryLessons/main/lessons/dmap/mls/nn1/html/copy2drive.png"/></p><p class="max-w-sm text-sm">2. <strong>Update</strong> the <strong><code>NET_ID</code></strong> in the notebook</p><p class="max-w-sm text-gray-800 text-sm">3. <strong>Hit ‚ñ∂Ô∏è¬†</strong> to install the INFO 490 IDE</p><div class="text-center font-bold text-xl">¬†</div><div class="text-center font-bold text-xl">Jupyter/PyCharm Notes</div><p class="max-w-sm text-gray-800 text-sm text-left">The testing framework does <strong>not work</strong> (at this time) for Jupyter  notebooks or local code development.</p></div></div></div></div><h1 class="section" id="section1">Neural Networks (part1)</h1><h2 id="the-perceptron-and-the-mlp">The Perceptron and the MLP</h2><p class="new">Artificial Neural Networks (ANN) is a üî• topic.  Although neural networks have a 
long history, recent advances including the application of using gradient descent 
and the use of many 'hidden layers' (aka 'deep' networks) to solve non-linear classification
  problems have made it a very popular field for both practical applications
   and research.</p><p class="new">This lesson starts at the beginning and discusses using ANN to solve
 linear classification first. Although if your inputs can be linearly separable, 
 you don't need neutral network; it's a great starting point to introduce how neural
  networks work.</p><h2 id="a-shortened-history-">A Shortened History </h2><h3 id="associationism">Associationism</h3><img alt="plato_aristotle.jpeg" class="iw200 float-left mr-3 mt -2 border2" src="https://raw.githubusercontent.com/habermanUIUC/CodeStoryLessons/main/lessons/dmap/mls/nn1/html/plato_aristotle.jpeg"/><p class="new">The history of neural networks begins when humans first tried to understand
 how they acquired knowledge. Trying to answer the questions of how cognition (learning, 
 solving problems, recognizing patterns, creating) works and how one can emulate
  it have been the driving forces behind machine learning and artificial intelligence (AI). </p><p class="new">As far back as 400 BC, Plato described learning as creating associations. This 
theory (associationism) was further refined for several years (thousands) and 
can be found in the ideas of Pavlov as well). Our ability to think is based on associations.</p><h3 id="connectionism">Connectionism</h3><img alt="bain.png" class="iw200 float-right ml-3 mt-2 border2" src="https://raw.githubusercontent.com/habermanUIUC/CodeStoryLessons/main/lessons/dmap/mls/nn1/html/bain.png"/><p class="new">In the mid 1800's the anatomy of the brain was discovered (w.r.t neurons) to 
be a mass of interconnected neurons. This structure was then 'connected' with a model of
 computation via Alexander Bain called connectionism. Bain claimed that the
  information lies in the connections between the neurons. He
   developed models to show that difference combinations of inputs could 
   result in different outputs. He also came up a learning theory as well.</p><p class="new">The connectionism framework still underlies the neural networks models we see today: 
a network of processing elements (connectionist architecture).</p><h3 id="an-artificial-neuron-">An Artificial Neuron </h3><img alt="m_pitts.jpeg" class="iw100 float-left mr-3 mt-2 border2" src="https://raw.githubusercontent.com/habermanUIUC/CodeStoryLessons/main/lessons/dmap/mls/nn1/html/m_pitts.jpeg"/><p class="new">The actual modeling of the brain started with Warren McCulloch and Walter
 Pitts (MP) who developed a single neuron model (early 1940's).</p><p class="new">The model imitates the functionality of a biological neuron (shown below):</p><img alt="neuron.png" class="center iw600 border2" src="https://raw.githubusercontent.com/habermanUIUC/CodeStoryLessons/main/lessons/dmap/mls/nn1/html/neuron.png"/><p class="new">The main components (for our purposes) are the Dendrites (the incoming signals 
come into the neuron), the Soma (the collection of incoming signals) and the 
Axon where the signal goes out. If the collection of incoming signals 'trips' 
a threshold, it then goes down the Axon. The Synapse is where two neurons 
connect to each other. </p><img alt="neuron_model.png" class="center iw500 border2" src="https://raw.githubusercontent.com/habermanUIUC/CodeStoryLessons/main/lessons/dmap/mls/nn1/html/neuron_model.png"/><p class="new">The first computational model for a individual neuron (the MP neuron) was 
divided into two parts:</p><ul><li>part 1: the inputs are aggregated</li><li>part 2: based on the value, a 0/1 decision (an output) is made</li></ul><img alt="MPitsNeuron.png" class="center iw400 border2" src="https://raw.githubusercontent.com/habermanUIUC/CodeStoryLessons/main/lessons/dmap/mls/nn1/html/MPitsNeuron.png"/><p class="new">The models could represent simple boolean logic gates (operations) such as AND, OR, NOT.  </p><h3 id="hebbian-learning-and-the-perceptron">Hebbian Learning and the Perceptron</h3><p class="new">The MP neuron model was without a process to show learning could occur. In 1949, 
Donald Heb provided a learning mechanism (Hebbian learning) that showed how neurons can work 
 together and improve over time. In a sense, this amounts to how a weight between 
 two neurons can be adjusted to strengthen the connection. </p><p class="new">The same MP neuron model was further refined by Frank Rosenblatt with his Perceptron 
model (1958). It embodies the aspects of the neuron. Each cell has several inputs. 
Each input also has a weight. Each weight is independent of the other
 weights as well. If the weighted sum of the inputs exceeds a threshold, 
it would 'fire'. This threshold function at would then send the weighted sum (or some other value) 
as an output. This was a close representation of the biological analog.</p><img alt="perceptron.png" class="center border2" src="https://raw.githubusercontent.com/habermanUIUC/CodeStoryLessons/main/lessons/dmap/mls/nn1/html/perceptron.png"/><h4 id="activation">Activation</h4><p class="new">An import aspect of the Perceptron is how it decides to excite and fire or simply 
absorb the incoming signals. This aspect is call the threshold or
 <em>activation</em> function. Mathematically, it can be expressed as:</p><img alt="math?math=%5CLarge%20y%20%3D%20%5Cbegin%7Bcases%7D1%20%26%20%5Ctext%7Bif%7D%5Cquad%20%5Csum_%7Bi%3D1%7D%5En%7Bw_i%7D%7Bx_i%7D%20-%20T%20%5Cge%200%20%5C%5C%200%20%26%20%5Ctext%7Botherwise%7D%20%5Cend%7Bcases%7D" class=" formula-block" src="https://render.githubusercontent.com/render/math?math=%5CLarge%20y%20%3D%20%5Cbegin%7Bcases%7D1%20%26%20%5Ctext%7Bif%7D%5Cquad%20%5Csum_%7Bi%3D1%7D%5En%7Bw_i%7D%7Bx_i%7D%20-%20T%20%5Cge%200%20%5C%5C%200%20%26%20%5Ctext%7Botherwise%7D%20%5Cend%7Bcases%7D"/><p class="new">The activation function uses some means to reduce the sum of input values 
to a 1 or a 0 (or a value very close to a 1 or 0) in order to represent activation or lack thereof.
This limit of only producing a 1 or 0 was rectified by using different
 activation functions (see part 2). </p><p class="new">The learning mechanism (sequential learning) was also provided by using examples (of input and 
output pairs) and updating the weights. Convergence (for boolean functions) was shown for 
data containing linearly separable classes.</p><h2 id="linear-separation">Linear Separation</h2><img alt="non-linear.png" class="float-left mr-3 iw300" src="https://raw.githubusercontent.com/habermanUIUC/CodeStoryLessons/main/lessons/dmap/mls/nn1/html/non-linear.png"/><p class="new">This lesson started with a mention of linear classification and linear 
separability.  What does it mean for data to be linear separable? The image shown 
to the left is an example of data that is <strong>not</strong> linear separable.</p><p class="new">When a set of output values can be split by a straight line, the output values 
are said to be linearly separable.  On one side of the 'line' are the
 instances with the one class label; on the other side are the instances with
  the other class label.</p><p class="new">Geometrically, this condition describes the 
situation in which there is a line or hyperplane (in higher dimensions) that separates, in 
the vector space of inputs, the data into separate distinct classes of outputs.
<img alt="linearSep.jpg" class="center iw300 mr-3" src="https://raw.githubusercontent.com/habermanUIUC/CodeStoryLessons/main/lessons/dmap/mls/nn1/html/linearSep.jpg"/></p><h3 id="or-by-example">Or by Example</h3><p class="new">Take a look at the following code that implements the basic Perceptron
 functionality.  The class <code>BooleanPerceptron</code> is used to indicate that the 
 outputs are 0 or 1. It is assumed that the client (the person using the code) 
  will initialize it such that the if dot product of the weights and inputs 
  exceeds (or equals) the threshold, it will return <code>1</code>; <code>0</code> otherwise.  </p><p class="new">Other threshold functions can be used, but the main point of this threshold 
 activation is that it only returns two different outputs. </p><pre><code>import numpy as np
class BooleanPerceptron(object):

    def __init__(self, weights, threshold):
        # weights is an array of size 2 [w1, w2]
        # threshold is a scalar

        self.weights = np.array(weights)
        self.threshold = threshold

    def predict(self, inputs):
        s = np.dot(self.weights, np.array(inputs))
        if s - self.threshold &gt;= 0:
            return 1
        return 0</code></pre><p class="new">The code is available via the <code>LessonUtil</code> module (we'll see an example of
 using it next).</p><h2 id="and-example"><em>An</em>d Example</h2><p class="new">Take a close look at the following diagram of the 4 data points that make up
 the different combinations of two boolean inputs (<code>x1</code> and <code>x2</code>) for an 'and' 
 boolean gate. The green dots are the 'positive' cases; the blue the 'negative' cases.</p><img alt="and.png" class="center mb-3 iw400" src="https://raw.githubusercontent.com/habermanUIUC/CodeStoryLessons/main/lessons/dmap/mls/nn1/html/and.png"/><p class="new">Note that the drawn line that separates the instances falls <em>on</em> the data points. 
There are many lines that separate the <code>0</code>'s vs <code>1</code>'s (i.e. blue from green). 
Since, <code>predict</code> is using <code>&gt;= 0</code> for the positive (i.e. green) cases, for
 simplicity, we drew the line going through the points to indicate inclusion.</p><p class="new">To implement the logic for a boolean <strong><code>and</code></strong> would be as follows:</p><pre><code>def build_and_gate():
    import LessonUtil as Util
    and_gate = Util.BooleanPerceptron([1, 1], 2)
    return and_gate</code></pre><p class="new">Be sure to understand how those weights (<code>[1,1]</code>) and threshold <code>2</code>, implement 
the <code>and</code> functionality.</p><p class="new">For example, the <code>1,0</code> point sum would be calculated as:</p><img alt="math?math=%5Clarge%201*1%20%2B%200*1%20%3D%201" class="my-1 formula-inline" src="https://render.githubusercontent.com/render/math?math=%5Clarge%201*1%20%2B%200*1%20%3D%201" style="display:inline-block"/><p class="new">Since <code>1</code> is less than the threshold, it would return <code>0</code>.</p><p class="new">You can test it too (you will need to write some of the code):</p><div class="ide code-starter clearfix"><pre><code>def test_and_gate():
    gate = build_and_gate()
    assert 1 == gate.predict([1, 1]), 'fail 1, 1'
    assert 0 == gate.predict([1, 0]), 'fail 1, 0'
    assert 0 == gate.predict([0, 1]), 'fail 0, 1'
    assert 0 == gate.predict([0, 0]), 'fail 0, 0'
    print('and passed')</code></pre></div><h2 id="exercise-or-gate">Exercise: Or Gate</h2><p class="new">Finish the function for <code>build_or_gate</code> such that it returns a <code>BooleanPerceptron</code> 
to implement the logic of an <code>or</code> gate.</p><img alt="or.png" class="center mb-3 iw400" src="https://raw.githubusercontent.com/habermanUIUC/CodeStoryLessons/main/lessons/dmap/mls/nn1/html/or.png"/><div class="ide code-starter clearfix"><pre><code>def build_or_gate():
    import LessonUtil as Util
    # come up with values for w1, w2, t
    # such that the 'or' is implemented
    or_gate = Util.BooleanPerceptron([w1, w2], t)
    return or_gate</code></pre></div><p class="new">Let's now test it:</p><div class="ide code-starter clearfix"><pre><code>def test_or_gate():
    or_gate = build_or_gate()
    assert 1 == or_gate.predict([1, 1]), 'fail 1, 1'
    assert 1 == or_gate.predict([1, 0]), 'fail 1, 0'
    assert 1 == or_gate.predict([0, 1]), 'fail 0, 1'
    assert 0 == or_gate.predict([0, 0]), 'fail 0, 0'
    print('or passed')</code></pre></div><p class="new">You can also use the BooleanPerceptron to implement the boolean <code>not</code> (exercise 
left to the reader).  </p><h3 id="the-xor-problem">The XOR problem</h3><p class="new">The XOR boolean operator is only true if (and only if ) one of it's operands 
is 1. Here's the XOR truth table and the corresponding diagram:</p><img alt="xor.png" class="center mb-3 iw400" src="https://raw.githubusercontent.com/habermanUIUC/CodeStoryLessons/main/lessons/dmap/mls/nn1/html/xor.png"/><p class="new">Can you figure out how to draw a 2D line such that the blue and green dots are separated?</p><p class="new">This was a considerable problem in the early days of the perceptron. Both the 
perceptron model (and the preceding MP neuron) could <em>not</em> model the basic XOR gate.</p><h1 class="section" id="section2">Enter the layer</h1><p class="new">Since a single perceptron wasn't able to solve one of the fundamental logic
 gates, it was soon 'discovered' that having a network of perceptrons could
  solve the problem. The idea of building layers of perceptrons (neurons) was done in the mid
 1960's. Alexey Grigoryevich Ivakhnenko along with Valentin Grigor πevich Lapa 
created the first hierarchical representation of neural network. The multi
-layer perceptron (MLP) model has 'hidden' layers -- meaning their input only
 comes from the other layers.  The MLP was the origin of deep learning.</p><img alt="MultiLayerP.png" class="center mb-3 iw400" src="https://raw.githubusercontent.com/habermanUIUC/CodeStoryLessons/main/lessons/dmap/mls/nn1/html/MultiLayerP.png"/><p class="new">In the above image, the first layer is the input layer (usually not counted
 in the 'depth' of a network). The inputs are fully connected to all the
  nodes in the next layer (the hidden layer). Each of the hidden layers is
   also connected to each node in the output (or final) layer. </p><h3 id="weights-on-edges">Weights on edges</h3><p class="new">Each connection from one node to the next node is assigned a weight.  The
 weight indicates the strength of the relationship.</p><h3 id="activation-in-each-node">Activation in each node</h3><p class="new">Each node/neuron performs the summation (i.e. dot product) of the weights and inputs.<br/>  The output of each neuron is the result of applying the activation function. </p><h3 id="output-layer-">Output Layer </h3><p class="new">The final layer is the output layer.  It can have more than one neuron as we
 will see later.  The final output can also go through a 'filter' or process 
 to present the results back into the 'domain' of the prediction (e.g. a class label).</p><h2 id="an-xor-path">An Xor Path</h2><img alt="xor_truth.png" class="float-left mr-3 iw200" src="https://raw.githubusercontent.com/habermanUIUC/CodeStoryLessons/main/lessons/dmap/mls/nn1/html/xor_truth.png"/><p class="new">We can solve the XOR problem by hand crafting a MLP model. Let's take a look
 at the XOR logic gate and MLP network that implements the same logic:</p><img alt="xor_mlp.png" class="center mb-3 iw400" src="https://raw.githubusercontent.com/habermanUIUC/CodeStoryLessons/main/lessons/dmap/mls/nn1/html/xor_mlp.png"/><h3 id="exercise">Exercise</h3><p class="new">Build the above network using and, or, and not gates.  The 'nand' gate is
 simply a <code>not</code> gate applied to the output of an <code>and</code> gate.</p><p class="new">You can use the <code>LessonUtil</code> module to create <code>not</code> gate (if you didn't do so
 yourself).</p><pre><code>import LessonUtil as Util
not_gate = Util.build_not_gate()

# your code
and_gate = build_and_gate()
or_gate  = build_or_gate()</code></pre><div class="ide code-starter clearfix"><pre><code>class XorNetwork(object):

    def __init__(self):
       pass

    def predict(self, inputs):
       return -1</code></pre></div><p class="new">You can test it like the following (be sure to code this up):</p><pre><code>def test_xor_network():

    xor = XorNetwork()
    
    assert 0 == xor.predict([1, 1]), "fail 1, 1"
    assert 1 == xor.predict([1, 0]), "fail 1, 0"
    assert 1 == xor.predict([0, 1]), "fail 0, 1"
    assert 0 == xor.predict([0, 0]), "fail 0, 0"
    print('xor passed')</code></pre><h2 id="universal-theorem">Universal Theorem</h2><p class="new">As it turns out, an MLP with a single hidden layer can expression <em>any</em> boolean
 expression (it's a Universal Boolean Function).  Any boolean expression can
 be modeled using truth tables, and it's a mechanical process then to take
 the truth table and turn it into an 2 layer MLP (one hidden layer, one output layer).</p><h3 id="universal-approximation-theorem-real-valued-functions">Universal Approximation Theorem: Real Valued Functions</h3><p class="new">In <a href="https://deeplearning.cs.cmu.edu/S20/document/readings/Hornik_Stinchcombe_White.pdf" target="_blank">1988</a>
it was shown that a multilayered network of neurons with a single hidden layer can be
 used to approximate any continuous function to any desired precision (the
 universal approximation theorem). That is, an MLP (or ANN) could indeed find a 
function that separated data in higher dimensions. </p><h2 id="multiple-layers-and-activation-requirements">Multiple Layers and Activation Requirements</h2><p class="new">The Universal Approximation Theorem states that a neural network with 1 hidden layer can 
approximate any continuous function for inputs within a specific range. However, 
it did show that a non-linear activation function is needed as well. That is, to 
learn non-linear decision boundaries when classifying the output, we need a 
non-linear activation function.</p><img alt="unit_step.png" class="float-left mr-3 border2 iw400" src="https://raw.githubusercontent.com/habermanUIUC/CodeStoryLessons/main/lessons/dmap/mls/nn1/html/unit_step.png"/><p class="new">In the examples so far we are using a step-function (also called a binary
 threshold, binary-step, Heaviside, unit step function) to generate a 0 or 1
 regardless of how strong (or weak) the signal is.  </p><p class="new">The value returned at zero can be 0, ¬Ω (shown), or 1. </p><h1 class="section" id="section3">Modeling the Threshold and Bias</h1><p class="new">We used the following activation function for our simple perceptron:</p><img alt="math?math=%5CLarge%20y%20%3D%20%5Cbegin%7Bcases%7D1%20%26%20%5Ctext%7Bif%7D%5Cquad%20%5Csum_%7Bi%3D1%7D%5En%7Bw_i%7D%7Bx_i%7D%20-%20T%20%5Cge%200%20%5C%5C%200%20%26%20%5Ctext%7Botherwise%7D%20%5Cend%7Bcases%7D" class=" formula-block" src="https://render.githubusercontent.com/render/math?math=%5CLarge%20y%20%3D%20%5Cbegin%7Bcases%7D1%20%26%20%5Ctext%7Bif%7D%5Cquad%20%5Csum_%7Bi%3D1%7D%5En%7Bw_i%7D%7Bx_i%7D%20-%20T%20%5Cge%200%20%5C%5C%200%20%26%20%5Ctext%7Botherwise%7D%20%5Cend%7Bcases%7D"/><p class="new">The <code>T</code> value or threshold can be modeled in different ways when you are
 building neural networks.  This value, also called the <em>bias</em>, is necessary
 in order to shift the output of the activation level. For example, for a 
simple line, without the bias (the y-intercept), you could never find a line 
that didn't pass through the origin (0,0).</p><p class="new">In the image below the function <img alt="math?math=%5Clarge%20%5Cfrac%7B1%7D%7B1%2Be%5E%7B-x%7D%7D" class="my-1 formula-inline" src="https://render.githubusercontent.com/render/math?math=%5Clarge%20%5Cfrac%7B1%7D%7B1%2Be%5E%7B-x%7D%7D" style="display:inline-block"/> is drawn with 
<code>+2</code> and <code>-2</code> bias values.</p><img alt="bias_sigmoid.png" class="center border2 iw400" src="https://raw.githubusercontent.com/habermanUIUC/CodeStoryLessons/main/lessons/dmap/mls/nn1/html/bias_sigmoid.png"/><h2 id="bias-implementations-">Bias Implementations: </h2><p class="new">There are two different approaches to managing the bias. You will see 
implementations handle the bias differently, but mathematically they are doing 
the same thing.</p><h3 id="option-1-">Option 1: </h3><h4 id="as-an-input-to-the-neuron">As an Input to the Neuron</h4><img alt="bias_on_node.png" class="float-left mr-3 border2 iw400" src="https://raw.githubusercontent.com/habermanUIUC/CodeStoryLessons/main/lessons/dmap/mls/nn1/html/bias_on_node.png"/><p class="new">In this model, the bias value is calculated independent of the weights. Even
 when the gradients (the updates) are calculated (details coming) the calculations 
for the bias can be done separately as well.</p><h3 id="option-2-">Option 2: </h3><h4 id="as-an-weighted-edge-">As an Weighted Edge </h4><img alt="bias_as_node.png" class="float-left p-4 mr-3 border2 iw400" src="https://raw.githubusercontent.com/habermanUIUC/CodeStoryLessons/main/lessons/dmap/mls/nn1/html/bias_as_node.png"/><p class="new">Another way to manage the bias is to always set an additional 'phantom' input
 to be clamped to <code>1</code> and make the weight that connects the input to the
 neuron act as the bias.</p><h2 id="a-look-ahead-for-learning">A Look Ahead for Learning</h2><p class="new">In the next lesson we will go over different activation functions that are
 used in neural networks.  We will also discuss why networks have more than 1
 hidden layer. If only 1 hidden layer is needed, you might be wondering why we build
 networks with multiple (i.e. 'deep') hidden layers.  We will discuss this in
 more detail later, but it has been shown that deeper networks 'learn' and
 generalize better. </p><p class="new">We will also discuss how the learning process works; as of now all our
 examples have been manually constructed.  There needs to be a process to
 find (or learn) the weights of the network.</p><h1 class="section" id="section4">Lesson Assignment</h1><h2 id="2d-shape-detection">2D Shape Detection</h2><img alt="square.png" class="float-left mr-3 iw200 border2" src="https://raw.githubusercontent.com/habermanUIUC/CodeStoryLessons/main/lessons/dmap/mls/nn1/html/square.png"/><p class="new">Take a look at the red diamond defined by the four points: <code>(-2,0),(0,2),(0,-2),(2,0)</code></p><p class="new">We are going to manually build a MLP network using just threshold based perceptrons such
 that the network will output 1 if a point is inside (or touching) the diamond.
No learning is done; we are doing the work by constructing the network by üñê.</p><h2 id="linear-classifier">Linear Classifier</h2><p class="new">We created this class in a previous lesson.  It finds the equation of a line
 <img alt="math?math=%5Clarge%20y%3Dmx%20%2B%20b" class="my-1 formula-inline" src="https://render.githubusercontent.com/render/math?math=%5Clarge%20y%3Dmx%20%2B%20b" style="display:inline-block"/> from 2 points.</p><div class="ide code-starter clearfix"><pre><code>class Line(object):
    def __init__(self, p1, p2):
        p1x = p1[0]
        p1y = p1[1]
        p2x = p2[0]
        p2y = p2[1]

        dx = p2x - p1x
        dy = p2y - p1y

        # slope and y intercept
        self.slope = dy/dx
        self.b = p1y - self.slope * p1x</code></pre></div><p class="new">We can use the equation of a line <img alt="math?math=%5Clarge%20y%3Dmx%20%2B%20b%20%5Cquad%20(i.e.%20%5Cquad%200%20%3D%20x_1w_1%20-%20x_2w_2%20%2B%20b)" class="my-1 formula-inline" src="https://render.githubusercontent.com/render/math?math=%5Clarge%20y%3Dmx%20%2B%20b%20%5Cquad%20(i.e.%20%5Cquad%200%20%3D%20x_1w_1%20-%20x_2w_2%20%2B%20b)" style="display:inline-block"/> as a way to initial our weights.</p><p class="new">We can build a LinearPerceptron initialized with a <code>Line</code> object
such that it will detect if a point is in the half-plane defined by the line:</p><img alt="greenline.png" class="center iw300 border2" src="https://raw.githubusercontent.com/habermanUIUC/CodeStoryLessons/main/lessons/dmap/mls/nn1/html/greenline.png"/><p class="new">For example, assume you have the following points (as shown above): </p><pre><code>p1 = (-2, 0)
p2 = (0,2)
p3 = (0,-2)
p4 = (2,0)</code></pre><h2 id="step-1">Step 1</h2><p class="new">Write the necessary code <code>LinearPerceptron</code> such that the following test works:</p><pre><code>def test_p2_p4():
    lp = LinearPerceptron(Line(p2, p4))
    assert 1 == lp.predict(p2)
    assert 1 == lp.predict(p4)
    assert 1 == lp.predict([0,0])
    assert 0 == lp.predict([2,2])
    assert 0 == lp.predict([1,1.0001])

test_p2_p4()</code></pre><h2 id="step-2">Step 2</h2><img alt="blueline.png" class="float-left mr-3 iw200 border2" src="https://raw.githubusercontent.com/habermanUIUC/CodeStoryLessons/main/lessons/dmap/mls/nn1/html/blueline.png"/><p class="new">Write the necessary code <code>LinearPerceptron</code> for the half-plane defined by the
 line for points p1 and p3.</p><p class="new">Note that the decision boundary has 'flipped'. You can decide how to flip the
 logic (use a <code>not</code> gate or invert the answer).</p><p class="new">Update your constructor so that a named parameter <code>invert</code> can be used to
 flip the meaning of which side of the line the 'positive' instances are on.<br/>Regardless, a point on the line is always 'positive' (e.g <code>1</code>)</p><pre><code>    lp = LinearPerceptron(Line(p1, p3), invert=True)</code></pre><p class="new">Once that is done, the
 following test should work:</p><pre><code>def test_p1_p3():
    lp = LinearPerceptron(Line(p1, p3), invert=True)
    assert 1 == lp.predict(p1)
    assert 1 == lp.predict(p3)
    assert 1 == lp.predict([0,0])
    assert 1 == lp.predict(p2)
    assert 1 == lp.predict(p4)
    assert 0 == lp.predict([-2,-2])
    assert 0 == lp.predict([-1,-1.0001])</code></pre><h2 id="step-3">Step 3</h2><p class="new">Create detectors for the lines defined by (p1, p2) and (p3, p4). Be sure to
 test your code</p><div class="ide code-starter clearfix"><pre><code></code></pre></div><h2 id="step-4">Step 4</h2><p class="new">Create the class named <code>DiamondDectector</code> that uses four <code>LinearPerceptrons</code> and
 one or more appropriate <code>BooleanPerceptron</code> such the the following diamond is
 detected:</p><img alt="4lines.png" class="center iw200 border2" src="https://raw.githubusercontent.com/habermanUIUC/CodeStoryLessons/main/lessons/dmap/mls/nn1/html/4lines.png"/><div class="ide code-starter clearfix"><pre><code>class DiamondDectector(object):
    pass
    
    
def test_dd():
    dd = DiamondDectector()
    assert 1 == dd.predict([0, 0])</code></pre></div><h2 id="wheres-the-learning">Where's the <em>learning</em>?</h2><p class="new">You can now see how a network could be built to detect shapes.  You might
 imagine how by using more inputs and more neurons could result in a network
 that is able to detect shapes with both higher resolution and in higher
 dimensions.</p><p class="new">Of course the real power of neural networks comes from the ability of the
 network to find the weights (and biases) such that shape detection (among
 other things) could be done automatically (aka learning).</p><p class="new">Hopefully you can see the potential power and how these networks help find
 patterns in data.</p><h1>Test and Submit</h1><p>Once you have finished, you can download your code (via <code>ide.tester</code>) and upload that file to Gradescope (find lesson with tag <strong>nn1</strong>).</p><div class="my-4"><pre><code><strong># to list the tests available</strong><br/>print(ide.tester.list_tests())<br/><strong># to perform a specific test</strong><br/>print(ide.tester.test_functionality('name of test'))<br/><strong># to test your code (either works)</strong><br/>print(ide.tester.test_notebook())<br/>print(ide.tester.test_notebook(verbose=True))<br/><strong># to prepare and download your code</strong><br/>ide.tester.download_solution()</code></pre></div><div class="lesson-footer flex bg-gray-200 justify-center"><div class="lesson-footer-card displaycard bg-blue-200 border-t border-gray-400 max-w-2xl rounded overflow-hidden shadow-lg"><div class="px-6 py-4"><div class="title-text text-center font-bold text-xl">Neural Networks (part1)</div><p class="text-center text-gray-800 text-xl">The Perceptron and the MLP</p><div class="text-center mt-6 text-xl"><i aria-hidden="true" class="fas fa-tags"></i> any questions on Piazza with <span class="font-bold">nn1</span></div><div class="text-gray-700 text-base">¬†</div><div></div><div class="text-gray-700 text-base">References and Additional Readings</div><div class="text-xs p-2 border border-solid border-gray-500 bg-gray-300"> <div class="text-gray-700 px-4 m-2">‚Ä¢ <!-- --> <a href="https://web.stanford.edu/class/history13/earlysciencelab/body/brainpages/brain.html" target="_blank">https://web.stanford.edu/class/history13/earlysciencelab/body/brainpages/brain.html</a></div></div><div class="flex mt-4 border-t border-solid border-gray-500 justify-around bg-gray-200"><div class="text-gray-700 text-center px-4 m-2 text-sm">D.M. &amp; the üêç</div><div class="text-gray-700 text-center px-4 m-2 text-sm"><strong>Version:</strong> <!-- -->SP21</div></div><div class="text-gray-700 mt-2 text-center text-sm font-bold">All Rights Reserved Michael Haberman</div><div class="text-gray-700 text-center text-sm">Do not distribute this notebook</div></div></div></div><div>¬†</div><div class="ide code-starter clearfix"><pre><code># print(ide.tester.test_notebook()) 
# print(ide.tester.test_notebook(verbose=True)) 

# once you are ready -- run this 
# ide.tester.download_solution() 
</code></pre></div></div></div></body></html>