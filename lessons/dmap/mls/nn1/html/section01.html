<!DOCTYPE html><html lang='en'><head><title>Neural Networks (part1)</title><meta charset="utf-8"><style>/*! normalize.css v8.0.1 | MIT License | github.com/necolas/normalize.css */html{line-height:1.15;-webkit-text-size-adjust:100%}body{margin:0}h1{font-size:2em;margin:.67em 0}pre{font-family:monospace,monospace;font-size:1em}a{background-color:transparent}strong{font-weight:bolder}code{font-family:monospace,monospace;font-size:1em}img{border-style:none}::-webkit-file-upload-button{-webkit-appearance:button;font:inherit}h1,h2,h3,h4,p,pre{margin:0}ul{list-style:none;margin:0;padding:0}html{font-family:system-ui,-apple-system,BlinkMacSystemFont,"Segoe UI",Roboto,"Helvetica Neue",Arial,"Noto Sans",sans-serif,"Apple Color Emoji","Segoe UI Emoji","Segoe UI Symbol","Noto Color Emoji";line-height:1.5}*,::after,::before{box-sizing:border-box;border-width:0;border-style:solid;border-color:#e2e8f0}img{border-style:solid}h1,h2,h3,h4{font-size:inherit;font-weight:inherit}a{color:inherit;text-decoration:inherit}code,pre{font-family:Menlo,Monaco,Consolas,"Liberation Mono","Courier New",monospace}img{display:block;vertical-align:middle}img{max-width:100%;height:auto}.bg-gray-200{--bg-opacity:1;background-color:#edf2f7;background-color:rgba(237,242,247,var(--bg-opacity))}.bg-gray-300{--bg-opacity:1;background-color:#e2e8f0;background-color:rgba(226,232,240,var(--bg-opacity))}.bg-blue-200{--bg-opacity:1;background-color:#bee3f8;background-color:rgba(190,227,248,var(--bg-opacity))}.bg-blue-300{--bg-opacity:1;background-color:#90cdf4;background-color:rgba(144,205,244,var(--bg-opacity))}.border-gray-400{--border-opacity:1;border-color:#cbd5e0;border-color:rgba(203,213,224,var(--border-opacity))}.border-gray-500{--border-opacity:1;border-color:#a0aec0;border-color:rgba(160,174,192,var(--border-opacity))}.rounded{border-radius:.25rem}.rounded-full{border-radius:9999px}.border-solid{border-style:solid}.border{border-width:1px}.border-t{border-top-width:1px}.inline-block{display:inline-block}.flex{display:flex}.justify-center{justify-content:center}.justify-around{justify-content:space-around}.float-right{float:right}.float-left{float:left}.clearfix:after{content:"";display:table;clear:both}.font-serif{font-family:Georgia,Cambria,"Times New Roman",Times,serif}.font-semibold{font-weight:600}.font-bold{font-weight:700}.h-64{height:16rem}.text-xs{font-size:.75rem}.text-sm{font-size:.875rem}.text-base{font-size:1rem}.text-xl{font-size:1.25rem}.leading-none{line-height:1}.m-2{margin:.5rem}.my-1{margin-top:.25rem;margin-bottom:.25rem}.my-4{margin-top:1rem;margin-bottom:1rem}.mt-1{margin-top:.25rem}.mt-2{margin-top:.5rem}.mr-2{margin-right:.5rem}.mb-2{margin-bottom:.5rem}.mr-3{margin-right:.75rem}.mb-3{margin-bottom:.75rem}.ml-3{margin-left:.75rem}.mt-4{margin-top:1rem}.mt-6{margin-top:1.5rem}.max-w-sm{max-width:24rem}.max-w-2xl{max-width:42rem}.object-contain{-o-object-fit:contain;object-fit:contain}.overflow-hidden{overflow:hidden}.p-1{padding:.25rem}.p-2{padding:.5rem}.p-4{padding:1rem}.py-1{padding-top:.25rem;padding-bottom:.25rem}.py-2{padding-top:.5rem;padding-bottom:.5rem}.px-3{padding-left:.75rem;padding-right:.75rem}.py-4{padding-top:1rem;padding-bottom:1rem}.px-4{padding-left:1rem;padding-right:1rem}.px-6{padding-left:1.5rem;padding-right:1.5rem}.pl-3{padding-left:.75rem}.shadow-lg{box-shadow:0 10px 15px -3px rgba(0,0,0,.1),0 4px 6px -2px rgba(0,0,0,.05)}.shadow-inner{box-shadow:inset 0 2px 4px 0 rgba(0,0,0,.06)}.text-left{text-align:left}.text-center{text-align:center}.text-black{--text-opacity:1;color:#000;color:rgba(0,0,0,var(--text-opacity))}.text-gray-700{--text-opacity:1;color:#4a5568;color:rgba(74,85,104,var(--text-opacity))}.text-gray-800{--text-opacity:1;color:#2d3748;color:rgba(45,55,72,var(--text-opacity))}.whitespace-no-wrap{white-space:nowrap}.w-1\/2{width:50%}.w-full{width:100%}.text-tiny{font-size:.5rem!important}body{color:#000!important;font-size:1.25rem!important}.main-content{max-width:900px}.lesson{padding-left:15px!important;padding-right:10px!important;--bg-opacity:1;background-color:#edf2f7;background-color:rgba(237,242,247,var(--bg-opacity))}.main-content,html{font-family:Arial,Georgia,Verdana,"Times New Roman"!important}.lesson-footer-card,.lesson-overview-card{font-family:"Times New Roman"!important}.lesson-footer{margin-top:50px;margin-top:20px}.lesson ul{list-style-position:inside;list-style-type:none;margin-left:1em}.lesson ul li{padding-left:1em;padding-right:5px}.lesson ul li::before{content:"â€¢";padding-right:5px}span{white-space:nowrap}p.new{padding-top:0;padding-bottom:.5em}p.new+p{padding-top:.5em}h1,h2,h3,h4{font-weight:700;margin-top:.25em!important;margin-bottom:.05em!important;font-family:Georgia,Cambria,"Times New Roman",Times,serif!important}h1{font-size:2em!important;clear:both;color:#000!important}div+h1,h2{margin-top:0!important}h2{margin-top:.5em!important;font-size:1.5em!important;clear:both;color:#8b0000!important}h3{font-size:1.25em!important;clear:both;color:#006400!important}h4{font-size:1em!important;clear:both;color:#00008b!important}ul{margin-bottom:30px}p.new a{text-decoration:underline}.lesson a{text-decoration:underline;color:#00f}.title-text{font-size:2rem}.shadow-lg{box-shadow:0 10px 15px -3px rgba(0,0,0,.1),0 4px 6px -2px rgba(0,0,0,.5)}img.formula-block{margin-left:auto;margin-right:auto;margin-top:.25rem;margin-bottom:.75rem}img.formula-inline{margin-top:.25rem;margin-bottom:.25rem}img.center{-o-object-position:center;object-position:center;margin-left:auto;margin-right:auto}img.border2{border:1px solid #94add4;margin-top:.5rem;margin-bottom:.75rem}img.iw600{height:auto;width:auto;max-width:600px}img.iw400{height:auto;width:auto;max-width:400px}img.iw500{height:auto;width:auto;max-width:500px}img.iw300{height:auto;width:auto;max-width:300px}img.iw200{height:auto;width:auto;max-width:200px}img.iw100{height:auto;width:auto;max-width:100px}code{font-size:smaller}pre code{font-size:15px}pre code:not(.line-number){background:#f4f4f4;font-family:monospace;font-size:15px;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;cursor:default;touch-action:none;-webkit-touch-callout:none;-webkit-tap-highlight-color:transparent;clear:both;border:1px solid #ddd;color:#666;page-break-inside:avoid;display:block;min-width:840px;max-width:840px;overflow:scroll;line-height:1.6;margin-bottom:1.6em;padding:1em 1.5em;-moz-tab-size:2;-o-tab-size:2;tab-size:2;word-wrap:break-word;white-space:pre-wrap;border-left:3px solid #f36d33}div.code-starter>pre code{border-left:3px solid #fdff44!important;background-image:radial-gradient(rgba(0,150,0,.75),#000 120%);color:#fff;font:.9rem Inconsolata,monospace}div.code-starter>pre code::after{content:"\a$_"}</style>
<script src="https://kit.fontawesome.com/7efc4bcee2.js" crossOrigin="anonymous"></script>
<script>
    let stateCheck = setInterval(function(){
      if (document.readyState === 'complete') {
        clearInterval(stateCheck);
        let s1 = document.getElementById('start');
        // console.log('doc is ready', s1);
        if (s1) {
           s1.setAttribute('tabindex', '-1');
           s1.focus(); 
           s1.scrollIntoView({behavior: 'smooth'}); 
           setTimeout(function(){s1.blur()}, 500);
           // console.log('focus set');
        }
      }
    }, 200);
    </script>
</head><body class="lesson"><div class="main-content lesson bg-gray-200 text-black p-1 pl-3 font-serif"><div class="md-inner">
<div id="start" class="section">&nbsp;</div><h1 class="section" id="section1">Neural Networks (part1)</h1><h2 id="the-perceptron-and-the-mlp">The Perceptron and the MLP</h2><p class="new">Artificial Neural Networks (ANN) is a ðŸ”¥ topic.  Although neural networks have a 
long history, recent advances including the application of using gradient descent 
and the use of many 'hidden layers' (aka 'deep' networks) to solve non-linear classification
  problems have made it a very popular field for both practical applications
   and research.</p><p class="new">This lesson starts at the beginning and discusses using ANN to solve
 linear classification first. Although if your inputs can be linearly separable, 
 you don't need neutral network; it's a great starting point to introduce how neural
  networks work.</p><h2 id="a-shortened-history-">A Shortened History </h2><h3 id="associationism">Associationism</h3><img alt="plato_aristotle.jpeg" class="iw200 float-left mr-3 mt -2 border2" src="https://raw.githubusercontent.com/habermanUIUC/CodeStoryLessons/main/lessons/dmap/mls/nn1/html/plato_aristotle.jpeg"/><p class="new">The history of neural networks begins when humans first tried to understand
 how they acquired knowledge. Trying to answer the questions of how cognition (learning, 
 solving problems, recognizing patterns, creating) works and how one can emulate
  it have been the driving forces behind machine learning and artificial intelligence (AI). </p><p class="new">As far back as 400 BC, Plato described learning as creating associations. This 
theory (associationism) was further refined for several years (thousands) and 
can be found in the ideas of Pavlov as well). Our ability to think is based on associations.</p><h3 id="connectionism">Connectionism</h3><img alt="bain.png" class="iw200 float-right ml-3 mt-2 border2" src="https://raw.githubusercontent.com/habermanUIUC/CodeStoryLessons/main/lessons/dmap/mls/nn1/html/bain.png"/><p class="new">In the mid 1800's the anatomy of the brain was discovered (w.r.t neurons) to 
be a mass of interconnected neurons. This structure was then 'connected' with a model of
 computation via Alexander Bain called connectionism. Bain claimed that the
  information lies in the connections between the neurons. He
   developed models to show that difference combinations of inputs could 
   result in different outputs. He also came up a learning theory as well.</p><p class="new">The connectionism framework still underlies the neural networks models we see today: 
a network of processing elements (connectionist architecture).</p><h3 id="an-artificial-neuron-">An Artificial Neuron </h3><img alt="m_pitts.jpeg" class="iw100 float-left mr-3 mt-2 border2" src="https://raw.githubusercontent.com/habermanUIUC/CodeStoryLessons/main/lessons/dmap/mls/nn1/html/m_pitts.jpeg"/><p class="new">The actual modeling of the brain started with Warren McCulloch and Walter
 Pitts (MP) who developed a single neuron model (early 1940's).</p><p class="new">The model imitates the functionality of a biological neuron (shown below):</p><img alt="neuron.png" class="center iw600 border2" src="https://raw.githubusercontent.com/habermanUIUC/CodeStoryLessons/main/lessons/dmap/mls/nn1/html/neuron.png"/><p class="new">The main components (for our purposes) are the Dendrites (the incoming signals 
come into the neuron), the Soma (the collection of incoming signals) and the 
Axon where the signal goes out. If the collection of incoming signals 'trips' 
a threshold, it then goes down the Axon. The Synapse is where two neurons 
connect to each other. </p><img alt="neuron_model.png" class="center iw500 border2" src="https://raw.githubusercontent.com/habermanUIUC/CodeStoryLessons/main/lessons/dmap/mls/nn1/html/neuron_model.png"/><p class="new">The first computational model for a individual neuron (the MP neuron) was 
divided into two parts:</p><ul><li>part 1: the inputs are aggregated</li><li>part 2: based on the value, a 0/1 decision (an output) is made</li></ul><img alt="MPitsNeuron.png" class="center iw400 border2" src="https://raw.githubusercontent.com/habermanUIUC/CodeStoryLessons/main/lessons/dmap/mls/nn1/html/MPitsNeuron.png"/><p class="new">The models could represent simple boolean logic gates (operations) such as AND, OR, NOT.  </p><h3 id="hebbian-learning-and-the-perceptron">Hebbian Learning and the Perceptron</h3><p class="new">The MP neuron model was without a process to show learning could occur. In 1949, 
Donald Heb provided a learning mechanism (Hebbian learning) that showed how neurons can work 
 together and improve over time. In a sense, this amounts to how a weight between 
 two neurons can be adjusted to strengthen the connection. </p><p class="new">The same MP neuron model was further refined by Frank Rosenblatt with his Perceptron 
model (1958). It embodies the aspects of the neuron. Each cell has several inputs. 
Each input also has a weight. Each weight is independent of the other
 weights as well. If the weighted sum of the inputs exceeds a threshold, 
it would 'fire'. This threshold function at would then send the weighted sum (or some other value) 
as an output. This was a close representation of the biological analog.</p><img alt="perceptron.png" class="center border2" src="https://raw.githubusercontent.com/habermanUIUC/CodeStoryLessons/main/lessons/dmap/mls/nn1/html/perceptron.png"/><h4 id="activation">Activation</h4><p class="new">An import aspect of the Perceptron is how it decides to excite and fire or simply 
absorb the incoming signals. This aspect is call the threshold or
 <em>activation</em> function. Mathematically, it can be expressed as:</p><img alt="math?math=%5CLarge%20y%20%3D%20%5Cbegin%7Bcases%7D1%20%26%20%5Ctext%7Bif%7D%5Cquad%20%5Csum_%7Bi%3D1%7D%5En%7Bw_i%7D%7Bx_i%7D%20-%20T%20%5Cge%200%20%5C%5C%200%20%26%20%5Ctext%7Botherwise%7D%20%5Cend%7Bcases%7D" class=" formula-block" src="https://render.githubusercontent.com/render/math?math=%5CLarge%20y%20%3D%20%5Cbegin%7Bcases%7D1%20%26%20%5Ctext%7Bif%7D%5Cquad%20%5Csum_%7Bi%3D1%7D%5En%7Bw_i%7D%7Bx_i%7D%20-%20T%20%5Cge%200%20%5C%5C%200%20%26%20%5Ctext%7Botherwise%7D%20%5Cend%7Bcases%7D"/><p class="new">The activation function uses some means to reduce the sum of input values 
to a 1 or a 0 (or a value very close to a 1 or 0) in order to represent activation or lack thereof.
This limit of only producing a 1 or 0 was rectified by using different
 activation functions (see part 2). </p><p class="new">The learning mechanism (sequential learning) was also provided by using examples (of input and 
output pairs) and updating the weights. Convergence (for boolean functions) was shown for 
data containing linearly separable classes.</p><h2 id="linear-separation">Linear Separation</h2><img alt="non-linear.png" class="float-left mr-3 iw300" src="https://raw.githubusercontent.com/habermanUIUC/CodeStoryLessons/main/lessons/dmap/mls/nn1/html/non-linear.png"/><p class="new">This lesson started with a mention of linear classification and linear 
separability.  What does it mean for data to be linear separable? The image shown 
to the left is an example of data that is <strong>not</strong> linear separable.</p><p class="new">When a set of output values can be split by a straight line, the output values 
are said to be linearly separable.  On one side of the 'line' are the
 instances with the one class label; on the other side are the instances with
  the other class label.</p><p class="new">Geometrically, this condition describes the 
situation in which there is a line or hyperplane (in higher dimensions) that separates, in 
the vector space of inputs, the data into separate distinct classes of outputs.
<img alt="linearSep.jpg" class="center iw300 mr-3" src="https://raw.githubusercontent.com/habermanUIUC/CodeStoryLessons/main/lessons/dmap/mls/nn1/html/linearSep.jpg"/></p><h3 id="or-by-example">Or by Example</h3><p class="new">Take a look at the following code that implements the basic Perceptron
 functionality.  The class <code>BooleanPerceptron</code> is used to indicate that the 
 outputs are 0 or 1. It is assumed that the client (the person using the code) 
  will initialize it such that the if dot product of the weights and inputs 
  exceeds (or equals) the threshold, it will return <code>1</code>; <code>0</code> otherwise.  </p><p class="new">Other threshold functions can be used, but the main point of this threshold 
 activation is that it only returns two different outputs. </p><pre><code>import numpy as np
class BooleanPerceptron(object):

    def __init__(self, weights, threshold):
        # weights is an array of size 2 [w1, w2]
        # threshold is a scalar

        self.weights = np.array(weights)
        self.threshold = threshold

    def predict(self, inputs):
        s = np.dot(self.weights, np.array(inputs))
        if s - self.threshold &gt;= 0:
            return 1
        return 0</code></pre><p class="new">The code is available via the <code>LessonUtil</code> module (we'll see an example of
 using it next).</p><h2 id="and-example"><em>An</em>d Example</h2><p class="new">Take a close look at the following diagram of the 4 data points that make up
 the different combinations of two boolean inputs (<code>x1</code> and <code>x2</code>) for an 'and' 
 boolean gate. The green dots are the 'positive' cases; the blue the 'negative' cases.</p><img alt="and.png" class="center mb-3 iw400" src="https://raw.githubusercontent.com/habermanUIUC/CodeStoryLessons/main/lessons/dmap/mls/nn1/html/and.png"/><p class="new">Note that the drawn line that separates the instances falls <em>on</em> the data points. 
There are many lines that separate the <code>0</code>'s vs <code>1</code>'s (i.e. blue from green). 
Since, <code>predict</code> is using <code>&gt;= 0</code> for the positive (i.e. green) cases, for
 simplicity, we drew the line going through the points to indicate inclusion.</p><p class="new">To implement the logic for a boolean <strong><code>and</code></strong> would be as follows:</p><pre><code>def build_and_gate():
    import LessonUtil as Util
    and_gate = Util.BooleanPerceptron([1, 1], 2)
    return and_gate</code></pre><p class="new">Be sure to understand how those weights (<code>[1,1]</code>) and threshold <code>2</code>, implement 
the <code>and</code> functionality.</p><p class="new">For example, the <code>1,0</code> point sum would be calculated as:</p><img alt="math?math=%5Clarge%201*1%20%2B%200*1%20%3D%201" class="my-1 formula-inline" src="https://render.githubusercontent.com/render/math?math=%5Clarge%201*1%20%2B%200*1%20%3D%201" style="display:inline-block"/><p class="new">Since <code>1</code> is less than the threshold, it would return <code>0</code>.</p><p class="new">You can test it too (you will need to write some of the code):</p></div></div></body></html>