{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data, Machines and the üêç \n",
    "<img src=\"https://raw.githubusercontent.com/habermanUIUC/CodeStoryLessons/main/lessons/dmap/text/tfidf-skl/html/section00.png\" align=\"left\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"install\"></a>\n",
    "## Notebook Preparation for Lesson 1‚Ä¢2‚Ä¢3\n",
    "Each lesson will start with a similar template (given in the course schedule):  \n",
    "1. **save** to your google drive (copy to drive)<br/><img src=\"https://raw.githubusercontent.com/habermanUIUC/CodeStoryLessons/main/assets/images/colab/copy-to-drive.png\"/>\n",
    "2. **update** the NET_ID to be your netID (no need to include @illinois.edu)\n",
    "3. **run** the next cell to install the IDE. <img src=\"https://raw.githubusercontent.com/habermanUIUC/CodeStoryLessons/main/assets/images/colab/play-button.png\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "LESSON_ID = 'dmap:text:tfidf-skl'   # keep this as is\n",
    "NET_ID    = 'CHANGE_ME' # CHANGE_ME to your netID (keep the quotes)\n",
    "\n",
    "def install_ide(net_id, lesson_id):\n",
    "  import sys\n",
    "  if 'codestories' not in sys.modules:\n",
    "      print('installing modules')\n",
    "      !pip install git+https://mehaberman@bitbucket.org/mehaberman/codestories.git --upgrade &> install.log\n",
    "  \n",
    "  from codestories.cs.CodeStories import CodeStory\n",
    "  return CodeStory(net_id, lesson_id)\n",
    "\n",
    "ide = install_ide(NET_ID, LESSON_ID)\n",
    "print(ide.welcome())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson TF‚Ä¢IDF (part 2)\n",
    "(hit ‚ñ∂ to read the first part of the lessonÔ∏è)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run to read the next section\n",
    "ide.reader.view_section(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_corpus():\n",
    "    c1 = \"Do you like Green eggs and ham\"\n",
    "    c2 = \"I do not like them Sam I am  I do not like Green eggs and ham\"\n",
    "    c3 = \"Would you like them Here or there\"\n",
    "    c4 = \"I would not like them Here or there I would not like them Anywhere\"\n",
    "    return [c1, c2, c3, c4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run to read the next section\n",
    "ide.reader.view_section(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cv_demo1():\n",
    "    # fill me in\n",
    "    pass\n",
    "    \n",
    "cv_demo1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run to read the next section\n",
    "ide.reader.view_section(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add in the function split_into_tokens\n",
    "# parameters: data, normalize, min_length (in that order)\n",
    "# returns a list of words/tokens\n",
    "# normalize has a default value of True \n",
    "# min_length has a default value of 0\n",
    "# if normalize, make tokens lowercase\n",
    "# only returns tokens longer than min_length\n",
    "def cv_demo2():\n",
    "\n",
    "    corpus = get_corpus()\n",
    "\n",
    "    # pass in our own tokenizer\n",
    "    cvec = CountVectorizer(tokenizer=split_into_tokens)\n",
    "\n",
    "    # convert the documents into a document-term matrix\n",
    "    doc_term_matrix = cvec.fit_transform(corpus)\n",
    "\n",
    "    # get the terms found in the corpus\n",
    "    tokens = cvec.get_feature_names()\n",
    "    \n",
    "    return doc_term_matrix, tokens\n",
    "\n",
    "dtm, tokens = cv_demo2()\n",
    "print(tokens)\n",
    "print(dtm.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run to read the next section\n",
    "ide.reader.view_section(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def word_matrix_to_df(wm, feature_names):\n",
    "    # create an index for each row\n",
    "    doc_names = ['Doc{:d}'.format(idx+1) for idx, _ in enumerate(wm)]\n",
    "    df = pd.DataFrame(data=wm.toarray(), index=doc_names, columns=feature_names)\n",
    "    return df\n",
    "\n",
    "def cv_demo3():\n",
    "\n",
    "    doc_term_matrix, tokens = cv_demo2()\n",
    "    df = word_matrix_to_df(doc_term_matrix, tokens)\n",
    "    \n",
    "    return df\n",
    "\n",
    "df = cv_demo3()\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run to read the next section\n",
    "ide.reader.view_section(9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# type&run the above example/exercise in this cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run to read the next section\n",
    "ide.reader.view_section(11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cv_demo_tf_idf():\n",
    "\n",
    "    doc_term_matrix, tokens = cv_demo2()\n",
    "    tfidf_transformer=TfidfTransformer(smooth_idf=True)\n",
    "\n",
    "    # learn the IDF vector\n",
    "    tfidf_transformer.fit(doc_term_matrix)\n",
    "    idf = tfidf_transformer.idf_\n",
    "\n",
    "    # transform the count matrix to tf-idf\n",
    "    tf_idf_vector = tfidf_transformer.transform(doc_term_matrix)\n",
    "    print(tf_idf_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run to read the next section\n",
    "ide.reader.view_section(13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cv_demo_pd_tf_idf():\n",
    "\n",
    "    doc_term_matrix, tokens = cv_demo2()\n",
    "    tfidf_transformer=TfidfTransformer(smooth_idf=True)\n",
    "\n",
    "    # learn the IDF vector\n",
    "    tfidf_transformer.fit(doc_term_matrix)\n",
    "    idf = tfidf_transformer.idf_\n",
    "\n",
    "    # transform the count matrix to tf-idf\n",
    "    tf_idf_vector = tfidf_transformer.transform(doc_term_matrix)\n",
    "\n",
    "    # print out the values \n",
    "    # for the token 'i' in the second document\n",
    "    token = 'i'\n",
    "    doc = 1\n",
    "    df_idf = pd.DataFrame(idf, index=tokens, columns=[\"idf_weights\"])\n",
    "    df_idf.sort_values(by=['idf_weights'], inplace=True, ascending=False)\n",
    "    idf_token = df_idf.loc[token]['idf_weights']\n",
    "\n",
    "    doc_vector = tf_idf_vector[doc]\n",
    "    df_tfidf = pd.DataFrame(doc_vector.T.todense(), index=tokens, columns=[\"tfidf\"])\n",
    "    df_tfidf.sort_values(by=[\"tfidf\"], ascending=False, inplace=True)\n",
    "    tfidf_token = df_tfidf.loc[token]['tfidf']\n",
    "\n",
    "    # tfidf = tf * idf\n",
    "    tf_token = tfidf_token / idf_token\n",
    "    print('TF    {:s} {:2.4f}'.format(token, tf_token))\n",
    "    print('IDF   {:s} {:2.4f}'.format(token, idf_token))\n",
    "    print('TFIDF {:s} {:2.4f}'.format(token, tfidf_token))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run to read the next section\n",
    "ide.reader.view_section(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit vs Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run to read the next section\n",
    "ide.reader.view_section(16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Sparse Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run to read the next section\n",
    "ide.reader.view_section(17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dump_sparse_matrix():\n",
    "\n",
    "    from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "    vec = TfidfVectorizer(use_idf=True)\n",
    "    \n",
    "    corpus = [\"another day of rain; rain rain go away, comeback another day\"]\n",
    "    matrix = vec.fit_transform(corpus)\n",
    "    print(matrix.shape)\n",
    "    print(vec.idf_)  # all 1's (there's only 1 document)\n",
    "\n",
    "    coo_format = matrix.tocoo()\n",
    "    print(coo_format.col)\n",
    "    print(coo_format.data)\n",
    "    tuples = zip(coo_format.col, coo_format.data)\n",
    "    in_order = sorted(tuples, key=lambda x: (x[1], x[0]), reverse=True)\n",
    "    features = vec.get_feature_names() # the unique words \n",
    "    print(features)\n",
    "    for score in in_order:\n",
    "        idx = score[0]\n",
    "        word = features[idx]\n",
    "        print(\"{:10s} tfidf:\".format(word), score)\n",
    "\n",
    "dump_sparse_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run to read the next section\n",
    "ide.reader.view_section(19)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run to read the next section\n",
    "ide.reader.view_section(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering and Distance Document Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run to read the next section\n",
    "ide.reader.view_section(21)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run to read the next section\n",
    "ide.reader.view_section(22)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson Assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run to read the next section\n",
    "ide.reader.view_section(23)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test and Submit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run to read the next section\n",
    "ide.reader.view_section(24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(ide.tester.test_notebook()) \n",
    "# print(ide.tester.test_notebook(verbose=True)) \n",
    "\n",
    "# once you are ready -- run this \n",
    "# ide.tester.download_solution()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "TF‚Ä¢IDF (part 2)",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language": "python",
  "story": {
   "auth_token": "4qcBQ0Giv5wY8kQJJ9PHOKNHFJGf_DboMaWV_8fizUU=",
   "authorship_tag": "AB",
   "chapters": 26,
   "name": "TF‚Ä¢IDF (part 2)",
   "parser": {},
   "root": "https://raw.githubusercontent.com/habermanUIUC/CodeStoryLessons/main/lessons",
   "tag": "dmap:text:tfidf-skl"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
