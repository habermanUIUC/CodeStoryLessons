<!DOCTYPE html><html lang='en'><head><title>TF•IDF (part 2)</title><meta charset="utf-8"><style>/*! normalize.css v8.0.1 | MIT License | github.com/necolas/normalize.css */html{line-height:1.15;-webkit-text-size-adjust:100%}body{margin:0}h1{font-size:2em;margin:.67em 0}pre{font-family:monospace,monospace;font-size:1em}a{background-color:transparent}strong{font-weight:bolder}code{font-family:monospace,monospace;font-size:1em}img{border-style:none}input{font-family:inherit;font-size:100%;line-height:1.15;margin:0}input{overflow:visible}[type=checkbox]{box-sizing:border-box;padding:0}::-webkit-file-upload-button{-webkit-appearance:button;font:inherit}blockquote,h1,h2,h3,p,pre{margin:0}html{font-family:system-ui,-apple-system,BlinkMacSystemFont,"Segoe UI",Roboto,"Helvetica Neue",Arial,"Noto Sans",sans-serif,"Apple Color Emoji","Segoe UI Emoji","Segoe UI Symbol","Noto Color Emoji";line-height:1.5}*,::after,::before{box-sizing:border-box;border-width:0;border-style:solid;border-color:#e2e8f0}img{border-style:solid}input:-ms-input-placeholder{color:#a0aec0}input::-ms-input-placeholder{color:#a0aec0}input::-moz-placeholder{color:#a0aec0}h1,h2,h3{font-size:inherit;font-weight:inherit}a{color:inherit;text-decoration:inherit}input{padding:0;line-height:inherit;color:inherit}code,pre{font-family:Menlo,Monaco,Consolas,"Liberation Mono","Courier New",monospace}img{display:block;vertical-align:middle}img{max-width:100%;height:auto}.container{width:100%}@media (min-width:640px){.container{max-width:640px}}@media (min-width:768px){.container{max-width:768px}}@media (min-width:1024px){.container{max-width:1024px}}@media (min-width:1280px){.container{max-width:1280px}}.bg-gray-200{--bg-opacity:1;background-color:#edf2f7;background-color:rgba(237,242,247,var(--bg-opacity))}.bg-gray-300{--bg-opacity:1;background-color:#e2e8f0;background-color:rgba(226,232,240,var(--bg-opacity))}.bg-orange-300{--bg-opacity:1;background-color:#fbd38d;background-color:rgba(251,211,141,var(--bg-opacity))}.bg-green-200{--bg-opacity:1;background-color:#c6f6d5;background-color:rgba(198,246,213,var(--bg-opacity))}.bg-blue-200{--bg-opacity:1;background-color:#bee3f8;background-color:rgba(190,227,248,var(--bg-opacity))}.bg-blue-300{--bg-opacity:1;background-color:#90cdf4;background-color:rgba(144,205,244,var(--bg-opacity))}.border-gray-400{--border-opacity:1;border-color:#cbd5e0;border-color:rgba(203,213,224,var(--border-opacity))}.border-gray-500{--border-opacity:1;border-color:#a0aec0;border-color:rgba(160,174,192,var(--border-opacity))}.border-green-500{--border-opacity:1;border-color:#48bb78;border-color:rgba(72,187,120,var(--border-opacity))}.border-indigo-500{--border-opacity:1;border-color:#667eea;border-color:rgba(102,126,234,var(--border-opacity))}.rounded{border-radius:.25rem}.rounded-full{border-radius:9999px}.border-solid{border-style:solid}.border-2{border-width:2px}.border{border-width:1px}.border-l-2{border-left-width:2px}.border-t{border-top-width:1px}.cursor-pointer{cursor:pointer}.block{display:block}.inline-block{display:inline-block}.flex{display:flex}.justify-center{justify-content:center}.justify-around{justify-content:space-around}.float-left{float:left}.clearfix:after{content:"";display:table;clear:both}.clear-both{clear:both}.font-sans{font-family:system-ui,-apple-system,BlinkMacSystemFont,"Segoe UI",Roboto,"Helvetica Neue",Arial,"Noto Sans",sans-serif,"Apple Color Emoji","Segoe UI Emoji","Segoe UI Symbol","Noto Color Emoji"}.font-serif{font-family:Georgia,Cambria,"Times New Roman",Times,serif}.font-semibold{font-weight:600}.font-bold{font-weight:700}.h-64{height:16rem}.text-xs{font-size:.75rem}.text-sm{font-size:.875rem}.text-base{font-size:1rem}.text-xl{font-size:1.25rem}.leading-none{line-height:1}.leading-normal{line-height:1.5}.m-2{margin:.5rem}.my-3{margin-top:.75rem;margin-bottom:.75rem}.mx-3{margin-left:.75rem;margin-right:.75rem}.my-4{margin-top:1rem;margin-bottom:1rem}.mt-0{margin-top:0}.mt-1{margin-top:.25rem}.mt-2{margin-top:.5rem}.mr-2{margin-right:.5rem}.mb-2{margin-bottom:.5rem}.mr-3{margin-right:.75rem}.mb-3{margin-bottom:.75rem}.mt-4{margin-top:1rem}.mb-4{margin-bottom:1rem}.mt-6{margin-top:1.5rem}.max-w-sm{max-width:24rem}.max-w-2xl{max-width:42rem}.object-contain{-o-object-fit:contain;object-fit:contain}.opacity-0{opacity:0}.overflow-hidden{overflow:hidden}.p-1{padding:.25rem}.p-2{padding:.5rem}.p-3{padding:.75rem}.py-1{padding-top:.25rem;padding-bottom:.25rem}.py-2{padding-top:.5rem;padding-bottom:.5rem}.px-3{padding-left:.75rem;padding-right:.75rem}.py-4{padding-top:1rem;padding-bottom:1rem}.px-4{padding-left:1rem;padding-right:1rem}.px-6{padding-left:1.5rem;padding-right:1.5rem}.pl-3{padding-left:.75rem}.absolute{position:absolute}.shadow-md{box-shadow:0 4px 6px -1px rgba(0,0,0,.1),0 2px 4px -1px rgba(0,0,0,.06)}.shadow-lg{box-shadow:0 10px 15px -3px rgba(0,0,0,.1),0 4px 6px -2px rgba(0,0,0,.05)}.shadow-inner{box-shadow:inset 0 2px 4px 0 rgba(0,0,0,.06)}.text-left{text-align:left}.text-center{text-align:center}.text-black{--text-opacity:1;color:#000;color:rgba(0,0,0,var(--text-opacity))}.text-gray-700{--text-opacity:1;color:#4a5568;color:rgba(74,85,104,var(--text-opacity))}.text-gray-800{--text-opacity:1;color:#2d3748;color:rgba(45,55,72,var(--text-opacity))}.whitespace-no-wrap{white-space:nowrap}.w-1\/2{width:50%}.w-2\/3{width:66.666667%}.w-full{width:100%}@media (min-width:768px){.md\:w-2\/3{width:66.666667%}}.text-tiny{font-size:.5rem!important}body{color:#000!important;font-size:1.25rem!important}.main-content{max-width:900px}.lesson{padding-left:15px!important;padding-right:10px!important;--bg-opacity:1;background-color:#edf2f7;background-color:rgba(237,242,247,var(--bg-opacity))}.main-content,html{font-family:Arial,Georgia,Verdana,"Times New Roman"!important}.lesson-footer-card,.lesson-overview-card{font-family:"Times New Roman"!important}blockquote em:first-child{font-family:Times!important;font-size:1.35em;margin-right:10px}blockquote em:first-child:after{content:":"}.lesson-footer{margin-top:50px;margin-top:20px}span{white-space:nowrap}p.new{padding-top:0;padding-bottom:.5em}p.new+p{padding-top:.5em}h1,h2,h3{font-weight:700;margin-top:.25em!important;margin-bottom:.05em!important;font-family:Georgia,Cambria,"Times New Roman",Times,serif!important}h1{font-size:2em!important;clear:both;color:#000!important}div+h1,h2{margin-top:0!important}h2{margin-top:.5em!important;font-size:1.5em!important;clear:both;color:#8b0000!important}h3{font-size:1.25em!important;clear:both;color:#006400!important}p.new a{text-decoration:underline}.lesson a{text-decoration:underline;color:#00f}.title-text{font-size:2rem}blockquote{font-size:1em;background:#f9f9f9;border-left:10px solid #ccc;margin:.5em 10px;padding:.5em 10px;border-left-color:#ffcd69;border-right-color:#f6ba59;quotes:"\201C""\201D""\2018""\2019"}blockquote:before{color:#ccc;content:open-quote;font-size:4em;line-height:.1em;margin-right:.25em;vertical-align:-.4em}blockquote:after{color:#ccc;content:no-close-quote}blockquote p{display:inline}.shadow-lg{box-shadow:0 10px 15px -3px rgba(0,0,0,.1),0 4px 6px -2px rgba(0,0,0,.5)}img.formula-block{margin-left:auto;margin-right:auto;margin-top:.25rem;margin-bottom:.75rem}img.wrap{margin-right:1rem!important;border:1px solid #021a40}img.border{border:1px solid #021a40;margin-top:.5rem;margin-bottom:.75rem}img.iw300{height:auto;width:auto;max-width:300px}img:not([class]){width:66.666667%;margin-left:1.25rem;border:1px solid #021a40}code{font-size:smaller}pre code{font-size:15px}pre code:not(.line-number){background:#f4f4f4;font-family:monospace;font-size:15px;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;cursor:default;touch-action:none;-webkit-touch-callout:none;-webkit-tap-highlight-color:transparent;clear:both;border:1px solid #ddd;color:#666;page-break-inside:avoid;display:block;min-width:840px;max-width:840px;overflow:scroll;line-height:1.6;margin-bottom:1.6em;padding:1em 1.5em;-moz-tab-size:2;-o-tab-size:2;tab-size:2;word-wrap:break-word;white-space:pre-wrap;border-left:3px solid #f36d33}div.code-starter>pre code{border-left:3px solid #fdff44!important;background-image:radial-gradient(rgba(0,150,0,.75),#000 120%);color:#fff;font:.9rem Inconsolata,monospace}div.code-starter>pre code::after{content:"\a$_"}.tab{font-size:1rem;border-color:#8c6728}.tab-content{max-height:0;max-width:100%;transition:max-height .35s}.tab input:checked~.tab-content{max-height:100vh}.tab input:checked+label{padding:1rem;border-left-width:2px;border-color:#6574cd;background-color:#f8fafc;color:#6574cd}.tab label::after{float:right;right:0;top:0;display:block;width:1em;height:1.5em;line-height:1.5;font-size:1rem;text-align:center;transition:all .35s}.tab input[type=checkbox]+label::after{content:"+";font-weight:700;border-width:1px;border-radius:9999px;border-color:#8c6728}.tab input[type=checkbox]:checked+label::after{transform:rotate(315deg);background-color:#6574cd;color:#f8fafc}</style>
<script src="https://kit.fontawesome.com/7efc4bcee2.js" crossOrigin="anonymous"></script>
<script>
    let stateCheck = setInterval(function(){
      if (document.readyState === 'complete') {
        clearInterval(stateCheck);
        let s1 = document.getElementById('start');
        // console.log('doc is ready', s1);
        if (s1) {
           s1.setAttribute('tabindex', '-1');
           s1.focus(); 
           s1.scrollIntoView({behavior: 'smooth'}); 
           setTimeout(function(){s1.blur()}, 500);
           // console.log('focus set');
        }
      }
    }, 200);
    </script>
</head><body class="lesson"><div class="main-content lesson bg-gray-200 text-black p-1 pl-3 font-serif"><div class="md-inner">
<div id="start" class="section">&nbsp;</div><h1 class="overview"></h1><div class="lesson-overview bg-gray-200 flex justify-center"><div class="text-center px-4 py-2 m-2"><div class="lesson-overview-card displaycard bg-blue-200 max-w-sm rounded overflow-hidden shadow-lg"><div> </div><img alt="Text" class="object-contain h-64 w-full" src="https://raw.githubusercontent.com/habermanUIUC/CodeStoryLessons/main/lessons/dmap/text/tfidf-skl/html/TextTripping-sm.png"/><div class="px-6 py-4"><div class="title-text text-center leading-none font-bold text-xl">TF•IDF (part 2)</div><p class="text-center mt-2 text-gray-800 text-xl">Sklearn's TF•IDF</p><div class="text-gray-700 text-base"> </div><div class="text-center mb-3"><span class="inline-block bg-gray-300 rounded-full px-3 py-1 text-sm font-semibold text-gray-700 mr-2">#scikit-learn</span><span class="inline-block bg-gray-300 rounded-full px-3 py-1 text-sm font-semibold text-gray-700 mr-2">#text</span></div><div class="flex border-t border-solid border-gray-500 shadow-inner justify-around bg-blue-300"><div class="text-gray-700 text-center px-4 m-2 text-sm"><span class="whitespace-no-wrap">D.M. &amp; the 🐍</span></div><div class="text-gray-700 text-center px-4 m-2 text-sm"><span class="whitespace-no-wrap"><strong>Version:</strong> <!-- -->SP21</span></div></div><div class="text-gray-700 mt-1 text-center text-tiny">All Rights Reserved</div></div></div></div><div class="text-center px-4 py-2 m-2 w-1/2"><div class="displaycard bg-gray-200 max-w-sm rounded overflow-hidden shadow-lg"><div class="px-6 py-4 text-left"><div class="text-center font-bold text-xl">TF•IDF (part 2)<br/><div><span>prerequisites</span><div class="text-center text-xs mb-2">(start only after finishing)</div><p class="max-w-sm text-gray-800 text-sm">⦿ <strong>tfidf</strong></p></div></div></div><div class="px-6 py-4 text-left text-gray-800"><div class="text-center font-bold text-xl">Colab Notes</div><p class="max-w-sm text-sm">1. <strong>Copy</strong> this notebook <img alt="copy2drive.png" class="inline-block" src="https://raw.githubusercontent.com/habermanUIUC/CodeStoryLessons/main/lessons/dmap/text/tfidf-skl/html/copy2drive.png"/></p><p class="max-w-sm text-sm">2. <strong>Update</strong> the <strong><code>NET_ID</code></strong> in the notebook</p><p class="max-w-sm text-gray-800 text-sm">3. <strong>Hit ▶️ </strong> to install the INFO 490 IDE</p><div class="text-center font-bold text-xl"> </div><div class="text-center font-bold text-xl">Jupyter/PyCharm Notes</div><p class="max-w-sm text-gray-800 text-sm text-left">The testing framework does <strong>not work</strong> (at this time) for Jupyter  notebooks or local code development.</p></div></div></div></div><h1 class="section" id="section1">TF•IDF (part 2) </h1><p class="new">This lesson continues our learning of TF•IDF, specifically how to use the powerful scikit-learn library to 
perform a TF•IDF analysis.  Not only will you learn how to use the library but also how it calculates TF and IDF.
Our goal for this lesson is to build a tf•idf representation using scikit-learn.  You will learn how it differs from your implementation in the previous lesson.</p><p class="new">In order to get the most out of this lesson, you might want to make a copy of
 the tf•idf lesson so that you can make the requested edits without disturbing
  your original workbook.</p><h2 id="welcome-to-scikit-learn">Welcome to Scikit-learn</h2><img alt="scikit-learn-logo-small.png" class="sm mr-2 float-left max-w-2xl" src="https://raw.githubusercontent.com/habermanUIUC/CodeStoryLessons/main/lessons/dmap/text/tfidf-skl/html/scikit-learn-logo-small.png"/><div class="mt-0"><p class="new">The Python library scikit-learn (pronounced s-eye (like 'science') kit) is a set of modules 
(think Python classes) that can be used to build data analysis and machine learning software.
It is just one toolkit of a family (see <a href="https://scikits.appspot.com/scikits" target="_blank">https://scikits.appspot.com/scikits</a>). SciKit-learn
uses Numpy, SciPy and Matplotlib -- all libraries you worked with in INFO 490.  It's another
mountain to climb, but we will explore it slowly and try to see as much of it as possible.  </p><p class="new">The Python package uses the prefix <code>sklearn</code>, so we may use either <code>sklearn</code>, sci-kit, or scikit-learn to reference the software. </p></div><h2 id="feature-vectors">Feature Vectors</h2><p class="new">This begins our journey into learning some of the vocabulary of machine learning. When you use the <code>collections.Counter</code> class to count occurrences of words,
you are building a simple <strong><em>feature vector</em></strong>. A feature vector contains information describing
an object's more important characteristics.  The vector is usually used as input into a machine 
learning algorithm. Another closely related term, <strong><em>feature engineering</em></strong> is the process of 
determining which features (attributes, 'columns', traits) to use that best represent the input. </p><p class="new">Feature engineering may involve introducing new 'features' that 
are the result of human intervention, algorithm processing, or both.
<img alt="featurevector.png" class="float-left mt-2 mb-4 mr-3 sm border-2 border-green-500" src="https://raw.githubusercontent.com/habermanUIUC/CodeStoryLessons/main/lessons/dmap/text/tfidf-skl/html/featurevector.png"/>A feature vector is almost always numerical and there is a mapping between the original item and its numerical 
representation. </p><p class="new">For now you can think of a feature vector as a set of columns (attributes) for a 
row (an instance/observation). We will be using feature vectors throughout this class. Luckily, scikit-learn has a sub-module that specializes in building feature vectors (a numerical
representation) for text documents.  </p><h2 id="vectorization-">Vectorization </h2><p class="new">Scikit uses the word <em>vectorization</em> as the general process of turning a 
collection of text documents into numerical feature vectors.  We will start with
the <code>CountVectorizer</code> class that builds a fancy version of the <code>collections.Counter</code>.</p><h3 id="the-countvectorizer-class">The <code>CountVectorizer</code> Class</h3><img alt="green_eggs-300.png" class="float-left mb-3 wrap" src="https://raw.githubusercontent.com/habermanUIUC/CodeStoryLessons/main/lessons/dmap/text/tfidf-skl/html/green_eggs-300.png"/><p class="new">We will start our tour using a simple, reduced four chapter story. The function
<code>get_corpus</code> returns this data (and you won't have to type it in!).</p><pre><code>def get_corpus():
    c1 = "Do you like Green eggs and ham"
    c2 = "I do not like them Sam I am  I do not like Green eggs and ham"
    c3 = "Would you like them Here or there"
    c4 = "I would not like them Here or there I would not like them Anywhere"
    return [c1, c2, c3, c4]</code></pre><div class="ide code-starter clearfix"><pre><code>def get_corpus():
    c1 = "Do you like Green eggs and ham"
    c2 = "I do not like them Sam I am  I do not like Green eggs and ham"
    c3 = "Would you like them Here or there"
    c4 = "I would not like them Here or there I would not like them Anywhere"
    return [c1, c2, c3, c4]</code></pre></div><p class="new">Let's now see how we can use the <code>CountVectorizer</code> class with that corpus:</p><pre><code>from sklearn.feature_extraction.text import CountVectorizer

def cv_demo1():

    corpus = get_corpus()

    # normalize all the words to lowercase
    cvec = CountVectorizer(lowercase=True)

    # convert the documents into a document-term matrix
    doc_term_matrix = cvec.fit_transform(corpus)

    # get the terms found in the corpus
    print(cvec.get_feature_names())

    # get the counts
    print(doc_term_matrix.toarray())

cv_demo1()</code></pre><p class="new">Type in the above code, run it (fix any errors) </p><div class="ide code-starter clearfix"><pre><code>def cv_demo1():
    # fill me in
    pass
    
cv_demo1()</code></pre></div><p class="new">Your output should match the following:</p><pre><code>['am', 'and', 'anywhere', 'do', 'eggs', 'green', 'ham', 'here', 'like', 'not', 'or', 'sam', 'them', 'there', 'would', 'you']
[[0 1 0 1 1 1 1 0 1 0 0 0 0 0 0 1]
 [1 1 0 2 1 1 1 0 2 2 0 1 1 0 0 0]
 [0 0 0 0 0 0 0 1 1 0 1 0 1 1 1 1]
 [0 0 1 0 0 0 0 1 2 2 1 0 2 1 2 0]]</code></pre><p class="new">The one issue is that the token 'I' is missing.  The <code>CountVectorizer</code> ignores tokens less than 2
characters long by default.  You can pass in your own tokenizer (among many parameters)
to customize how it parses the text.  </p><p class="new">Update the code below (see the comments), and run <code>cv_demo2</code> (note that the function has been updated to return both 
the tokens and the document matrix). Note we are passing in the same tokenizer you wrote (that had a default of <code>min_length=0</code>).  You can
re-implement it or just copy&amp;paste it from your previous tf•idf lesson.</p><div class="ide code-starter clearfix"><pre><code># add in the function split_into_tokens
# parameters: data, normalize, min_length (in that order)
# returns a list of words/tokens
# normalize has a default value of True 
# min_length has a default value of 0
# if normalize, make tokens lowercase
# only returns tokens longer than min_length
def cv_demo2():

    corpus = get_corpus()

    # pass in our own tokenizer
    cvec = CountVectorizer(tokenizer=split_into_tokens)

    # convert the documents into a document-term matrix
    doc_term_matrix = cvec.fit_transform(corpus)

    # get the terms found in the corpus
    tokens = cvec.get_feature_names()
    
    return doc_term_matrix, tokens

dtm, tokens = cv_demo2()
print(tokens)
print(dtm.toarray())</code></pre></div><p class="new">After you re-run the code, you should get the following output.</p><pre><code>['am', 'and', 'anywhere', 'do', 'eggs', 'green', 'ham', 'here', 'i', 'like', 'not', 'or', 'sam', 'them', 'there', 'would', 'you']
[[0 1 0 1 1 1 1 0 0 1 0 0 0 0 0 0 1]
 [1 1 0 2 1 1 1 0 3 2 2 0 1 1 0 0 0]
 [0 0 0 0 0 0 0 1 0 1 0 1 0 1 1 1 1]
 [0 0 1 0 0 0 0 1 2 2 2 1 0 2 1 2 0]]</code></pre><h2 id="visualize-with-pandas">'Visualize' with Pandas</h2><p class="new">We can use Pandas to help make this a bit easier to look at.  We are also going to reuse <code>cv_demo2</code> 
to allow us to focus on the new code. </p><div class="ide code-starter clearfix"><pre><code>import pandas as pd

def word_matrix_to_df(wm, feature_names):
    # create an index for each row
    doc_names = ['Doc{:d}'.format(idx+1) for idx, _ in enumerate(wm)]
    df = pd.DataFrame(data=wm.toarray(), index=doc_names, columns=feature_names)
    return df

def cv_demo3():

    doc_term_matrix, tokens = cv_demo2()
    df = word_matrix_to_df(doc_term_matrix, tokens)
    
    return df

df = cv_demo3()
print(df.head())</code></pre></div><p class="new">You should see data that matches the table below:
<img alt="tfidf-cv.png" class="mt-2 mr-3 sm" src="https://raw.githubusercontent.com/habermanUIUC/CodeStoryLessons/main/lessons/dmap/text/tfidf-skl/html/tfidf-cv.png"/></p><p class="new">Be sure to take a pause to understand everything that has been done so far. 
Also take a look at sklearn's <a href="https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html#sklearn.feature_extraction.text.CountVectorizer" target="_blank">documentation</a>.
The <code>CountVectorizer</code> allows you to customize many of the parsing routines:</p><pre><code>cvec = CountVectorizer(tokenizer=split_into_tokens,  # custom tokenizer
                       ngram_range=(1, 2),   # both single and bi-grams
                       stop_words=['and', 'but'])   # custom stop words</code></pre><h3 id="tfidftransformer"><code>TfidfTransformer</code></h3><p class="new">Now that we have a vector of word counts, we want to transform those counts into a TF•IDF 
matrix (essentially the same process as the previous lesson).  We will use sklearn's <code>TfidfTransformer</code> class.  It is meant to work side-by-side with
 the <code>CountVectorizer</code>.</p><pre><code>from sklearn.feature_extraction.text import TfidfTransformer
def cv_demo_idf():
    
    # get the data from the CountVectorizer
    doc_term_matrix, tokens = cv_demo2()
    
    # create the tf•idf transformer
    tfidf_transformer=TfidfTransformer()
    
    # transform the doc_term_matrix into TF•IDF
    tfidf_transformer.fit(doc_term_matrix)

    # make it a dataframe for easy viewing
    df = pd.DataFrame(tfidf_transformer.idf_,
                      index=tokens, columns=["idf_weights"])
    # sort descending
    df.sort_values(by=['idf_weights'], inplace=True, ascending=False)

    return df

df = cv_demo_idf()
print(df.head(20))</code></pre><p class="new">Type in the code above and run it in the next code cell.</p><div class="ide code-starter clearfix"><pre><code></code></pre></div><p class="new">For easy comparison, here's the same IDF values from the previous lesson.  What do you notice?
<img alt="idf-cv.png" class="mt-2 mb-4 mr-3 border" src="https://raw.githubusercontent.com/habermanUIUC/CodeStoryLessons/main/lessons/dmap/text/tfidf-skl/html/idf-cv.png"/></p><p class="new">The calculated values do not match (e.g. for the token 'i', we calculated 0.693; and sklearn's value is 1.511). 
However the relative magnitudes of the words seems about correct. In order to
 get the numbers to match, you will need to adjust <em>both</em> your TF and IDF calculations and adjust the calculations that <code>TfidfTransformer</code> uses.</p><p class="new">One of the key points of this lesson is to understand and confirm the output of 
various ]algorithms. It's almost always necessary to confirm or validate your 
implementation (or your use of another library) with a reference. 
Let's go through all the adjustments.</p><h3 id="idf-formula">IDF Formula</h3><p class="new">We used the standard formula for idf as
<img alt="math?math=%5CLarge%20idf_j%20%5C%2C%5C%2C%20%3D%20%5C%2C%5C%2C%20log(%5Cdfrac%7Bn%7D%7Bdf_j%20%7D)" class="mb-2 formula-block" src="https://render.githubusercontent.com/render/math?math=%5CLarge%20idf_j%20%5C%2C%5C%2C%20%3D%20%5C%2C%5C%2C%20log(%5Cdfrac%7Bn%7D%7Bdf_j%20%7D)"/></p><p class="new">Sklearn's default idf formula is <strong>1 + math.log( (N+1)/(n+1) )</strong> where N is total number of documents and n is the 
number of documents for the term.  They add one to prevent division by zero and prevent the taking the log 
of zero.  When setting the named parameter <code>smooth_idf</code> to False, the formula becomes 1 + ln(N/n).</p><pre><code>  # will use 1 + ln(N/n)
  tfidf_transformer=TfidfTransformer(smooth_idf=False)</code></pre><p class="new">If you go back to the previous lesson (and you should) and update the IDF formula to <code>1 + ln( (N+1)/(n + 1)</code>,
you should get 1.5108 for the word 'i'.  </p><div><span>[✅] Matching IDF</span></div><br/><p class="new">Let's now print out the TF•IDF values and see where we are: </p><div class="ide code-starter clearfix"><pre><code>def cv_demo_tf_idf():

    doc_term_matrix, tokens = cv_demo2()
    tfidf_transformer=TfidfTransformer(smooth_idf=True)

    # learn the IDF vector
    tfidf_transformer.fit(doc_term_matrix)
    idf = tfidf_transformer.idf_

    # transform the count matrix to tf-idf
    tf_idf_vector = tfidf_transformer.transform(doc_term_matrix)
    print(tf_idf_vector)</code></pre></div><p class="new">You should see the following, when you run the function (below is just a
 sample of the output):</p><pre><code>(0, 3)   0.39411340505265774 (doc 0, token w/index 3 has tf•idf = 0.3941)
(0, 1)   0.39411340505265774
(1, 13)  0.1568972451918658
(1, 12)  0.24580985322181814 (doc 1, token w/index 12 has tf•idf = 0.2458)</code></pre><p class="new">This isn't too easy to read, we can use Pandas to help make this easy.</p><div class="ide code-starter clearfix"><pre><code>def cv_demo_pd_tf_idf():

    doc_term_matrix, tokens = cv_demo2()
    tfidf_transformer=TfidfTransformer(smooth_idf=True)

    # learn the IDF vector
    tfidf_transformer.fit(doc_term_matrix)
    idf = tfidf_transformer.idf_

    # transform the count matrix to tf-idf
    tf_idf_vector = tfidf_transformer.transform(doc_term_matrix)

    # print out the values 
    # for the token 'i' in the second document
    token = 'i'
    doc = 1
    df_idf = pd.DataFrame(idf, index=tokens, columns=["idf_weights"])
    df_idf.sort_values(by=['idf_weights'], inplace=True, ascending=False)
    idf_token = df_idf.loc[token]['idf_weights']

    doc_vector = tf_idf_vector[doc]
    df_tfidf = pd.DataFrame(doc_vector.T.todense(), index=tokens, columns=["tfidf"])
    df_tfidf.sort_values(by=["tfidf"], ascending=False, inplace=True)
    tfidf_token = df_tfidf.loc[token]['tfidf']

    # tfidf = tf * idf
    tf_token = tfidf_token / idf_token
    print('TF    {:s} {:2.4f}'.format(token, tf_token))
    print('IDF   {:s} {:2.4f}'.format(token, idf_token))
    print('TFIDF {:s} {:2.4f}'.format(token, tfidf_token))</code></pre></div><p class="new">Once you run the code you should see the following for the second document, token 'i':</p><pre><code>TF    i 0.3845
IDF   i 1.5108
TFIDF i 0.5814</code></pre><p class="new">However, in the previous lesson (with the updated IDF formula), you will see the following:</p><pre><code>TF:     0.1875
IDF:    1.5108
TFIDF:  0.28328</code></pre><div class="mb-3"><span>[❌] Matching TF (NO!!!)</span></div><p class="new">Since the IDF values match and that we can only really see the end result (TF •IDF), 
we can infer that the issue (two actually) must be with the TF formula.</p><h3 id="tf-formula-">TF Formula </h3><p class="new">By default sklearn is using the raw term counts.  You can adjust this slightly by setting the named parameter 
<code>sublinear_tf=True</code>.  This changes the formula to 1 + math.log(tf) where tf is the number of occurrences
of the term in the document.</p><p class="new">We used a normalized term count (which was divided by the length of the document).  There's no option
in sklearn for this option.  So let's update our TF calculation to be the following:</p><p class="new">1 + math.log(tf) to match sci-kit learn's.</p><p class="new">Where <strong>tf</strong> is the count of how many times term appear in the current document</p><h3 id="normalization">Normalization</h3><p class="new">By Default, the final TF•IDF calculations are normalized using the L2 norm (a.k.a. Euclidean).<br/>The L2 norm is just the square root of the sum of the squared vector values:</p><img alt="math?math=%5CLarge%20v_%7Bnorm%7D%20%3D%20%5Cfrac%7Bv%7D%7B%7C%7Cv%7C%7C_2%7D%20%3D%20%5Cfrac%7Bv%7D%7B%5Csqrt%7Bv_1%5E2%20%2B%20v_2%5E2%20%2B%20%5Cdots%20%2B%20v_n%5E2%7D%7D" class=" formula-block" src="https://render.githubusercontent.com/render/math?math=%5CLarge%20v_%7Bnorm%7D%20%3D%20%5Cfrac%7Bv%7D%7B%7C%7Cv%7C%7C_2%7D%20%3D%20%5Cfrac%7Bv%7D%7B%5Csqrt%7Bv_1%5E2%20%2B%20v_2%5E2%20%2B%20%5Cdots%20%2B%20v_n%5E2%7D%7D"/><p class="new">As an example, let's look at how L2 normalization is done using TF•IDF values for the word 'I':</p><pre><code>L2_norm = sqrt(tfidf('am')²    + tfidf('and')² + .. + tfidf('i')² + ..  +
               tfidf('would')² + tfidf('you')²)

tfidf_norm('i') = tfidf('i')/L2_norm</code></pre><p class="new">The nice thing about applying L2 normalization is that it puts different features on the same scale.<br/>Also, mathematically, the L2 Norm of the resulting normalized values is 1.0.  That is if you took the square root 
of the sum of the squared tfidf norm values (e.g. <code>tfidf_norm('i')</code>), it would be 1.0.</p><p class="new">The L1 norm uses the sum of the absolute values (a.k.a. Manhattan distance).  We will see more examples in the future of using 
L2 normalization.  A closely related topic, L2 regularization will be discussed later as well.</p><p class="new">We will NOT do this to our code in the previous lesson -- it would require a lot of code refactoring.  Instead, we 
will turn normalization off when we create a <code>TfidfTransformer</code>.</p><p class="new">In order to turn off normalization (remember our goal is to get sklearn's output to match our output from
the previous lesson), we can set the <code>norm</code> parameter to None:</p><pre><code>TfidfTransformer(smooth_idf=True, sublinear_tf=True, norm=None)</code></pre><p class="new">Now we should see the following after running <code>cv_demo_pd_tf_idf</code>. These numbers 
will match the output of your code in the previous lesson (if you made to two respective changes).</p><pre><code>TF    i 2.0986
IDF   i 1.5108
TFIDF i 3.1706</code></pre><div class="mb-3"><span>[✅] Matching IDF</span><br/><span>[✅] Matching TF</span></div><p class="new">Here's a quick summary of what we calculated (using the token 'i' for the example):
<img alt="tfidf-geah.png" src="https://raw.githubusercontent.com/habermanUIUC/CodeStoryLessons/main/lessons/dmap/text/tfidf-skl/html/tfidf-geah.png"/></p><p class="new">Please for the love of your brain, take a break to ensure you understand the
 above image and its numbers.  Its <em>so</em> easy to gloss over it and convince
 ourselves we 'get it'.</p><h2 id="the-tfidfvectorizer">The <code>TfidfVectorizer</code></h2><p class="new">Another class provided by scikit-learn is the <code>TfidfVectorizer</code> class. This class creates its own <code>CountVectorizer</code> to use.
You use it essentially the same, but it's bit more compact:</p><pre><code>cv = TfidfVectorizer(smooth_idf=True, use_idf=True, tokenizer=split_into_tokens, norm=None)
tfidf = cv.fit_transform(corpus)
tokens = cv.get_feature_names()
idf = cv.idf_
values = tfidf.todense().tolist()</code></pre><h1 class="section" id="section2">Fit vs Transform </h1><p class="new">As we have seen, there are both <code>fit</code> and <code>transform</code> methods on the <code>TfidfTransformer</code> (there's <code>fit_transform</code> too, 
but that's just the combination of the two).  The method <code>fit</code> attempts to build (and train) a model 
based on the data given to it. In this specific case, we give it the documents, and 'fit' 
builds an idf vector.  It essentially transforms the raw data into normalized data. </p><p class="new">Usually for sklearn, <code>fit</code> is associated with the <em>training</em> phase in machine learning. 
Fitting is similar to finding the best parameters for a model.  These parameters are then
used to transform the data.  As we march forward, this will become a bit clearer.</p><p class="new">If 'fitting' attempts to train a model and/or parameters, the <code>transform</code> method then 
applies what was fitted to incoming data.  For this specific situation, transform 
creates the TF•IDF vectors.  However, for sklearn, transform is associated with 
the <em>testing</em> phase in machine learning.  That is the part where we test the 
accuracy of the model.</p><p class="new">This will become much clearer when we get to machine learning. TF•IDF is 
actually preparing our data to be used (possibly) with other machine learning 
algorithms.  Since computers can't process text directly, changing them into 
numeric vectors is an essential preparation stage for almost all machine 
learning algorithms (that work on text).</p><blockquote><p class="new"><strong><em>Machine Learner's Log</em></strong> When building a machine learning 'model', you will typically split your data into
two parts: a <strong>training set</strong> and a <strong>validation set</strong>. The 'fit' method
 works on the training set.  The 'transform' method works on the validation set.<br/>The testing set helps determine the accuracy of the model (or the fitting part).</p></blockquote><p class="new">Sklearn is trying to use the fit/transform vocabulary in a non machine learning situation.  For us, it is
building a vector space model (VSM) or a sparse feature set.</p><h1 class="section" id="section3">The Sparse Matrix</h1><p class="new">Perhaps you are wondering exactly what is the thing/object/data returned from 
either <code>transform</code> or <code>fit_transform</code>?</p><pre><code>tf_idf_vector = tfidf_transformer.transform(doc_term_matrix)</code></pre><p class="new">It is actually a <em>sparse</em> matrix (<a href="https://docs.scipy.org/doc/scipy/reference/sparse.html" target="_blank">see sklearn doc</a>). 
As you can imagine, as the set of documents become larger, there will a large set of words that only belong
to a small set of documents; hence, the tf•idf matrix will contain mostly zeros.
<img alt="sparse_doc.png" class="float-left mx-3 my-3 border iw300" src="https://raw.githubusercontent.com/habermanUIUC/CodeStoryLessons/main/lessons/dmap/text/tfidf-skl/html/sparse_doc.png"/></p><p class="new">The sklearn sparse matrix efficiently stores the necessary data.  Many of sklearn's algorithms can work with a sparse matrix
as well as regular Numpy vectors.<br/><br class="clear-both"/></p><blockquote><p class="new"><em><strong>Coder's Log</strong></em>
As you become more familiar with using sparse matrices, you may come across
 references to <code>indptr</code>, <code>data</code>, <code>indices</code>. 
The image below helps depict sklearn's efficient (via indirection
) implementation of sparse matrices: 
<img alt="sparse_matrix.png" class="border" src="https://raw.githubusercontent.com/habermanUIUC/CodeStoryLessons/main/lessons/dmap/text/tfidf-skl/html/sparse_matrix.png"/></p></blockquote><div class="clear-both"></div><h2 id="visualizing-the-data">'Visualizing' the data</h2><p class="new">It's easy to get all the non-zero values out of a sparse matrix.  The <strong>Coo</strong>rdinate 
format allows you to get both the index (e.g row or row,column -- for 2D) 
and the values.  The following section of code can help to understand and work with
the sparse matrix data:</p><div class="ide code-starter clearfix"><pre><code>def dump_sparse_matrix():

    from sklearn.feature_extraction.text import TfidfVectorizer
    vec = TfidfVectorizer(use_idf=True)
    
    corpus = ["another day of rain; rain rain go away, comeback another day"]
    matrix = vec.fit_transform(corpus)
    print(matrix.shape)
    print(vec.idf_)  # all 1's (there's only 1 document)

    coo_format = matrix.tocoo()
    print(coo_format.col)
    print(coo_format.data)
    tuples = zip(coo_format.col, coo_format.data)
    in_order = sorted(tuples, key=lambda x: (x[1], x[0]), reverse=True)
    features = vec.get_feature_names() # the unique words 
    print(features)
    for score in in_order:
        idx = score[0]
        word = features[idx]
        print("{:10s} tfidf:".format(word), score)

dump_sparse_matrix()</code></pre></div><h2 id="pandas-to-the-rescue">Pandas to the rescue</h2><p class="new">When you use libraries like Pandas, you can easily convert these sparse matrices 
into regular Numpy arrays:</p><pre><code>df = pd.DataFrame(data=matrix.toarray(), columns=vec.get_feature_names())</code></pre><h1 class="section" id="section4">Gensim</h1><p class="new">Another popular library for working with text is <a href="https://radimrehurek.com/gensim/" target="_blank">gensim</a>.  It also
has TF•IDF implementations as well.  Although we will not be using this library now, it's important to know
that it is an option.  We will use gensim very soon!</p><pre><code>corpus  = [tokenize(doc) for doc in corpus]
lexicon = gensim.corpora.Dictionary(corpus)
tfidf   = gensim.models.TfidfModel(dictionary=lexicon, normalize=True)
vectors = [tfidf[lexicon.doc2bow(doc)] for doc in corpus]</code></pre><h1 class="section" id="section5">Clustering and Distance Document Similarity</h1><p class="new">Now that we have a matrix where the rows are documents (document vectors) of normalized TF•IDF values
we can "easily" compare two documents and see how similar (or different they are).</p><p class="new">The distance between two document vectors is the essence behind using TF•IDF for document 
clustering (an unsupervised machine learning algorithm), as well as trying to retrieved relevant 
documents by search terms.  Your search terms become a 'document' and gets converted to a vector.
Distances are then calculated for each document to the query/search vector.</p><p class="new">Much of this will be covered in a separate lesson on distance metrics. There's a lot more to cover.</p><h1 class="section" id="section6">Review</h1><div class="font-sans container mt-1 mb-4 "><p>🎗Before you go, you should <strong>know</strong>:</p><div class="w-2/3 md:w-2/3"><div class="shadow-md"><div class="tab overflow-hidden border-t bg-green-200"><input class="absolute opacity-0" id="tab-multi-0" name="tabs" type="checkbox"/><label class="block p-3 leading-normal cursor-pointer" for="tab-multi-0"><span> What does <code>CountVectorizer</code> do </span></label><div class="tab-content overflow-hidden border-l-2 bg-orange-300 border-indigo-500 leading-normal"><p class="p-3">No Answer</p></div></div><div class="tab overflow-hidden border-t bg-green-200"><input class="absolute opacity-0" id="tab-multi-1" name="tabs" type="checkbox"/><label class="block p-3 leading-normal cursor-pointer" for="tab-multi-1"><span> What does <code>TfidfTransformer</code> do </span></label><div class="tab-content overflow-hidden border-l-2 bg-orange-300 border-indigo-500 leading-normal"><p class="p-3">No Answer</p></div></div><div class="tab overflow-hidden border-t bg-green-200"><input class="absolute opacity-0" id="tab-multi-2" name="tabs" type="checkbox"/><label class="block p-3 leading-normal cursor-pointer" for="tab-multi-2"><span> The Meaning of sklearn's <code>fit</code> function </span></label><div class="tab-content overflow-hidden border-l-2 bg-orange-300 border-indigo-500 leading-normal"><p class="p-3">No Answer</p></div></div><div class="tab overflow-hidden border-t bg-green-200"><input class="absolute opacity-0" id="tab-multi-3" name="tabs" type="checkbox"/><label class="block p-3 leading-normal cursor-pointer" for="tab-multi-3"><span> The Meaning of sklearn's <code>transform</code> function </span></label><div class="tab-content overflow-hidden border-l-2 bg-orange-300 border-indigo-500 leading-normal"><p class="p-3">No Answer</p></div></div></div></div></div><h1 class="section" id="section7">Lesson Assignment</h1><p class="new">For this lesson, if you have been following along and implementing the
 requested functionality, you are done!  </p><p class="new">Be sure to test everything out:</p><pre><code>print(ide.tester.list_tests())
print(ide.tester.test_function('name of test'))</code></pre><h1>Test and Submit</h1><p>Once you have finished, you can download your code (via <code>ide.tester</code>) and upload that file to Gradescope (find lesson with tag <strong>tfidf-skl</strong>).</p><div class="my-4"><pre><code><strong># to list the tests available</strong><br/>print(ide.tester.list_tests())<br/><strong># to perform a specific test</strong><br/>print(ide.tester.test_functionality('name of test'))<br/><strong># to test your code (either works)</strong><br/>print(ide.tester.test_notebook())<br/>print(ide.tester.test_notebook(verbose=True))<br/><strong># to prepare and download your code</strong><br/>ide.tester.download_solution()</code></pre></div><div class="lesson-footer flex bg-gray-200 justify-center"><div class="lesson-footer-card displaycard bg-blue-200 border-t border-gray-400 max-w-2xl rounded overflow-hidden shadow-lg"><div class="px-6 py-4"><div class="title-text text-center font-bold text-xl">TF•IDF (part 2)</div><p class="text-center text-gray-800 text-xl">Sklearn's TF•IDF</p><div class="text-center mt-6 text-xl"><i aria-hidden="true" class="fas fa-tags"></i> any questions on Piazza with <span class="font-bold">tfidf-skl</span></div><div class="text-gray-700 text-base"> </div><div></div><div class="text-gray-700 text-base">References and Additional Readings</div><div class="text-xs p-2 border border-solid border-gray-500 bg-gray-300"> <div class="text-gray-700 px-4 m-2"><span> </span> <strong>towardsdatascience.com/how-sklearns-tf-idf-is-different-from-the-standard-tf-idf-275fa582e73d</strong></div><div class="text-gray-700 px-4 m-2"><span> </span> <strong>scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html</strong></div><div class="text-gray-700 px-4 m-2"><span> </span> <strong>blog.christianperone.com/2011/10/machine-learning-text-feature-extraction-tf-idf-part-ii</strong></div></div><div class="flex mt-4 border-t border-solid border-gray-500 justify-around bg-gray-200"><div class="text-gray-700 text-center px-4 m-2 text-sm">D.M. &amp; the 🐍</div><div class="text-gray-700 text-center px-4 m-2 text-sm"><strong>Version:</strong> <!-- -->SP21</div></div><div class="text-gray-700 mt-2 text-center text-sm font-bold">All Rights Reserved Michael Haberman</div><div class="text-gray-700 text-center text-sm">Do not distribute this notebook</div></div></div></div><div> </div><div class="ide code-starter clearfix"><pre><code># print(ide.tester.test_notebook()) 
# print(ide.tester.test_notebook(verbose=True)) 

# once you are ready -- run this 
# ide.tester.download_solution() 
</code></pre></div></div></div></body></html>