<!DOCTYPE html><html lang='en'><head><title>TF•IDF</title><meta charset="utf-8"><style>/*! normalize.css v8.0.1 | MIT License | github.com/necolas/normalize.css */html{line-height:1.15;-webkit-text-size-adjust:100%}body{margin:0}h1{font-size:2em;margin:.67em 0}pre{font-family:monospace,monospace;font-size:1em}strong{font-weight:bolder}code{font-family:monospace,monospace;font-size:1em}img{border-style:none}input{font-family:inherit;font-size:100%;line-height:1.15;margin:0}input{overflow:visible}[type=checkbox]{box-sizing:border-box;padding:0}::-webkit-file-upload-button{-webkit-appearance:button;font:inherit}h1,h2,p,pre{margin:0}ol,ul{list-style:none;margin:0;padding:0}html{font-family:system-ui,-apple-system,BlinkMacSystemFont,"Segoe UI",Roboto,"Helvetica Neue",Arial,"Noto Sans",sans-serif,"Apple Color Emoji","Segoe UI Emoji","Segoe UI Symbol","Noto Color Emoji";line-height:1.5}*,::after,::before{box-sizing:border-box;border-width:0;border-style:solid;border-color:#e2e8f0}img{border-style:solid}input:-ms-input-placeholder{color:#a0aec0}input::-ms-input-placeholder{color:#a0aec0}input::-moz-placeholder{color:#a0aec0}h1,h2{font-size:inherit;font-weight:inherit}input{padding:0;line-height:inherit;color:inherit}code,pre{font-family:Menlo,Monaco,Consolas,"Liberation Mono","Courier New",monospace}img{display:block;vertical-align:middle}img{max-width:100%;height:auto}.container{width:100%}@media (min-width:640px){.container{max-width:640px}}@media (min-width:768px){.container{max-width:768px}}@media (min-width:1024px){.container{max-width:1024px}}@media (min-width:1280px){.container{max-width:1280px}}.bg-gray-200{--bg-opacity:1;background-color:#edf2f7;background-color:rgba(237,242,247,var(--bg-opacity))}.bg-gray-300{--bg-opacity:1;background-color:#e2e8f0;background-color:rgba(226,232,240,var(--bg-opacity))}.bg-orange-300{--bg-opacity:1;background-color:#fbd38d;background-color:rgba(251,211,141,var(--bg-opacity))}.bg-green-200{--bg-opacity:1;background-color:#c6f6d5;background-color:rgba(198,246,213,var(--bg-opacity))}.bg-blue-200{--bg-opacity:1;background-color:#bee3f8;background-color:rgba(190,227,248,var(--bg-opacity))}.bg-blue-300{--bg-opacity:1;background-color:#90cdf4;background-color:rgba(144,205,244,var(--bg-opacity))}.border-gray-400{--border-opacity:1;border-color:#cbd5e0;border-color:rgba(203,213,224,var(--border-opacity))}.border-gray-500{--border-opacity:1;border-color:#a0aec0;border-color:rgba(160,174,192,var(--border-opacity))}.border-indigo-500{--border-opacity:1;border-color:#667eea;border-color:rgba(102,126,234,var(--border-opacity))}.rounded{border-radius:.25rem}.rounded-full{border-radius:9999px}.border-solid{border-style:solid}.border{border-width:1px}.border-l-2{border-left-width:2px}.border-t{border-top-width:1px}.cursor-pointer{cursor:pointer}.block{display:block}.inline-block{display:inline-block}.flex{display:flex}.justify-center{justify-content:center}.justify-around{justify-content:space-around}.float-left{float:left}.clearfix:after{content:"";display:table;clear:both}.clear-both{clear:both}.font-sans{font-family:system-ui,-apple-system,BlinkMacSystemFont,"Segoe UI",Roboto,"Helvetica Neue",Arial,"Noto Sans",sans-serif,"Apple Color Emoji","Segoe UI Emoji","Segoe UI Symbol","Noto Color Emoji"}.font-serif{font-family:Georgia,Cambria,"Times New Roman",Times,serif}.font-semibold{font-weight:600}.font-bold{font-weight:700}.h-64{height:16rem}.text-xs{font-size:.75rem}.text-sm{font-size:.875rem}.text-base{font-size:1rem}.text-xl{font-size:1.25rem}.leading-none{line-height:1}.leading-normal{line-height:1.5}.m-2{margin:.5rem}.my-4{margin-top:1rem;margin-bottom:1rem}.mt-1{margin-top:.25rem}.mt-2{margin-top:.5rem}.mr-2{margin-right:.5rem}.mb-2{margin-bottom:.5rem}.mt-3{margin-top:.75rem}.mb-3{margin-bottom:.75rem}.mt-4{margin-top:1rem}.mb-4{margin-bottom:1rem}.mt-6{margin-top:1.5rem}.mb-6{margin-bottom:1.5rem}.max-w-sm{max-width:24rem}.max-w-2xl{max-width:42rem}.object-contain{-o-object-fit:contain;object-fit:contain}.opacity-0{opacity:0}.overflow-hidden{overflow:hidden}.p-1{padding:.25rem}.p-3{padding:.75rem}.py-1{padding-top:.25rem;padding-bottom:.25rem}.py-2{padding-top:.5rem;padding-bottom:.5rem}.px-3{padding-left:.75rem;padding-right:.75rem}.py-4{padding-top:1rem;padding-bottom:1rem}.px-4{padding-left:1rem;padding-right:1rem}.px-6{padding-left:1.5rem;padding-right:1.5rem}.pl-3{padding-left:.75rem}.pb-6{padding-bottom:1.5rem}.absolute{position:absolute}.shadow-md{box-shadow:0 4px 6px -1px rgba(0,0,0,.1),0 2px 4px -1px rgba(0,0,0,.06)}.shadow-lg{box-shadow:0 10px 15px -3px rgba(0,0,0,.1),0 4px 6px -2px rgba(0,0,0,.05)}.shadow-inner{box-shadow:inset 0 2px 4px 0 rgba(0,0,0,.06)}.text-left{text-align:left}.text-center{text-align:center}.text-black{--text-opacity:1;color:#000;color:rgba(0,0,0,var(--text-opacity))}.text-gray-700{--text-opacity:1;color:#4a5568;color:rgba(74,85,104,var(--text-opacity))}.text-gray-800{--text-opacity:1;color:#2d3748;color:rgba(45,55,72,var(--text-opacity))}.whitespace-no-wrap{white-space:nowrap}.w-1\/2{width:50%}.w-2\/3{width:66.666667%}.w-full{width:100%}@media (min-width:768px){.md\:w-2\/3{width:66.666667%}}.text-tiny{font-size:.5rem!important}body{color:#000!important;font-size:1.25rem!important}.main-content{max-width:900px}.lesson{padding-left:15px!important;padding-right:10px!important;--bg-opacity:1;background-color:#edf2f7;background-color:rgba(237,242,247,var(--bg-opacity))}.main-content,html{font-family:Arial,Georgia,Verdana,"Times New Roman"!important}.lesson-footer-card,.lesson-overview-card{font-family:"Times New Roman"!important}.lesson-footer{margin-top:50px;margin-top:20px}.lesson ol{list-style-type:decimal;list-style-position:inside;margin-left:1em}.lesson ul{list-style-position:inside;list-style-type:none;margin-left:1em}.lesson ul li{padding-left:1em;padding-right:5px}.lesson ul li::before{content:"•";padding-right:5px}span{white-space:nowrap}p.new{padding-top:0;padding-bottom:.5em}p.new+p{padding-top:.5em}div.no-indent p.new{text-indent:0!important}h1,h2{font-weight:700;margin-top:.25em!important;margin-bottom:.05em!important;font-family:Georgia,Cambria,"Times New Roman",Times,serif!important}h1{font-size:2em!important;clear:both;color:#000!important}div+h1,h2{margin-top:0!important}h2{margin-top:.5em!important;font-size:1.5em!important;clear:both;color:#8b0000!important}ul{margin-bottom:30px}.title-text{font-size:2rem}.shadow-lg{box-shadow:0 10px 15px -3px rgba(0,0,0,.1),0 4px 6px -2px rgba(0,0,0,.5)}img.formula-block{margin-left:auto;margin-right:auto;margin-top:.25rem;margin-bottom:.75rem}img.formula-inline{margin-top:.25rem;margin-bottom:.25rem}img.wrap{margin-right:1rem!important;border:1px solid #021a40}img.border{border:1px solid #021a40;margin-top:.5rem;margin-bottom:.75rem}img.iw300{height:auto;width:auto;max-width:300px}code{font-size:smaller}pre code{font-size:15px}.code-large{background:#f4f4f4;font-family:monospace;font-size:15px;margin:10px;font-size:15px}.code{background:#f4f4f4;font-family:monospace;font-size:15px;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;cursor:default;touch-action:none;-webkit-touch-callout:none;-webkit-tap-highlight-color:transparent;clear:both;border:1px solid #ddd;color:#666;page-break-inside:avoid;display:block;min-width:840px;max-width:840px;overflow:scroll;line-height:1.6;margin-bottom:1.6em;padding:1em 1.5em;-moz-tab-size:2;-o-tab-size:2;tab-size:2;word-wrap:break-word;white-space:pre-wrap;border-left:3px solid #f36d33!important}pre code:not(.line-number){background:#f4f4f4;font-family:monospace;font-size:15px;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;cursor:default;touch-action:none;-webkit-touch-callout:none;-webkit-tap-highlight-color:transparent;clear:both;border:1px solid #ddd;color:#666;page-break-inside:avoid;display:block;min-width:840px;max-width:840px;overflow:scroll;line-height:1.6;margin-bottom:1.6em;padding:1em 1.5em;-moz-tab-size:2;-o-tab-size:2;tab-size:2;word-wrap:break-word;white-space:pre-wrap;border-left:3px solid #f36d33}div.code-starter>pre code{border-left:3px solid #fdff44!important;background-image:radial-gradient(rgba(0,150,0,.75),#000 120%);color:#fff;font:.9rem Inconsolata,monospace}div.code-starter>pre code::after{content:"\a$_"}.code pre{counter-reset:line}code.line-number{counter-increment:line}code.line-number::before{padding:0 .5em;margin-right:.5em;border-right:1px solid #ddd}code.line-number:before{content:"0" counter(line)}code.line-number:nth-child(n+10):before{content:counter(line)}.tab{font-size:1rem;border-color:#8c6728}.tab-content{max-height:0;max-width:100%;transition:max-height .35s}.tab input:checked~.tab-content{max-height:100vh}.tab input:checked+label{padding:1rem;border-left-width:2px;border-color:#6574cd;background-color:#f8fafc;color:#6574cd}.tab label::after{float:right;right:0;top:0;display:block;width:1em;height:1.5em;line-height:1.5;font-size:1rem;text-align:center;transition:all .35s}.tab input[type=checkbox]+label::after{content:"+";font-weight:700;border-width:1px;border-radius:9999px;border-color:#8c6728}.tab input[type=checkbox]:checked+label::after{transform:rotate(315deg);background-color:#6574cd;color:#f8fafc}</style>
<script src="https://kit.fontawesome.com/7efc4bcee2.js" crossOrigin="anonymous"></script>
<script>
    let stateCheck = setInterval(function(){
      if (document.readyState === 'complete') {
        clearInterval(stateCheck);
        let s1 = document.getElementById('start');
        // console.log('doc is ready', s1);
        if (s1) {
           s1.setAttribute('tabindex', '-1');
           s1.focus(); 
           s1.scrollIntoView({behavior: 'smooth'}); 
           setTimeout(function(){s1.blur()}, 500);
           // console.log('focus set');
        }
      }
    }, 200);
    </script>
</head><body class="lesson"><div class="main-content lesson bg-gray-200 text-black p-1 pl-3 font-serif"><div class="md-inner">
<div id="start" class="section">&nbsp;</div><h1 class="overview"></h1><div class="lesson-overview bg-gray-200 flex justify-center"><div class="text-center px-4 py-2 m-2"><div class="lesson-overview-card displaycard bg-blue-200 max-w-sm rounded overflow-hidden shadow-lg"><div> </div><img alt="Text" class="object-contain h-64 w-full" src="https://raw.githubusercontent.com/habermanUIUC/CodeStoryLessons/main/lessons/dmap/text/tfidf/html/TextTripping-sm.png"/><div class="px-6 py-4"><div class="title-text text-center leading-none font-bold text-xl">TF•IDF</div><p class="text-center mt-2 text-gray-800 text-xl">Finding the Important Words</p><div class="text-gray-700 text-base"> </div><div class="text-center mb-3"><span class="inline-block bg-gray-300 rounded-full px-3 py-1 text-sm font-semibold text-gray-700 mr-2">#python</span><span class="inline-block bg-gray-300 rounded-full px-3 py-1 text-sm font-semibold text-gray-700 mr-2">#text</span></div><div class="flex border-t border-solid border-gray-500 shadow-inner justify-around bg-blue-300"><div class="text-gray-700 text-center px-4 m-2 text-sm"><span class="whitespace-no-wrap">D.M. &amp; the 🐍</span></div><div class="text-gray-700 text-center px-4 m-2 text-sm"><span class="whitespace-no-wrap"><strong>Version:</strong> <!-- -->SP21</span></div></div><div class="text-gray-700 mt-1 text-center text-tiny">All Rights Reserved</div></div></div></div><div class="text-center px-4 py-2 m-2 w-1/2"><div class="displaycard bg-gray-200 max-w-sm rounded overflow-hidden shadow-lg"><div class="px-6 py-4 text-left"><div class="text-center font-bold text-xl">TF•IDF<br/><div><span>prerequisites</span><div class="text-center text-xs mb-2">(start only after finishing)</div><p class="max-w-sm text-gray-800 text-sm">⦿ <strong>info490</strong></p><p class="max-w-sm text-gray-800 text-sm">⦿ <strong>NLP</strong></p></div></div></div><div class="px-6 py-4 text-left text-gray-800"><div class="text-center font-bold text-xl">Colab Notes</div><p class="max-w-sm text-sm">1. <strong>Copy</strong> this notebook <img alt="copy2drive.png" class="inline-block" src="https://raw.githubusercontent.com/habermanUIUC/CodeStoryLessons/main/lessons/dmap/text/tfidf/html/copy2drive.png"/></p><p class="max-w-sm text-sm">2. <strong>Update</strong> the <strong><code>NET_ID</code></strong> in the notebook</p><p class="max-w-sm text-gray-800 text-sm">3. <strong>Hit ▶️ </strong> to install the INFO 490 IDE</p><div class="text-center font-bold text-xl"> </div><div class="text-center font-bold text-xl">Jupyter/PyCharm Notes</div><p class="max-w-sm text-gray-800 text-sm text-left">The testing framework does <strong>not work</strong> (at this time) for Jupyter  notebooks or local code development.</p></div></div></div></div><h1 class="section" id="section1">TF-IDF</h1><p class="new">Imagine having an algorithm that can automatically determine which words of a novel are 
unique/important to each chapter as well as filter out the words that are common across all chapters 
without bootstrapping the algorithm with a set of stop words. If we needed an algorithm to 'read'
our novels for us, this sounds almost perfect.</p><p class="new">The idea of ranking each term (i.e. a single word or an n-gram) within the context of a 
set of documents (or chapters in a novel) in which it appears is what the algorithm TF-IDF attempts 
to do. The Term Frequency Inverse Document Frequency is an information retrieval technique that scores 
each term by using its frequency (the TF weight) and its inverse document frequency (the IDF weight).
These two weights are multiplied together to get the TF•IDF score. </p><p class="new">Each term has a score and the score determines how valuable the term is in separating out a set of documents. 
Variations of the TF•IDF weighting scheme are often used by search engines as a central 
tool in scoring and ranking a document's relevance given a user query.</p><h2 id="documents">Documents</h2><img alt="green_eggs-300.png" class="wrap float-left iw300 border" src="https://raw.githubusercontent.com/habermanUIUC/CodeStoryLessons/main/lessons/dmap/text/tfidf/html/green_eggs-300.png"/><p class="new">A document for our purposes is very generic.  It can refer to an article, a book, a journal, a web page, 
a chapter, and even a sentence.  Usually a document needs to be long enough to have some context and a set 
of documents makes up a <em>corpus</em>.  In practice, a document is usually no smaller than a sentence or a set 
of sentences.  We will use the following as a running example to help us understand how to build a TF•IDF model.</p><p class="new">Assume you have the following corpus (a set of documents unified by some theme/attribute):</p><pre><code>c1 = "Do you like Green eggs and ham"
c2 = "I do not like them Sam I am  I do not like Green eggs and ham"
c3 = "Would you like them Here or there"
c4 = "I would not like them Here or there I would not like them Anywhere"

corpus = [c1, c2, c3, c4]</code></pre><p class="new">Let's write some code to turn this array of strings into a set of 'documents'. 
In the following cell, implement the function <code>split_into_tokens</code>.  It takes in a string and simply uses 
white-space to break the string into words (i.e. tokens).</p><p class="new">Once that is done, you should see 7 words (i.e. tokens).  If you process all the
documents, you can confirm (using <code>set</code>) that there are 44 words (17 unique).</p><div class="ide code-starter clearfix"><pre><code>import LessonUtil as Util

def get_corpus():

    c1 = "Do you like Green eggs and ham"
    c2 = "I do not like them Sam I am  I do not like Green eggs and ham"
    c3 = "Would you like them Here or there"
    c4 = "I would not like them Here or there I would not like them Anywhere"
    return [c1, c2, c3, c4]
    
def split_into_tokens(data, normalize=True, min_length=0):

   # returns an array of words
   # each word is simply a string
  
   # splits the incoming data (a string) based on white-space
   # if normalize is True, normalize the case of the words
   
   # only return those words/tokens longer than min_length
   return []
  
def test_split():
  corpus   = get_corpus()
  doc1     = corpus[0]
  print(split_into_tokens(doc1))
  
test_split()

# also, use the test framework to test
# print(ide.tester.test_function(split_into_tokens))</code></pre></div><h2 id="tf">TF</h2><p class="new">We have already been calculating TF (term frequency) since we started working with Python dictionaries and 
counting word occurrences.  So for us, TF would describe the frequency of a word within a document; it 
measures how common a term is within a document.  It is the ratio of the number of times the word 
appears in a document compared to the total number of words in that document:</p><img alt="math?math=%5CLarge%20%5Ctext%7B%0Atf(t%2Cd)%20%3D%20(count%20of%20term%20t%20in%20document%20d)%20%2F%20(number%20of%20words%20in%20document%20d)%0A%7D%0A" class="mt-3 mb-6 formula-block" src="https://render.githubusercontent.com/render/math?math=%5CLarge%20%5Ctext%7B%0Atf(t%2Cd)%20%3D%20(count%20of%20term%20t%20in%20document%20d)%20%2F%20(number%20of%20words%20in%20document%20d)%0A%7D%0A"/><img alt="math?math=%5CHuge%20tf_%7Bt%2Cd%7D%20%3D%20%5Cdfrac%7Bn_%7Bt%2Cd%7D%7D%7B%5Csum_%7B%7D%5E%7B%7D%20n_%7Bt%2Cd%7D%7D" class="mt-3 mb-3 formula-block" src="https://render.githubusercontent.com/render/math?math=%5CHuge%20tf_%7Bt%2Cd%7D%20%3D%20%5Cdfrac%7Bn_%7Bt%2Cd%7D%7D%7B%5Csum_%7B%7D%5E%7B%7D%20n_%7Bt%2Cd%7D%7D"/><ul><li><img alt="math?math=%5CLarge%20n_%7Bt%2Cd%7D" class=" formula-inline" src="https://render.githubusercontent.com/render/math?math=%5CLarge%20n_%7Bt%2Cd%7D" style="display:inline-block"/> (the numerator) is the 
number of times term <strong>t</strong> appears in document <strong>d</strong>.</li><li>Each document has its own TF.</li><li>A term can be an n-gram as well.</li></ul><p class="new">For this lesson, we will implement the TF data structure as a list of dictionaries (actually <code>collections.Counter</code>).
Each counter (i.e document) associates the word (the key) to it count (the value). 
For this example, <code>tf[1]['like']</code> would be the TF for the first document
for the word 'like' (i.e. 2/len(document1)). </p><p class="new">In the following code cell, implement the function named <code>build_tf</code></p><pre><code>import collections

def build_tf(corpus, min_length=0):

   # corpus is an list of documents
   # a document is an unparsed string of words
   
   # returns a tuple with two items:
   # first item is a collections.Counter 
   # for all the words (&gt; min_length) in the corpus
   vocab = collections.Counter()
   tfs = None 
  
   # the second item is the TF array, one for each document
   # each TF is a collection.Counter, 
   # each counter maps a word to the relative frequency 
   # of each word in that document 
   
   return  vocab, tfs</code></pre><p class="new">A few notes.</p><ul><li>Use <img alt="math?math=%5Clarge%20tf_%7Bt%2Cd%7D%20%3D%20%5Cdfrac%7Bn_%7Bt%2Cd%0A%7D%7D%7B%5Csum_%7B%7D%5E%7B%7D%20n_%7Bt%2Cd%7D%7D" class=" formula-inline" src="https://render.githubusercontent.com/render/math?math=%5Clarge%20tf_%7Bt%2Cd%7D%20%3D%20%5Cdfrac%7Bn_%7Bt%2Cd%0A%7D%7D%7B%5Csum_%7B%7D%5E%7B%7D%20n_%7Bt%2Cd%7D%7D" style="display:inline-block"/></li><li>Each TF is a <code>collections.Counter</code>, as is the corpus vocabulary</li><li><code>vocab['eggs']/len(vocab)</code> is frequency of 'eggs' across all documents (chapters)</li><li><code>tf[0]['eggs']</code> is the relative frequency of 'eggs' in the first document (chapter)</li></ul><div class="ide code-starter clearfix"><pre><code>import collections

def build_tf(corpus, min_length=0):

   # corpus is a list of documents
   # a document is an unparsed string of words
   return  None, None
   
def test_tf():
  corpus = get_corpus()
  vocab, tf = build_tf(corpus)
  print(tf[0]['eggs'])  # 0.143
  print(tf[3]['there']) # 0.0714
  # see the next section for hints 

# test_tf()
# print(ide.tester.test_function(build_tf))</code></pre></div><p class="new">After you are done, you can compare your results to the chart below:
</p><img alt="tf2-gham.png" class="w-full border my-4" src="https://raw.githubusercontent.com/habermanUIUC/CodeStoryLessons/main/lessons/dmap/text/tfidf/html/tf2-gham.png"/><p class="new">Note that the word 'like' is the only word that appears in all the documents.
You can also test it against the grader:</p><pre><code>print(vocab.most_common(5)) # 5 most common words
print(tf[1].most_common(5)) # 5 most common words in second document
print(tf[1]['eggs']         # relative freq of 'eggs' in the second document

print(ide.tester.test_function(build_tf))</code></pre><h2 id="idf">IDF</h2><p class="new">The IDF (inverse document frequency) weight isn't as intuitive.  TF measures commonness; 
IDF measures rarity. However, IDF is looking at a corpus. We can illustrate 
this using the following table -- which lists the number of documents a term appears in.  Assume there 
are 100 million documents:</p><pre><code>term          # of documents
             containing term
----------------------------
a                100,000,000
boat               1,000,000
mobile               100,000
mobilegeddon           1,000</code></pre><p class="new">Clearly, the term 'a' provides no value in telling the documents apart.  However, since 'mobilegeddon' 
only appears in 1,000 documents, that word does help separate a small set of the documents. </p><p class="new">To calculate the IDF of each term we take the total number of documents and divide it by the count of document 
occurrences for that word (ie. how many documents contain the term).  Then we take the log of that 
number to scale the result so that the weight is 'dampened'.  This prevents interpreting that 'mobilegeddon'
is 1000 times more important than 'boat'.</p><pre><code>term               n/df[j]            IDF[j]
a              Log(100M/100,000,000) -&gt; 0
boat           Log(100M/1,000,000)   -&gt; 1
mobile         Log(100M/100,000)     -&gt; 2
mobilegeddon   Log(100M/1,000)       -&gt; 3</code></pre><p class="new">Hence, the following formula is the standard way to calculate IDF for term j:</p><img alt="math?math=%5CHuge%20idf_j%20%5C%2C%5C%2C%20%3D%20%5C%2C%5C%2C%20log(%5Cdfrac%7Bn%7D%7Bdf_j%7D)" class=" formula-block" src="https://render.githubusercontent.com/render/math?math=%5CHuge%20idf_j%20%5C%2C%5C%2C%20%3D%20%5C%2C%5C%2C%20log(%5Cdfrac%7Bn%7D%7Bdf_j%7D)"/><ul><li>The value <img alt="math?math=%5CLarge%20n" class=" formula-inline" src="https://render.githubusercontent.com/render/math?math=%5CLarge%20n" style="display:inline-block"/> is the total number of documents</li><li><img alt="math?math=%5CLarge%20df_j" class=" formula-inline" src="https://render.githubusercontent.com/render/math?math=%5CLarge%20df_j" style="display:inline-block"/> is the total number of documents that has term <code>j</code></li></ul><p class="new">So IDF helps measure uniqueness or how much information a term provides.  For search engines, 
it helps identify what terms within each document are special. </p><p class="new">Let's write the function <code>build_idf</code>:</p><p class="new">A few notes:</p><ul><li>Use <code>math.log</code> (natural logarithm)</li><li>IDF is computed once for all documents.</li><li>next section has testing hints</li></ul><div class="ide code-starter clearfix"><pre><code>import math
def build_idf(vocabulary, corpus_tf):

    # return a collection.Counter object
    # such that counter[term] is the idf for that term
    
    term_idf = collections.Counter()
    
    #
    # code in the magic
    #
    # hint:  for term in vocabulary
    #           for doc in doc_tfs 
    #              calculate term_count
    #           calculate idf
    
    
    return term_idf</code></pre></div><p class="new">You can test your code using the following template:</p><pre><code>def test_idf():
    corpus    = get_corpus()
    vocab, tf = build_tf(corpus)
    idf = build_idf(vocab, tf)
    
    print(idf['eggs'])

test_idf()

# print(ide.tester.test_function(build_idf))</code></pre><p class="new">After you are done, you can compare your results to the chart below.
<img alt="idf2-gham.png" class="w-full border mt-3" src="https://raw.githubusercontent.com/habermanUIUC/CodeStoryLessons/main/lessons/dmap/text/tfidf/html/idf2-gham.png"/></p><h2 id="tfidf">TF•IDF</h2><p class="new">As a reminder, the TF is calculated per document, and IDF is calculated across a set of documents. The final 
calculation (the assignment of this lesson) is to create the final TF * IDF. So for each document in the corpus, 
the TF•IDF value is TF * IDF for each term. Values close to zero are your custom set of stop words -- 
words that did not help separate out the documents. Note that you can get these stopwords by just looking at the IDF values.</p><h2 id="adjusting-the-math-">Adjusting The Math </h2><p class="new">At first glance, it might seem 'easy' to calculate TF•IDF (we are almost there). However, for 
both weights (the TF and the IDF) there are different ways to calculate them.   </p><p class="new">The TF we have been describing, scales (e.g. divides) the TF by the document length so that long 
documents don't skew the results.</p><div class="mt-3 mb-6 code-large">TF = (raw count of term in a document)/len
(document)</div><p class="new">A long chapter will have higher raw counts of a term in a document, but may be no more 
important than a shorter chapter.  However, some will still use the raw counts.</p><p class="new">For IDF, there are many variants as well.  The one issue for calculating IDF is that the 
denominator could be zero (if a term does not show up in a document).  In most situations, 
all the terms are coming from the documents themselves, so this can never happen.  </p><p class="new">Some implementations will calculate the 'inverse' going from this equation: </p><div class="no-indent pb-6">
<div class="code-large">term_idf[term] = log(N/term_count)</div><span>to this one</span><div class="code-large"><p class="new">term_idf[term] = -1.0 * log(term_count/N)</p></div><p class="new">If you remember your 'log' rules (here's a small 'proof' for the right hand side):<br/><span><img alt="math?math=%5Clarge%201.%5Cquad%20log(%5Cdfrac%7Ba%7D%7Bb%7D)%20%3D%20log(a)%20-%20log(b)" class=" formula-inline" src="https://render.githubusercontent.com/render/math?math=%5Clarge%201.%5Cquad%20log(%5Cdfrac%7Ba%7D%7Bb%7D)%20%3D%20log(a)%20-%20log(b)" style="display:inline-block"/> (by definition)

</span><br/><span><img alt="math?math=%5Clarge%202.%5Cquad%20log(%5Cdfrac%7Bb%7D%7Ba%7D)%20%3D%20log(b)%20-%20log(a)" class=" formula-inline" src="https://render.githubusercontent.com/render/math?math=%5Clarge%202.%5Cquad%20log(%5Cdfrac%7Bb%7D%7Ba%7D)%20%3D%20log(b)%20-%20log(a)" style="display:inline-block"/> (by definition)

</span><br/><span><img alt="math?math=%5Clarge%203.%5Cquad%20log(a)%20%3D%20log(b)%20-%20log(%5Cdfrac%7Bb%7D%7Ba%7D)" class=" formula-inline" src="https://render.githubusercontent.com/render/math?math=%5Clarge%203.%5Cquad%20log(a)%20%3D%20log(b)%20-%20log(%5Cdfrac%7Bb%7D%7Ba%7D)" style="display:inline-block"/> (rearrange terms in #2)

</span><br/><span><img alt="math?math=%5Clarge%204.%5Cquad%20log(a)%20-%20log(b)%20%3D%20-log(%5Cdfrac%7Bb%7D%7Ba%7D)" class=" formula-inline" src="https://render.githubusercontent.com/render/math?math=%5Clarge%204.%5Cquad%20log(a)%20-%20log(b)%20%3D%20-log(%5Cdfrac%7Bb%7D%7Ba%7D)" style="display:inline-block"/> (rearrange terms in #3)

</span><br/><strong>SO</strong><br/><span><img alt="math?math=%5Clarge%205.%5Cquad%20log(%5Cdfrac%7Ba%7D%7Bb%7D)%20%3D%20-1.0%20%5Ctimes%20log(%5Cdfrac%7Bb%7D%7Ba%7D)" class=" formula-inline" src="https://render.githubusercontent.com/render/math?math=%5Clarge%205.%5Cquad%20log(%5Cdfrac%7Ba%7D%7Bb%7D)%20%3D%20-1.0%20%5Ctimes%20log(%5Cdfrac%7Bb%7D%7Ba%7D)" style="display:inline-block"/></span><br/></p></div><p class="new">Some will add 1 to the result, to the numerator, and/or to the denominator (to set a lower bound on common words and 
avoid giving a weight of zero to common terms).</p><p class="new">For the most part, if your documents are long and the number of terms large, the calculation 
will not matter for comparison purposes.  However, for simple test data, it can matter. 
In practice, everyone seems to have their favorite tweaks to the classic formula. </p><p class="new">The Python library <code>sklearn</code> (coming just around the corner) allows you to slightly adjust
some of the TF/IDF formulas.</p><h2 id="stop-words-reprise">Stop Words Reprise</h2><p class="new">Sometimes it is still useful to remove stop words from the document for the TF•IDF calculations.  In this case, 
the common words that TF•IDF finds are common themes/terms/ngrams that occur throughout the corpus. </p><p class="new">In the case of using TF•IDF to analyze novels, it might be important to which words don't differentiate chapters, but 
are still seen as a common thread.  We could use that in our essay that we are supposed to write for English, 
but only had time to let Python 'read' it. There is an extra credit portion in this lesson that demonstrates 
this with the text of Huckleberry Finn. </p><h2 id="how-fast">How Fast?</h2><p class="new">TF•IDF involves lots of loops and in our Python only version, it can take a long time.  However, once you have 
finished, each document can now be represented as a compact vector (set of values).  Algorithms that 
cluster (grouping documents based on some similarity measure), classify (assign a label) or topic modeling 
sometimes use the TF•IDF representation as the document.  </p><p class="new">Most implements will use Numpy to make the entire process as fast as possible. In the next lesson, we will 
use sklearn's implementation and understand how it differs from this 'classical' definition.</p><h2 id="topic-modeling">Topic Modeling</h2><p class="new">You may be wondering how TF•IDF is related to topic modeling.  If you aren't, then you may not know what 
topic modeling is.  Topic modeling attempts to find a set of themes (e.g. topics) that describe a set of documents.
In the context of topic modeling, a topic is a probability distribution over the vocabulary (i.e. words) and a
document is a probability distribution over topics.  </p><p class="new">You can think of a topic as a set of words that co-occur together and ideally, the topics are as distinct as 
possible.  There are many topic modeling algorithms with LDA (Latent Dirichlet Allocation) being one of the 
more popular ones. You may think of the top N TF•IDF words for a document as
 a 'topic'; however, they are not the same.  As mentioned above, some topic modeling algorithms will use the TF•IDF scores as the 
input for the algorithm.</p><h2 id="review">Review</h2><div class="font-sans container mt-1 mb-4 "><p>🎗Before you go, you should <strong>know</strong>:</p><div class="w-2/3 md:w-2/3"><div class="shadow-md"><div class="tab overflow-hidden border-t bg-green-200"><input class="absolute opacity-0" id="tab-multi-0" name="tabs" type="checkbox"/><label class="block p-3 leading-normal cursor-pointer" for="tab-multi-0"><span> What does TF•IDF measure </span></label><div class="tab-content overflow-hidden border-l-2 bg-orange-300 border-indigo-500 leading-normal"><p class="p-3">No Answer</p></div></div><div class="tab overflow-hidden border-t bg-green-200"><input class="absolute opacity-0" id="tab-multi-1" name="tabs" type="checkbox"/><label class="block p-3 leading-normal cursor-pointer" for="tab-multi-1"><span> The Meaning of TF </span></label><div class="tab-content overflow-hidden border-l-2 bg-orange-300 border-indigo-500 leading-normal"><p class="p-3">No Answer</p></div></div><div class="tab overflow-hidden border-t bg-green-200"><input class="absolute opacity-0" id="tab-multi-2" name="tabs" type="checkbox"/><label class="block p-3 leading-normal cursor-pointer" for="tab-multi-2"><span> The Meaning of IDF </span></label><div class="tab-content overflow-hidden border-l-2 bg-orange-300 border-indigo-500 leading-normal"><p class="p-3"><span>This is a good quiz question</span></p></div></div></div></div></div><h1 class="section" id="section2">Lesson Assignment</h1><p class="new">There's only one more part to write (the final calculation). Finish the 
function <code>compute_TFIDF</code> function. You can see how it will be used as well:</p><pre><code>def compute_TFIDF(doc_tf, idf):
    # returns a collection counter for the document_tf and the corpus idf
    return None

def build_tf_idf(tfs, idf):
    tfidf = [collections.Counter() for x in tfs]
    for idx, doc_tf in enumerate(tfs):
        tfidf[idx] = compute_TFIDF(doc_tf, idf)
    return tfidf

def test_tfidf():

    corpus    = get_corpus()
    vocab, tf = build_tf(corpus)
    idf       = build_idf(vocab, tf)
    
    tfidf = build_tf_idf(tf, idf)
    
    print(idf['eggs'])
    print(tf[0]['eggs'])

# test_tfidf()
# print(ide.tester.test_function(build_tf_idf))</code></pre><div class="ide code-starter clearfix"><pre><code></code></pre></div><p class="new">After you are done, you can compare your results to the chart below.
<img alt="tfidf2-gham.png" class="w-full border" src="https://raw.githubusercontent.com/habermanUIUC/CodeStoryLessons/main/lessons/dmap/text/tfidf/html/tfidf2-gham.png"/></p><p class="new">A few Observations:</p><ol start="1"><li>The word <strong>I</strong> helps differentiate one of the documents</li><li>TF•IDF could be used to help reduce the essence of each document</li></ol><h2 id="more-fun">More Fun.</h2><img alt="cith-150.png" class="float-left wrap mt-1 mb-3 sm" src="https://raw.githubusercontent.com/habermanUIUC/CodeStoryLessons/main/lessons/dmap/text/tfidf/html/cith-150.png"/><p class="new">The lesson framework (installed early on), has many of the components already coded
up for you. The module  <code>LessonUtil.py</code> has most of the parsing and cleaning routines.
You can try your tf•idf on different text.  </p><p class="new">You can look at that code to refresh your Python memory, but the following 
should work once everything is done:
<br class="clear-both"/></p><pre><code>import LessonUtil as Util

def process_cith():
    data     = Util.read_data_file('cith.txt') 
    chapters = Util.split_into_chapters(data)
    corpus   = Util.clean_chapters(chapters)
    
    # first 3 chapters, tokenize words &gt; 2 characters long
    vocab, tf = build_tf(corpus[0:3], 2)
    idf = build_idf(vocab, tf)
    tfidf = build_tf_idf(tf, idf)
    
    Util.print_tfidf(tfidf, top_n=5)</code></pre><p class="new">Output if the corpus is the first 3 chapters and you ignore words &lt; 2 characters long</p><pre><code>Document: 1
 Word: sat            TF-IDF:    0.07324
 Word: too            TF-IDF:    0.05493
 Word: cold           TF-IDF:    0.05493
 Word: sit            TF-IDF:    0.02703
 Word: did            TF-IDF:    0.02027
Document: 2
 Word: bump           TF-IDF:    0.04120
 Word: looked         TF-IDF:    0.02747
 Word: saw            TF-IDF:    0.02747
 Word: him            TF-IDF:    0.02747
 Word: cat            TF-IDF:    0.01520
Document: 3
 Word: hold           TF-IDF:    0.03315
 Word: look           TF-IDF:    0.03315
 Word: fish           TF-IDF:    0.02841
 Word: now            TF-IDF:    0.01894
 Word: can            TF-IDF:    0.01748</code></pre><p class="new">Looks like you can boil the first 3 chapters of the Cat in the Hat down to sitting in the cold, bumping with a cat, 
and looking/holding fish.</p><div class="ide code-starter clearfix"><pre><code></code></pre></div><h1 class="section" id="section3">Extra Credit</h1><p class="new">Modify both <code>build_tf</code> and <code>split_into_tokens</code>, such that a <strong>stoplist</strong> 
could be passed in (see lines 6 and 7).<br/>This is a named parameter (named <code>stopwords</code>) whose default value is the empty list.</p><p class="new">Regardless of the value of <code>normalize</code>, if <code>stopwords</code> is non-empty, it
 indicates that they are to be removed</p><p class="new">If <code>normalize</code> is <code>false</code>: </p><ul><li>still remove stopwords if necessary</li><li>only remove stopwords if they match in case, however</li></ul><p class="new">The idea is that if a word in a document is also a stopword, that word is
 removed and not processed.  This will involve adjusting a few other
 functions as well.  Be sure that if you add a new parameter to handle the
 stopwords, it is a named parameter with a default value of the empty list.</p><div class="code-block code clearfix"><pre><code class="line-number">def process_huck():
</code><code class="line-number">    data     = Util.read_data_file('huck.txt') 
</code><code class="line-number">    chapters = Util.split_into_chapters(data)
</code><code class="line-number">    corpus   = Util.clean_chapters(chapters)
</code><code class="line-number">
</code><code class="line-number">    stopwords = Util.load_stopwords()
</code><code class="line-number">    vocab, tf = build_tf(corpus, min_length=2, stopwords)
</code><code class="line-number">    idf = build_idf(vocab, tf)
</code><code class="line-number">    tfidf = build_tf_idf(tf, idf)
</code><code class="line-number">    
</code><code class="line-number">    Util.print_tfidf(tfidf, top_n=5)
</code></pre></div><h1>Test and Submit</h1><p>Once you have finished, you can download your code (via <code>ide.tester</code>) and upload that file to Gradescope (find lesson with tag <strong>tfidf</strong>).</p><div class="my-4"><pre><code><strong># to list the tests available</strong><br/>print(ide.tester.list_tests())<br/><strong># to perform a specific test</strong><br/>print(ide.tester.test_functionality('name of test'))<br/><strong># to test your code (either works)</strong><br/>print(ide.tester.test_notebook())<br/>print(ide.tester.test_notebook(verbose=True))<br/><strong># to prepare and download your code</strong><br/>ide.tester.download_solution()</code></pre></div><div class="lesson-footer flex bg-gray-200 justify-center"><div class="lesson-footer-card displaycard bg-blue-200 border-t border-gray-400 max-w-2xl rounded overflow-hidden shadow-lg"><div class="px-6 py-4"><div class="title-text text-center font-bold text-xl">TF•IDF</div><p class="text-center text-gray-800 text-xl">Finding the Important Words</p><div class="text-center mt-6 text-xl"><i aria-hidden="true" class="fas fa-tags"></i> any questions on Piazza with <span class="font-bold">tfidf</span></div><div class="text-gray-700 text-base"> </div><div></div><div></div><div class="flex mt-4 border-t border-solid border-gray-500 justify-around bg-gray-200"><div class="text-gray-700 text-center px-4 m-2 text-sm">D.M. &amp; the 🐍</div><div class="text-gray-700 text-center px-4 m-2 text-sm"><strong>Version:</strong> <!-- -->SP21</div></div><div class="text-gray-700 mt-2 text-center text-sm font-bold">All Rights Reserved Michael Haberman</div><div class="text-gray-700 text-center text-sm">Do not distribute this notebook</div></div></div></div><div> </div><div class="ide code-starter clearfix"><pre><code># print(ide.tester.test_notebook()) 
# print(ide.tester.test_notebook(verbose=True)) 

# once you are ready -- run this 
# ide.tester.download_solution() 
</code></pre></div></div></div></body></html>