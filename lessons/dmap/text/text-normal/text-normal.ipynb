{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data, Machines and the üêç \n",
    "<img src=\"https://raw.githubusercontent.com/habermanUIUC/CodeStoryLessons/main/lessons/dmap/text/text-normal/html/section00.png\" align=\"left\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"install\"></a>\n",
    "## Notebook Preparation for Lesson 1‚Ä¢2‚Ä¢3\n",
    "Each lesson will start with a similar template (given in the course schedule):  \n",
    "1. **save** to your google drive (copy to drive)<br/><img src=\"https://raw.githubusercontent.com/habermanUIUC/CodeStoryLessons/main/assets/images/colab/copy-to-drive.png\"/>\n",
    "2. **update** the NET_ID to be your netID (no need to include @illinois.edu)\n",
    "3. **run** the next cell to install the IDE. <img src=\"https://raw.githubusercontent.com/habermanUIUC/CodeStoryLessons/main/assets/images/colab/play-button.png\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "LESSON_ID = 'dmap:text:text-normal'   # keep this as is\n",
    "NET_ID    = 'CHANGE_ME' # CHANGE_ME to your netID (keep the quotes)\n",
    "\n",
    "def install_ide(net_id, lesson_id):\n",
    "  import sys\n",
    "  if 'codestories' not in sys.modules:\n",
    "      print('installing modules')\n",
    "      !pip install git+https://mehaberman@bitbucket.org/mehaberman/codestories.git --upgrade &> install.log\n",
    "  \n",
    "  from codestories.cs.CodeStories import CodeStory\n",
    "  return CodeStory(net_id, lesson_id)\n",
    "\n",
    "ide = install_ide(NET_ID, LESSON_ID)\n",
    "print(ide.welcome())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson Text Normalization\n",
    "(hit ‚ñ∂ to read the first part of the lessonÔ∏è)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run to read the next section\n",
    "ide.reader.view_section(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "import LessonUtil as Util\n",
    "\n",
    "def tokenize(text):\n",
    "    import re\n",
    "  \n",
    "    # allow numbers\n",
    "    # reg = r\"['A-Za-z0-9]+-?[A-Za-z0-9']+\"\n",
    "  \n",
    "    # exclude numbers\n",
    "    reg = r\"['A-Za-z]+-?[A-Za-z']+\"\n",
    "    regex = re.compile(reg)\n",
    "    return regex.findall(text)\n",
    "\n",
    "def normalize(words):\n",
    "    return [w.lower().strip(\"'\") for w in words]\n",
    "\n",
    "def normalization_demo():\n",
    "    path = Util.path_for_data('harryPotter.txt')\n",
    "\n",
    "    with open(path, 'r') as fd:\n",
    "        all = fd.read()\n",
    "        # the most basic way to tokenize\n",
    "        raw = all.split()\n",
    "            \n",
    "        # use a regular expression to tokenize\n",
    "        words = tokenize(all)\n",
    "        normalized = normalize(words)\n",
    "    \n",
    "        uniq = set(words)\n",
    "        uniq_norm = set(normalized)\n",
    "        uniq_norm_min = set([w for w in uniq_norm if len(w) > 2])\n",
    "    \n",
    "        import nltk\n",
    "        nltk.download('stopwords')\n",
    "        from nltk.corpus import stopwords\n",
    "        stop = stopwords.words('english')\n",
    "    \n",
    "        uniq_no_stop = set([w.lower() for w in uniq_norm_min if w not in stop])\n",
    "    \n",
    "        # some basic counts of the different techniques\n",
    "        print(len(raw))    # 78706\n",
    "        print(len(words))  # 75529 with numbers; 75277 w/out\n",
    "        print(len(uniq))\n",
    "        print(len(uniq_norm))\n",
    "        print(len(uniq_norm_min))\n",
    "        print(len(uniq_no_stop))\n",
    "        #print(sorted(uniq_norm_min))\n",
    "\n",
    "normalization_demo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run to read the next section\n",
    "ide.reader.view_section(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "def porter_test(words):\n",
    "    from nltk.stem.porter import PorterStemmer\n",
    "    p_stemmer = PorterStemmer()\n",
    "    for word in words:\n",
    "        msg = \"{:10s} --> {:s}\".format(word, p_stemmer.stem(word))\n",
    "        print(msg)\n",
    "\n",
    "words = 'run runner running ran runs easily fairly'.split()\n",
    "porter_test(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run to read the next section\n",
    "ide.reader.view_section(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "def snowball_test(words):\n",
    "    # Porter2\n",
    "    # The Snowball Stemmer requires that you pass a language parameter\n",
    "    stemmer = nltk.stem.snowball.SnowballStemmer(language='english')\n",
    "    for word in words:\n",
    "        msg = \"{:10s} --> {:s}\".format(word, stemmer.stem(word))\n",
    "        print(msg)\n",
    "snowball_test(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run to read the next section\n",
    "ide.reader.view_section(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lancaster_test(words):\n",
    "    stemmer = nltk.stem.lancaster.LancasterStemmer()\n",
    "    for word in words:\n",
    "        msg = \"{:10s} --> {:s}\".format(word, stemmer.stem(word))\n",
    "        print(msg)\n",
    "lancaster_test(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run to read the next section\n",
    "ide.reader.view_section(9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demo_nltk_lemma(words):\n",
    "    import nltk\n",
    "    nltk.download('wordnet')\n",
    "    lemmer  = nltk.stem.WordNetLemmatizer() \n",
    "    for word in words:\n",
    "        msg = \"{:10s} --> {:s}\".format(word, lemmer.lemmatize(word))\n",
    "        print(msg)\n",
    "\n",
    "    # ask for a specific usage\n",
    "    msg = \"{:10s} --> {:s}\".format('better', lemmer.lemmatize('better', pos=\"a\"))\n",
    "    print(msg)  \n",
    "demo_nltk_lemma(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run to read the next section\n",
    "ide.reader.view_section(11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spacy_lemma_demo1():\n",
    "\n",
    "    import spacy\n",
    "    nlp = spacy.load('en')\n",
    "  \n",
    "    # tokens have a lemma_\n",
    "    doc = nlp(\"Apples are better than ducks\")\n",
    "    for token in doc:\n",
    "        print(token.text, '==>', token.lemma_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run to read the next section\n",
    "ide.reader.view_section(13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spacy_lemma_demo2():\n",
    "    import spacy\n",
    "    from spacy.lemmatizer import Lemmatizer, ADJ, NOUN, VERB\n",
    "    nlp = spacy.load('en')\n",
    "    lemmatizer = nlp.vocab.morphology.lemmatizer\n",
    "    l = lemmatizer('ducks', NOUN)\n",
    "    print(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run to read the next section\n",
    "ide.reader.view_section(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spacy_lemma_demo3():\n",
    "    from spacy.lemmatizer import Lemmatizer\n",
    "    from spacy.lookups import Lookups\n",
    "    lookups = Lookups()\n",
    "    # add a custom conversion for all nouns\n",
    "    lookups.add_table(\"lemma_rules\", {\"noun\": [[\"s\", \"\"]]})\n",
    "    lemmatizer = Lemmatizer(lookups)\n",
    "    lemmas = lemmatizer(\"ducks\", \"NOUN\")\n",
    "    print(lemmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run to read the next section\n",
    "ide.reader.view_section(17)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run to read the next section\n",
    "ide.reader.view_section(18)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run to read the next section\n",
    "ide.reader.view_section(19)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test and Submit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run to read the next section\n",
    "ide.reader.view_section(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(ide.tester.test_notebook()) \n",
    "# print(ide.tester.test_notebook(verbose=True)) \n",
    "\n",
    "# once you are ready -- run this \n",
    "# ide.tester.download_solution()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Text Normalization",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language": "python",
  "story": {
   "auth_token": "4qcBQ0Giv5wY8kQJJ9PHOBBcXmg7TJOnqKCW39sShj4=",
   "authorship_tag": "AB",
   "chapters": 22,
   "name": "Text Normalization",
   "parser": {},
   "root": "https://raw.githubusercontent.com/habermanUIUC/CodeStoryLessons/main/lessons",
   "tag": "dmap:text:text-normal"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
