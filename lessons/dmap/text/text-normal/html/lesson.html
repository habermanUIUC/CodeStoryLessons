<!DOCTYPE html><html lang='en'><head><title>Text Normalization</title><meta charset="utf-8"><style>/*! normalize.css v8.0.1 | MIT License | github.com/necolas/normalize.css */html{line-height:1.15;-webkit-text-size-adjust:100%}body{margin:0}h1{font-size:2em;margin:.67em 0}pre{font-family:monospace,monospace;font-size:1em}a{background-color:transparent}strong{font-weight:bolder}code{font-family:monospace,monospace;font-size:1em}img{border-style:none}input{font-family:inherit;font-size:100%;line-height:1.15;margin:0}input{overflow:visible}[type=checkbox]{box-sizing:border-box;padding:0}::-webkit-file-upload-button{-webkit-appearance:button;font:inherit}blockquote,h1,h2,h3,p,pre{margin:0}html{font-family:system-ui,-apple-system,BlinkMacSystemFont,"Segoe UI",Roboto,"Helvetica Neue",Arial,"Noto Sans",sans-serif,"Apple Color Emoji","Segoe UI Emoji","Segoe UI Symbol","Noto Color Emoji";line-height:1.5}*,::after,::before{box-sizing:border-box;border-width:0;border-style:solid;border-color:#e2e8f0}img{border-style:solid}input:-ms-input-placeholder{color:#a0aec0}input::-ms-input-placeholder{color:#a0aec0}input::-moz-placeholder{color:#a0aec0}h1,h2,h3{font-size:inherit;font-weight:inherit}a{color:inherit;text-decoration:inherit}input{padding:0;line-height:inherit;color:inherit}code,pre{font-family:Menlo,Monaco,Consolas,"Liberation Mono","Courier New",monospace}img{display:block;vertical-align:middle}img{max-width:100%;height:auto}.container{width:100%}@media (min-width:640px){.container{max-width:640px}}@media (min-width:768px){.container{max-width:768px}}@media (min-width:1024px){.container{max-width:1024px}}@media (min-width:1280px){.container{max-width:1280px}}.bg-gray-200{--bg-opacity:1;background-color:#edf2f7;background-color:rgba(237,242,247,var(--bg-opacity))}.bg-gray-300{--bg-opacity:1;background-color:#e2e8f0;background-color:rgba(226,232,240,var(--bg-opacity))}.bg-orange-300{--bg-opacity:1;background-color:#fbd38d;background-color:rgba(251,211,141,var(--bg-opacity))}.bg-green-200{--bg-opacity:1;background-color:#c6f6d5;background-color:rgba(198,246,213,var(--bg-opacity))}.bg-blue-200{--bg-opacity:1;background-color:#bee3f8;background-color:rgba(190,227,248,var(--bg-opacity))}.bg-blue-300{--bg-opacity:1;background-color:#90cdf4;background-color:rgba(144,205,244,var(--bg-opacity))}.border-gray-400{--border-opacity:1;border-color:#cbd5e0;border-color:rgba(203,213,224,var(--border-opacity))}.border-gray-500{--border-opacity:1;border-color:#a0aec0;border-color:rgba(160,174,192,var(--border-opacity))}.border-indigo-500{--border-opacity:1;border-color:#667eea;border-color:rgba(102,126,234,var(--border-opacity))}.rounded{border-radius:.25rem}.rounded-full{border-radius:9999px}.border-solid{border-style:solid}.border{border-width:1px}.border-l-2{border-left-width:2px}.border-t{border-top-width:1px}.cursor-pointer{cursor:pointer}.block{display:block}.inline-block{display:inline-block}.flex{display:flex}.justify-center{justify-content:center}.justify-around{justify-content:space-around}.float-right{float:right}.float-left{float:left}.clearfix:after{content:"";display:table;clear:both}.font-sans{font-family:system-ui,-apple-system,BlinkMacSystemFont,"Segoe UI",Roboto,"Helvetica Neue",Arial,"Noto Sans",sans-serif,"Apple Color Emoji","Segoe UI Emoji","Segoe UI Symbol","Noto Color Emoji"}.font-serif{font-family:Georgia,Cambria,"Times New Roman",Times,serif}.font-semibold{font-weight:600}.font-bold{font-weight:700}.h-64{height:16rem}.text-xs{font-size:.75rem}.text-sm{font-size:.875rem}.text-base{font-size:1rem}.text-xl{font-size:1.25rem}.leading-none{line-height:1}.leading-normal{line-height:1.5}.m-2{margin:.5rem}.my-4{margin-top:1rem;margin-bottom:1rem}.mt-1{margin-top:.25rem}.mt-2{margin-top:.5rem}.mr-2{margin-right:.5rem}.mb-2{margin-bottom:.5rem}.mr-3{margin-right:.75rem}.mb-3{margin-bottom:.75rem}.ml-3{margin-left:.75rem}.mt-4{margin-top:1rem}.mb-4{margin-bottom:1rem}.mt-6{margin-top:1.5rem}.max-w-sm{max-width:24rem}.max-w-2xl{max-width:42rem}.object-contain{-o-object-fit:contain;object-fit:contain}.opacity-0{opacity:0}.overflow-hidden{overflow:hidden}.p-1{padding:.25rem}.p-3{padding:.75rem}.py-1{padding-top:.25rem;padding-bottom:.25rem}.py-2{padding-top:.5rem;padding-bottom:.5rem}.px-3{padding-left:.75rem;padding-right:.75rem}.py-4{padding-top:1rem;padding-bottom:1rem}.px-4{padding-left:1rem;padding-right:1rem}.px-6{padding-left:1.5rem;padding-right:1.5rem}.pl-3{padding-left:.75rem}.absolute{position:absolute}.shadow-md{box-shadow:0 4px 6px -1px rgba(0,0,0,.1),0 2px 4px -1px rgba(0,0,0,.06)}.shadow-lg{box-shadow:0 10px 15px -3px rgba(0,0,0,.1),0 4px 6px -2px rgba(0,0,0,.05)}.shadow-inner{box-shadow:inset 0 2px 4px 0 rgba(0,0,0,.06)}.text-left{text-align:left}.text-center{text-align:center}.text-black{--text-opacity:1;color:#000;color:rgba(0,0,0,var(--text-opacity))}.text-gray-700{--text-opacity:1;color:#4a5568;color:rgba(74,85,104,var(--text-opacity))}.text-gray-800{--text-opacity:1;color:#2d3748;color:rgba(45,55,72,var(--text-opacity))}.whitespace-no-wrap{white-space:nowrap}.w-1\/2{width:50%}.w-2\/3{width:66.666667%}.w-full{width:100%}@media (min-width:768px){.md\:w-2\/3{width:66.666667%}}.text-tiny{font-size:.5rem!important}body{color:#000!important;font-size:1.25rem!important}.main-content{max-width:900px}.lesson{padding-left:15px!important;padding-right:10px!important;--bg-opacity:1;background-color:#edf2f7;background-color:rgba(237,242,247,var(--bg-opacity))}.main-content,html{font-family:Arial,Georgia,Verdana,"Times New Roman"!important}.lesson-footer-card,.lesson-overview-card{font-family:"Times New Roman"!important}blockquote em:first-child{font-family:Times!important;font-size:1.35em;margin-right:10px}blockquote em:first-child:after{content:":"}.lesson-footer{margin-top:50px;margin-top:20px}span{white-space:nowrap}p.new{padding-top:0;padding-bottom:.5em}p.new+p{padding-top:.5em}h1,h2,h3{font-weight:700;margin-top:.25em!important;margin-bottom:.05em!important;font-family:Georgia,Cambria,"Times New Roman",Times,serif!important}h1{font-size:2em!important;clear:both;color:#000!important}div+h1,h2{margin-top:0!important}h2{margin-top:.5em!important;font-size:1.5em!important;clear:both;color:#8b0000!important}h3{font-size:1.25em!important;clear:both;color:#006400!important}p.new a{text-decoration:underline}.lesson a{text-decoration:underline;color:#00f}.title-text{font-size:2rem}blockquote{font-size:1em;background:#f9f9f9;border-left:10px solid #ccc;margin:.5em 10px;padding:.5em 10px;border-left-color:#ffcd69;border-right-color:#f6ba59;quotes:"\201C""\201D""\2018""\2019"}blockquote:before{color:#ccc;content:open-quote;font-size:4em;line-height:.1em;margin-right:.25em;vertical-align:-.4em}blockquote:after{color:#ccc;content:no-close-quote}blockquote p{display:inline}.shadow-lg{box-shadow:0 10px 15px -3px rgba(0,0,0,.1),0 4px 6px -2px rgba(0,0,0,.5)}img.border{border:1px solid #021a40;margin-top:.5rem;margin-bottom:.75rem}img.iw400{height:auto;width:auto;max-width:400px}img.iw300{height:auto;width:auto;max-width:300px}img.iw200{height:auto;width:auto;max-width:200px}code{font-size:smaller}pre code{font-size:15px}pre code:not(.line-number){background:#f4f4f4;font-family:monospace;font-size:15px;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;cursor:default;touch-action:none;-webkit-touch-callout:none;-webkit-tap-highlight-color:transparent;clear:both;border:1px solid #ddd;color:#666;page-break-inside:avoid;display:block;min-width:840px;max-width:840px;overflow:scroll;line-height:1.6;margin-bottom:1.6em;padding:1em 1.5em;-moz-tab-size:2;-o-tab-size:2;tab-size:2;word-wrap:break-word;white-space:pre-wrap;border-left:3px solid #f36d33}div.code-starter>pre code{border-left:3px solid #fdff44!important;background-image:radial-gradient(rgba(0,150,0,.75),#000 120%);color:#fff;font:.9rem Inconsolata,monospace}div.code-starter>pre code::after{content:"\a$_"}.tab{font-size:1rem;border-color:#8c6728}.tab-content{max-height:0;max-width:100%;transition:max-height .35s}.tab input:checked~.tab-content{max-height:100vh}.tab input:checked+label{padding:1rem;border-left-width:2px;border-color:#6574cd;background-color:#f8fafc;color:#6574cd}.tab label::after{float:right;right:0;top:0;display:block;width:1em;height:1.5em;line-height:1.5;font-size:1rem;text-align:center;transition:all .35s}.tab input[type=checkbox]+label::after{content:"+";font-weight:700;border-width:1px;border-radius:9999px;border-color:#8c6728}.tab input[type=checkbox]:checked+label::after{transform:rotate(315deg);background-color:#6574cd;color:#f8fafc}</style>
<script src="https://kit.fontawesome.com/7efc4bcee2.js" crossOrigin="anonymous"></script>
<script>
    let stateCheck = setInterval(function(){
      if (document.readyState === 'complete') {
        clearInterval(stateCheck);
        let s1 = document.getElementById('start');
        // console.log('doc is ready', s1);
        if (s1) {
           s1.setAttribute('tabindex', '-1');
           s1.focus(); 
           s1.scrollIntoView({behavior: 'smooth'}); 
           setTimeout(function(){s1.blur()}, 500);
           // console.log('focus set');
        }
      }
    }, 200);
    </script>
</head><body class="lesson"><div class="main-content lesson bg-gray-200 text-black p-1 pl-3 font-serif"><div class="md-inner">
<div id="start" class="section">&nbsp;</div><h1 class="overview"></h1><div class="lesson-overview bg-gray-200 flex justify-center"><div class="text-center px-4 py-2 m-2"><div class="lesson-overview-card displaycard bg-blue-200 max-w-sm rounded overflow-hidden shadow-lg"><div> </div><img alt="Text" class="object-contain h-64 w-full" src="https://raw.githubusercontent.com/habermanUIUC/CodeStoryLessons/main/lessons/dmap/text/text-normal/html/TextTripping-sm.png"/><div class="px-6 py-4"><div class="title-text text-center leading-none font-bold text-xl">Text Normalization</div><p class="text-center mt-2 text-gray-800 text-xl">Normalizing &amp; Cleaning 🧾📚</p><div class="text-gray-700 text-base"> </div><div class="text-center mb-3"><span class="inline-block bg-gray-300 rounded-full px-3 py-1 text-sm font-semibold text-gray-700 mr-2">#cleaning</span><span class="inline-block bg-gray-300 rounded-full px-3 py-1 text-sm font-semibold text-gray-700 mr-2">#text</span></div><div class="flex border-t border-solid border-gray-500 shadow-inner justify-around bg-blue-300"><div class="text-gray-700 text-center px-4 m-2 text-sm"><span class="whitespace-no-wrap">D.M. &amp; the 🐍</span></div><div class="text-gray-700 text-center px-4 m-2 text-sm"><span class="whitespace-no-wrap"><strong>Version:</strong> <!-- -->SP21</span></div></div><div class="text-gray-700 mt-1 text-center text-tiny">All Rights Reserved</div></div></div></div><div class="text-center px-4 py-2 m-2 w-1/2"><div class="displaycard bg-gray-200 max-w-sm rounded overflow-hidden shadow-lg"><div class="px-6 py-4 text-left"><div class="text-center font-bold text-xl">Text Normalization<br/><div><span>prerequisites</span><div class="text-center text-xs mb-2">(start only after finishing)</div><p class="max-w-sm text-gray-800 text-sm">⦿ <strong>ML Preparation</strong></p><p class="max-w-sm text-gray-800 text-sm">⦿ <strong>normalization (part 1)</strong></p></div></div></div><div class="px-6 py-4 text-left text-gray-800"><div class="text-center font-bold text-xl">Colab Notes</div><p class="max-w-sm text-sm">1. <strong>Copy</strong> this notebook <img alt="copy2drive.png" class="inline-block" src="https://raw.githubusercontent.com/habermanUIUC/CodeStoryLessons/main/lessons/dmap/text/text-normal/html/copy2drive.png"/></p><p class="max-w-sm text-sm">2. <strong>Update</strong> the <strong><code>NET_ID</code></strong> in the notebook</p><p class="max-w-sm text-gray-800 text-sm">3. <strong>Hit ▶️ </strong> to install the INFO 490 IDE</p><div class="text-center font-bold text-xl"> </div><div class="text-center font-bold text-xl">Jupyter/PyCharm Notes</div><p class="max-w-sm text-gray-800 text-sm text-left">The testing framework does <strong>not work</strong> (at this time) for Jupyter  notebooks or local code development.</p></div></div></div></div><h1 class="section" id="section1">Text Normalization </h1><img alt="dataText.png" class="float-left mr-3 iw400 border" src="https://raw.githubusercontent.com/habermanUIUC/CodeStoryLessons/main/lessons/dmap/text/text-normal/html/dataText.png"/><p class="new">This lesson continues the discussion on data preparation and normalization. 
What follows are some common techniques we can use to apply rules to help bring 
some consistency to handling text.</p><p class="new">One of the underlying issues with the analysis that involved word frequency 
counters or dictionaries is that a word is separately counted even though 
it may exist in the dictionary but in a different form. For example, <strong>argue</strong>, <strong>arguing</strong>, <strong>argues</strong>, <strong>argued</strong> would all be distinct keys in our 
counter even though they are essentially the 'same' word. As we saw in both 
the lessons on tf•idf, word embeddings and word2vec, the more
 features (usually unique words), the more space (longer vectors) will be
  required to manage each word (i.e. feature).</p><h2 id="text-cleaning">Text Cleaning</h2><img alt="text.png" class="float-right ml-3 iw300 border" src="https://raw.githubusercontent.com/habermanUIUC/CodeStoryLessons/main/lessons/dmap/text/text-normal/html/text.png"/><p class="new">Cleaning usually means removing or intelligently dealing with errors. But 
cleaning can also involve processing the text such that down stream users
 of the data can easily tokenize or process the text.</p><p class="new">For processing books and other forms of 'printed' media, text, cleaning can involve
 removing front and back matter, chapter headings, and page numbers. However, 
if the text is digitized via OCR (optical character recognition), it's 
quite possible that additional cleaning (even via machine learning) will be 
needed to deal with any digital artifacts.</p><p class="new">It's also possible that if the corpus is large or the text documents are long enough, 
the analysis is no better off by doing additional cleaning. This is not (usually) true when 
dealing with 'raw' human text.</p><h2 id="human-cleaning--">Human Cleaning 💬 </h2><p class="new">When you are dealing with text generated from humans (surveys, emails, 
transcriptions, web pages, recipes, tweets, txt, chats etc) that doesn't go
 through a rigorous editing process, additional cleaning can be extremely useful. 
 However, it is extremely difficult. It can include spell correction, making abbreviations 
consistent (Dr, DR., Doctor, Dr., Doc.), working with text emojis (emoticons) 
and Emojis (😀), handling poor grammar, improper word usage, sentence structure 
and inconsistent punctuation (just to name a few).</p><p class="new">Most systems that process human generated text rely on rule sets as it's
 usually too time consuming (and costly) and difficult to clean the text manually.
Hence, the author's use (or misuse) of slang, puns, idioms, spelling will often lead 
to a wrong analysis of the authors intended meaning.  </p><h2 id="case-normalization-stopwords-and-cut-off-lengths-">Case normalization, stopwords and cut off lengths </h2><p class="new">One of easiest processes to reduce the number of unique words in your corpus
 is to simply make them all lowercase.  For most situations 'The' and 'the' 
 are equivalent.  Another quick method is to simply drop words that are less
  than 3 characters long.  This cleans up any leftovers from parsing
   artifacts (having single punctuation 'words').</p><p class="new">Another effective processing step is to properly handle contractions. 
Depending on the analysis, the words didn't and 'did not' should be treated
 equally.  Most contractions can be expanded with a few regular expression
  rules. </p><p class="new">Removing stop words (words that are so common that they provide no 
information).  Determiners (a, an, another), conjunctions (and, or, but, yet), 
prepositions (in, of) are all good candidates. Run the following demo that
 illustrates how one can cut down from 75K words to 5K by using some simple
  normalization techniques:
</p><div class="ide code-starter clearfix"><pre><code>import LessonUtil as Util

def tokenize(text):
    import re
  
    # allow numbers
    # reg = r"['A-Za-z0-9]+-?[A-Za-z0-9']+"
  
    # exclude numbers
    reg = r"['A-Za-z]+-?[A-Za-z']+"
    regex = re.compile(reg)
    return regex.findall(text)

def normalize(words):
    return [w.lower().strip("'") for w in words]

def normalization_demo():
    path = Util.path_for_data('harryPotter.txt')

    with open(path, 'r') as fd:
        all = fd.read()
        # the most basic way to tokenize
        raw = all.split()
            
        # use a regular expression to tokenize
        words = tokenize(all)
        normalized = normalize(words)
    
        uniq = set(words)
        uniq_norm = set(normalized)
        uniq_norm_min = set([w for w in uniq_norm if len(w) &gt; 2])
    
        import nltk
        nltk.download('stopwords')
        from nltk.corpus import stopwords
        stop = stopwords.words('english')
    
        uniq_no_stop = set([w.lower() for w in uniq_norm_min if w not in stop])
    
        # some basic counts of the different techniques
        print(len(raw))    # 78706
        print(len(words))  # 75529 with numbers; 75277 w/out
        print(len(uniq))
        print(len(uniq_norm))
        print(len(uniq_norm_min))
        print(len(uniq_no_stop))
        #print(sorted(uniq_norm_min))

normalization_demo()</code></pre></div><h2 id="stemming">Stemming</h2><img alt="stemming.png" class="iw200 float-left border mr-3" src="https://raw.githubusercontent.com/habermanUIUC/CodeStoryLessons/main/lessons/dmap/text/text-normal/html/stemming.png"/><p class="new">Stemming is the process of reducing words to a base or root form.  However, 
the result may not be an actual word. The stemming process applies an
 algorithm in an attempt to get to the root word.  One of the easiest
  transformations to make is to remove suffixes (e.g. 'ed' 'ing', 'ly').  The
 stemming process can result in some strange words ('ties' becomes 'ti'). Algorithmic 
stemming has a rich history in computer science and there are multiple algorithms to do so.</p><h3 id="porter-stemmer">Porter stemmer</h3><p class="new">The Porter Stemmer algorithm (by Martin Porter) is one of the more popular
 stemmers. NLTK provides an implementation of it as well. Those interested
 in the details of the algorithm can <a href="http://facweb.cs.depaul.edu/mobasher/classes/csc575/papers/porter-algorithm.html" target="_blank">read</a>
about them.</p><div class="ide code-starter clearfix"><pre><code>import nltk

def porter_test(words):
    from nltk.stem.porter import PorterStemmer
    p_stemmer = PorterStemmer()
    for word in words:
        msg = "{:10s} --&gt; {:s}".format(word, p_stemmer.stem(word))
        print(msg)

words = 'run runner running ran runs easily fairly'.split()
porter_test(words)</code></pre></div><h3 id="snowball-stemmer-">Snowball Stemmer </h3><p class="new">The <a href="https://snowballstem.org/" target="_blank">snowball</a> stemmer <a href="http://snowball.tartarus.org/" target="_blank">historical reference</a> 
fixes a few of the issues with the Porter algorithm. It is written by 
the same author.  You may also come across 'Porter2' references -- which 
refers to the same improved algorithm.</p><div class="ide code-starter clearfix"><pre><code>def snowball_test(words):
    # Porter2
    # The Snowball Stemmer requires that you pass a language parameter
    stemmer = nltk.stem.snowball.SnowballStemmer(language='english')
    for word in words:
        msg = "{:10s} --&gt; {:s}".format(word, stemmer.stem(word))
        print(msg)
snowball_test(words)</code></pre></div><p class="new">You can also use Snowball to build your own domain-specific stemmer.</p><h3 id="lancaster-stemmer">Lancaster Stemmer</h3><p class="new">The Lancaster stemmer is a very aggressive stemming algorithm, sometimes to a fault. 
With porter and snowball, the stemmed representations are usually fairly intuitive.
However, with Lancaster, many shorter words will become totally obfuscated. 
It is the fastest algorithm of the three and will reduce your working set of 
words hugely.  If you want more distinction, it is not the tool you would want. </p><div class="ide code-starter clearfix"><pre><code>def lancaster_test(words):
    stemmer = nltk.stem.lancaster.LancasterStemmer()
    for word in words:
        msg = "{:10s} --&gt; {:s}".format(word, stemmer.stem(word))
        print(msg)
lancaster_test(words)</code></pre></div><blockquote><p class="new"><strong><em>Data Scientist Log</em></strong>  It's important that you not only run the above
 code examples, but actually read and interpret their results.  Test it with
 your set of words.  How do the different algorithms treat those words?</p></blockquote><h2 id="lemmatization-">Lemmatization </h2><img alt="lemming.png" class="border float-left mr-3 iw200" src="https://raw.githubusercontent.com/habermanUIUC/CodeStoryLessons/main/lessons/dmap/text/text-normal/html/lemming.png"/><p class="new">Unlike stemming, lemmatization attempts to reduce the word and keep it's part 
of speech. In linguistics, it is the process of grouping together the 
different inflected forms of a word and treat the set a single item.
Lemmatization looks at surrounding text to determine a given word’s part of speech</p><p class="new">A lemma is the form of the word that usually appears in the dictionary and used 
to represent other forms of that word. Lemmatization is the algorithmic process 
of determining the lemma of a word based on its intended meaning. </p><h3 id="nltk-lemmatization-via-wordnet-">NLTK Lemmatization via Wordnet </h3><p class="new">The nltk nlp toolkit has a lemmatizer that uses <a href="https://wordnet.princeton.edu/" target="_blank">Wordnet</a>, a 
product from Princeton, and is a large database (lexical) of English nouns, 
verbs, adjectives and adverbs. It also provides Synsets which provides a linked 
network of related words.</p><p class="new">The following show a simple demonstration:</p><div class="ide code-starter clearfix"><pre><code>def demo_nltk_lemma(words):
    import nltk
    nltk.download('wordnet')
    lemmer  = nltk.stem.WordNetLemmatizer() 
    for word in words:
        msg = "{:10s} --&gt; {:s}".format(word, lemmer.lemmatize(word))
        print(msg)

    # ask for a specific usage
    msg = "{:10s} --&gt; {:s}".format('better', lemmer.lemmatize('better', pos="a"))
    print(msg)  
demo_nltk_lemma(words)</code></pre></div><h3 id="spacy-lemmatization-">Spacy Lemmatization </h3><p class="new">Spacy has opted to only have lemmatization available instead of having stemming features.
The following shows that whey you tokenize a passage of text, each item includes
 the lemma. </p><div class="ide code-starter clearfix"><pre><code>def spacy_lemma_demo1():

    import spacy
    nlp = spacy.load('en')
  
    # tokens have a lemma_
    doc = nlp("Apples are better than ducks")
    for token in doc:
        print(token.text, '==&gt;', token.lemma_)</code></pre></div><p class="new">You can also use the lemmatizer directly:</p><div class="ide code-starter clearfix"><pre><code>def spacy_lemma_demo2():
    import spacy
    from spacy.lemmatizer import Lemmatizer, ADJ, NOUN, VERB
    nlp = spacy.load('en')
    lemmatizer = nlp.vocab.morphology.lemmatizer
    l = lemmatizer('ducks', NOUN)
    print(l)</code></pre></div><p class="new">You can also build your own lemmatizer and add rules depending on your
 situation:</p><div class="ide code-starter clearfix"><pre><code>
def spacy_lemma_demo3():
    from spacy.lemmatizer import Lemmatizer
    from spacy.lookups import Lookups
    lookups = Lookups()
    # add a custom conversion for all nouns
    lookups.add_table("lemma_rules", {"noun": [["s", ""]]})
    lemmatizer = Lemmatizer(lookups)
    lemmas = lemmatizer("ducks", "NOUN")
    print(lemmas)</code></pre></div><p class="new">Additionally Gensim, TextBlob, and Stanford's CoreNLP provide lemmatizers.</p><h1 class="section" id="section2">Review</h1><div class="font-sans container mt-1 mb-4 "><p>🎗Before you go, you should <strong>know</strong>:</p><div class="w-2/3 md:w-2/3"><div class="shadow-md"><div class="tab overflow-hidden border-t bg-green-200"><input class="absolute opacity-0" id="tab-multi-0" name="tabs" type="checkbox"/><label class="block p-3 leading-normal cursor-pointer" for="tab-multi-0"><span> what is the hardest type of text to clean </span></label><div class="tab-content overflow-hidden border-l-2 bg-orange-300 border-indigo-500 leading-normal"><p class="p-3">No Answer</p></div></div><div class="tab overflow-hidden border-t bg-green-200"><input class="absolute opacity-0" id="tab-multi-1" name="tabs" type="checkbox"/><label class="block p-3 leading-normal cursor-pointer" for="tab-multi-1"><span> what is lemmaization </span></label><div class="tab-content overflow-hidden border-l-2 bg-orange-300 border-indigo-500 leading-normal"><p class="p-3">No Answer</p></div></div><div class="tab overflow-hidden border-t bg-green-200"><input class="absolute opacity-0" id="tab-multi-2" name="tabs" type="checkbox"/><label class="block p-3 leading-normal cursor-pointer" for="tab-multi-2"><span> what is stemming </span></label><div class="tab-content overflow-hidden border-l-2 bg-orange-300 border-indigo-500 leading-normal"><p class="p-3">No Answer</p></div></div></div></div></div><h1 class="section" id="section3">Lesson </h1><p class="new">The only thing you need to do is download and submit it to gradescope. Although there wasn't any code to be written, it's important you know the
 key concepts taught in this lesson.</p><h1>Test and Submit</h1><p>Once you have finished, you can download your code (via <code>ide.tester</code>) and upload that file to Gradescope (find lesson with tag <strong>text-normal</strong>).</p><div class="my-4"><pre><code><strong># to list the tests available</strong><br/>print(ide.tester.list_tests())<br/><strong># to perform a specific test</strong><br/>print(ide.tester.test_functionality('name of test'))<br/><strong># to test your code (either works)</strong><br/>print(ide.tester.test_notebook())<br/>print(ide.tester.test_notebook(verbose=True))<br/><strong># to prepare and download your code</strong><br/>ide.tester.download_solution()</code></pre></div><div class="lesson-footer flex bg-gray-200 justify-center"><div class="lesson-footer-card displaycard bg-blue-200 border-t border-gray-400 max-w-2xl rounded overflow-hidden shadow-lg"><div class="px-6 py-4"><div class="title-text text-center font-bold text-xl">Text Normalization</div><p class="text-center text-gray-800 text-xl">Normalizing &amp; Cleaning 🧾📚</p><div class="text-center mt-6 text-xl"><i aria-hidden="true" class="fas fa-tags"></i> any questions on Piazza with <span class="font-bold">text-normal</span></div><div class="text-gray-700 text-base"> </div><div></div><div></div><div class="flex mt-4 border-t border-solid border-gray-500 justify-around bg-gray-200"><div class="text-gray-700 text-center px-4 m-2 text-sm">D.M. &amp; the 🐍</div><div class="text-gray-700 text-center px-4 m-2 text-sm"><strong>Version:</strong> <!-- -->SP21</div></div><div class="text-gray-700 mt-2 text-center text-sm font-bold">All Rights Reserved Michael Haberman</div><div class="text-gray-700 text-center text-sm">Do not distribute this notebook</div></div></div></div><div> </div><div class="ide code-starter clearfix"><pre><code># print(ide.tester.test_notebook()) 
# print(ide.tester.test_notebook(verbose=True)) 

# once you are ready -- run this 
# ide.tester.download_solution() 
</code></pre></div></div></div></body></html>