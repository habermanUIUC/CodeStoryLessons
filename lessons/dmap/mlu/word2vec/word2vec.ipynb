{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data, Machines and the üêç\n",
    "<img src=\"https://raw.githubusercontent.com/habermanUIUC/CodeStoryLessons/main/lessons/dmap/mlu/word2vec/html/section00.png\" align=\"left\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"install\"></a>\n",
    "## Notebook Preparation for Lesson 1‚Ä¢2‚Ä¢3\n",
    "Each lesson will start with a similar template (given in the course schedule):  \n",
    "1. **save** to your google drive (copy to drive)<br/><img src=\"https://raw.githubusercontent.com/habermanUIUC/CodeStoryLessons/main/assets/images/colab/copy-to-drive.png\"/>\n",
    "2. **update** the NET_ID to be your netID (no need to include @illinois.edu)\n",
    "3. **run** the next cell to install the IDE. <img src=\"https://raw.githubusercontent.com/habermanUIUC/CodeStoryLessons/main/assets/images/colab/play-button.png\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "LESSON_ID = 'dmap:mlu:word2vec'   # keep this as is\n",
    "NET_ID    = 'CHANGE_ME' # CHANGE_ME to your netID (keep the quotes)\n",
    "\n",
    "def install_ide(net_id, lesson_id):\n",
    "  import sys\n",
    "  if 'codestories' not in sys.modules:\n",
    "      print('installing modules')\n",
    "      !pip install git+https://mehaberman@bitbucket.org/mehaberman/codestories.git --upgrade &> install.log\n",
    "  \n",
    "  from codestories.cs.CodeStories import CodeStory\n",
    "  return CodeStory(net_id, lesson_id)\n",
    "\n",
    "ide = install_ide(NET_ID, LESSON_ID)\n",
    "print(ide.welcome())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson Word2Vec\n",
    "(hit ‚ñ∂ to read the first part of the lessonÔ∏è)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run to read the next section\n",
    "ide.reader.view_section(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import gensim\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.simplefilter(action='ignore', category=UserWarning)\n",
    "\n",
    "import LessonUtil as Util\n",
    "\n",
    "def build_dataset_raw():\n",
    "\n",
    "  # here's an example of how to use a zipped (compressed) file\n",
    "  filename = Util.path_for_data('cars.csv.gz')\n",
    "  # https://www.kaggle.com/CooperUnion/cardataset?select=data.csv\n",
    "  file = gzip.open(filename, 'rb')\n",
    "\n",
    "  # clean and tokenize the text\n",
    "  return [gensim.utils.simple_preprocess(line) for line in file]\n",
    "\n",
    "def test_raw():\n",
    "  document = build_dataset_raw()\n",
    "  print(document[0])  # make note of the column names\n",
    "  print(document[10]) # row '9'\n",
    "  \n",
    "test_raw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run to read the next section\n",
    "ide.reader.view_section(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "import LessonUtil as Util\n",
    "def build_dataset():\n",
    "\n",
    "  # another way to read compressed data\n",
    "  filename = Util.path_for_data('cars.csv.gz')\n",
    "  df = pd.read_csv(filename, compression='gzip')\n",
    "\n",
    "  # feature selection\n",
    "  # select the fields we want to train word2vec on\n",
    "  features = ['Market Category','Vehicle Size','Vehicle Style',\n",
    "              'Engine Fuel Type','Transmission Type','Driven_Wheels']\n",
    "              \n",
    "  df = df[features]\n",
    "  doc = []\n",
    "  for index, row in df.iterrows(): \n",
    "    line = [r for v in row.values for r in str(v).split(',')]\n",
    "    doc.append(line)\n",
    "  \n",
    "  return doc, df\n",
    "\n",
    "def test_pd_data():\n",
    "  document, df = build_dataset()\n",
    "  print(document[0][0:5])\n",
    "\n",
    "test_pd_data()\n",
    "\n",
    "# ==> ['Factory Tuner', 'Luxury', 'High-Performance', 'Compact', 'Coupe']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run to read the next section\n",
    "ide.reader.view_section(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# type&run the above example/exercise in this cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run to read the next section\n",
    "ide.reader.view_section(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, df=None):\n",
    "\n",
    "  output = ''\n",
    "  if df is not None:\n",
    "    unique_set = df['Make_Model'].unique()\n",
    "    missing=0\n",
    "    for mm in unique_set:\n",
    "      if mm not in model.wv.vocab:\n",
    "        missing += 1\n",
    "    output += \"{:d} models are missing of {:d}\\n\".format(missing, len(unique_set))\n",
    "  \n",
    "  try:\n",
    "    t = 'Toyota Camry'\n",
    "    other = ['Honda Accord', 'Nissan Van', 'Mercedes-Benz SLK-Class']\n",
    "    for o in other:\n",
    "      output += t + '->' + o + ' ' + \"{:0.4f}\\n\".format(model.wv.similarity(t,o))\n",
    "      \n",
    "    tuples = model.wv.most_similar(positive='Honda Odyssey', topn=3)\n",
    "    for mm, v in tuples:\n",
    "      output += mm + ', '\n",
    "    output = output.strip(', ')\n",
    "    \n",
    "  except KeyError as e:\n",
    "    output += \"\\nError:\" + str(e)\n",
    "\n",
    "  return output\n",
    "\n",
    "def test_v0():\n",
    "  document, df = build_dataset()\n",
    "  model = build_model_v0(document)\n",
    "  print(evaluate_model(model, df))\n",
    "\n",
    "test_v0()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run to read the next section\n",
    "ide.reader.view_section(9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# type&run the above example/exercise in this cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run to read the next section\n",
    "ide.reader.view_section(11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model_v2(doc):\n",
    "  model = gensim.models.Word2Vec(\n",
    "          doc,\n",
    "          min_count=1, # ignore words that occur less than 2 times\n",
    "          workers=1,   # threads to use\n",
    "          window=10,   # size of window around the target word\n",
    "          iter=15      # 15 epochs\n",
    "          )  \n",
    "  return model\n",
    "\n",
    "def test_v2():\n",
    "  document, df = build_dataset()\n",
    "  model = build_model_v2(document)\n",
    "  print(evaluate_model(model,df))\n",
    "  \n",
    "test_v2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run to read the next section\n",
    "ide.reader.view_section(13)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Two Ways To Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run to read the next section\n",
    "ide.reader.view_section(14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model_v3(doc):\n",
    "  model = gensim.models.Word2Vec(\n",
    "            doc,\n",
    "            min_count=1, # ignore words that occur less than 2 times\n",
    "            workers=1,   # threads to use\n",
    "            window=10,   # size of window around the target word\n",
    "            iter=15,     # how many times to iterate over the corpus (train)\n",
    "            size=100,    # how big the output vectors (spacy == 300)\n",
    "            sg=1,        # 0 == CBOW (default) 1 == skip gram\n",
    "          )     \n",
    "  return model\n",
    "\n",
    "def test_v3():\n",
    "  document, df = build_dataset()\n",
    "  model = build_model_v3(document)\n",
    "  print(evaluate_model(model,df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run to read the next section\n",
    "ide.reader.view_section(16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GloVe (2014)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run to read the next section\n",
    "ide.reader.view_section(17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "def install_build_glove():\n",
    "  # copy the software to the VM\n",
    "  !git clone https://github.com/stanfordnlp/GloVe.git glove\n",
    "  # compile the software\n",
    "  !cd glove && make\n",
    "  # copy a small dataset for GloVe to use\n",
    "  !cp /content/info490/INFO490Assets/src/datasets/books/hp/harryPotter.txt /content/sample_data\n",
    "\n",
    "install_build_glove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run to read the next section\n",
    "ide.reader.view_section(19)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "def run_glove():\n",
    "  !cd glove ; ./demo.sh\n",
    "\n",
    "def test_glove():\n",
    "  # convert glove to word2vec format\n",
    "  w2v_info = glove2word2vec('glove/vectors.txt', 'vec.word2vec')\n",
    "  print('voc. size, vector size', w2v_info)\n",
    "\n",
    "  # now read in the format\n",
    "  hp_model = KeyedVectors.load_word2vec_format('vec.word2vec', binary=False)\n",
    "  print(type(hp_model))\n",
    "  print(hp_model.most_similar('Harry', topn=5))\n",
    "    \n",
    "run_glove()\n",
    "test_glove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run to read the next section\n",
    "ide.reader.view_section(21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "import time\n",
    "\n",
    "def load_glove_model(resource):\n",
    "  #print(api.info())\n",
    "  print(api.info(resource))\n",
    "  st = time.time()\n",
    "  \n",
    "  # download the model, unzip, convert it \n",
    "  model = api.load(resource)\n",
    "  \n",
    "  print(\"load time\", time.time() - st)\n",
    "  return model\n",
    "\n",
    "#gw50 = load_glove_model('glove-wiki-gigaword-50')   # 66 MB,  about a minute\n",
    "#gt200 = load_glove_model('glove-twitter-200')       # 758 MB, about 7 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run to read the next section\n",
    "ide.reader.view_section(23)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# fastText  (2016)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run to read the next section\n",
    "ide.reader.view_section(24)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run to read the next section\n",
    "ide.reader.view_section(25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson Assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run to read the next section\n",
    "ide.reader.view_section(26)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test and Submit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run to read the next section\n",
    "ide.reader.view_section(27)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(ide.tester.test_notebook()) \n",
    "# print(ide.tester.test_notebook(verbose=True)) \n",
    "\n",
    "# once you are ready -- run this \n",
    "# ide.tester.download_solution()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Word2Vec",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language": "python",
  "story": {
   "auth_token": "odNm_htrLFKbiR2peUxJnShKK8AKlo1FL5M479z7f9U=",
   "authorship_tag": "AB",
   "chapters": 29,
   "name": "Word2Vec",
   "parser": {},
   "root": "https://raw.githubusercontent.com/habermanUIUC/CodeStoryLessons/main/lessons",
   "tag": "dmap:mlu:word2vec"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
